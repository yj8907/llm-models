# WARNING: Yaml configs is currently an experimental feature
# gpt config for bidirectional gpt

language_model:
  ####################
  # model architecture
  ####################
  num_layers: 4
  mtp_num_layers: null
  mtp_loss_scaling_factor: null
  num_layers_in_first_pipeline_stage: null
  num_layers_in_last_pipeline_stage: null
  pipeline_model_parallel_layout: null
  account_for_embedding_in_pipeline_split: False
  account_for_loss_in_pipeline_split: False
  hidden_size: 512
  num_attention_heads: 16
  attention_backend: auto  # Options: auto, local, flash, fused, unfused, cudnn
  softmax_scale: null
  softmax_type: 'vanilla'  # Options: 'vanilla', 'off-by-one', 'learnable'
  num_query_groups: null
  ffn_hidden_size: null  # Defaults to 4*hidden_size if null
  kv_channels: null  # Defaults to hidden_size // num_attention_heads if null
  hidden_dropout: 0.0
  attention_dropout: 0.0
  fp32_residual_connection: False
  apply_residual_connection_post_layernorm: False
  layernorm_epsilon: 1.e-5
  layernorm_zero_centered_gamma: True
  add_bias_linear: False
  add_qkv_bias: False
  gated_linear_unit: False
  activation_func: swiglu
  activation_func_fp8_input_store: False
  glu_linear_offset: 0.0
  activation_func_clamp_value: null
  num_moe_experts: null
  rotary_interleaved: False
  window_size: null
  window_attn_skip_freq: null
  normalization: "RMSNorm"  # Options: "LayerNorm", "RMSNorm"
  qk_layernorm: False
  qk_l2_norm: False
  qk_clip: False
  qk_clip_alpha: 0.5
  qk_clip_threshold: 100
  log_max_attention_logit: False
  attention_output_gate: False
  test_mode: False
  calculate_per_token_loss: False
  multi_latent_attention: False
  no_rope_freq: null
  use_legacy_models: False
  padded_vocab_size: 0
  use_rope_scaling: False

  ####################
  # MLA (Multi-Latent Attention) config
  ####################
  q_lora_rank: 512
  kv_lora_rank: 512
  qk_head_dim: 128
  qk_pos_emb_head_dim: 64
  v_head_dim: 128
  rope_type: "yarn"  # Options: "rope", "yarn"
  rotary_base: 10000
  rotary_percent: 1.0
  rotary_scaling_factor: 40
  original_max_position_embeddings: 4096
  beta_fast: 32
  beta_slow: 1
  mscale: 1.0
  mscale_all_dim: 0.0
  cache_mla_latents: False

  ####################
  # attention variant
  ####################
  experimental_attention_variant: null  # Options: null, "gated_delta_net", "dsa"

  ####################
  # DSA
  ####################
  dsa_indexer_n_heads: null
  dsa_indexer_head_dim: null
  dsa_indexer_topk: null
  dsa_indexer_loss_coeff: null
  dsa_indexer_use_sparse_loss: False

  ####################
  # linear attention
  ####################
  linear_attention_freq: null
  linear_conv_kernel_dim: null
  linear_key_head_dim: null
  linear_value_head_dim: null
  linear_num_key_heads: null
  linear_num_value_heads: null

  ####################
  # initialization
  ####################
  init_method: null  # Options: null, "xavier_uniform", or callable
  output_layer_init_method: null
  scaled_init_method: null  # Options: null, "xavier_uniform", or callable
  init_method_std: 0.02
  embedding_init_method: null  # Options: null, "xavier_uniform", or callable
  embedding_init_method_std: null
  init_model_with_meta_device: False

  ####################
  # mixed-precision
  ####################
  apply_query_key_layer_scaling: False
  attention_softmax_in_fp32: True
  disable_bf16_reduced_precision_matmul: False

  ####################
  # fusion
  ####################
  bias_activation_fusion: False
  bias_swiglu_fusion: True
  masked_softmax_fusion: True
  persist_layer_norm: False
  memory_efficient_layer_norm: False
  bias_dropout_fusion: True
  apply_rope_fusion: True
  use_fused_weighted_squared_relu: False
  fused_single_qkv_rope: False

  ####################
  # activation recomputation
  ####################
  recompute_granularity: null  # Options: null, "full", "selective"
  recompute_method: null  # Options: null, "block", "uniform"
  recompute_num_layers: null
  distribute_saved_activations: null
  recompute_modules: null  # Options: "core_attn", "moe_act", "layernorm", "mla_up_proj", "mlp", "moe", "shared_experts"

  ####################
  # fp8 related
  ####################
  fp8: null  # Options: null, "e4m3", "hybrid"
  fp8_recipe: "delayed"  # Options: "tensorwise", "delayed", "mxfp8", "blockwise", "custom"
  fp8_param: False
  fp8_quantizer_factory: null
  fp8_margin: 0
  fp8_interval: 1  # Deprecated from TE v1.8.0
  fp8_amax_history_len: 1
  fp8_amax_compute_algo: "most_recent"  # Options: "max", "most_recent"
  fp8_wgrad: True
  fp8_dot_product_attention: False
  fp8_multi_head_attention: False
  tp_only_amax_red: False
  first_last_layers_bf16: False
  num_layers_at_start_in_bf16: 1
  num_layers_at_end_in_bf16: 1
  use_kitchen: False
  use_kitchen_attention: False
  kitchen_attention_backend: "sdpa"  # Options: "sdpa", "fa"

  ####################
  # fp4 related
  ####################
  fp4: null  # Options: null, "nvfp4"
  fp4_recipe: "nvfp4"
  fp4_param: False
  fp4_quantizer_factory: null

  ####################
  # MoE related
  ####################
  moe_shared_expert_intermediate_size: null
  moe_shared_expert_gate: False
  moe_shared_expert_overlap: False
  moe_layer_freq: 1
  moe_ffn_hidden_size: null
  moe_router_load_balancing_type: "aux_loss"  # Options: "aux_loss", "seq_aux_loss", "global_aux_loss", "sinkhorn", "none"
  moe_router_topk: 2
  moe_enable_routing_replay: False
  moe_router_topk_limited_devices: null  # Deprecated
  moe_router_padding_for_quantization: False
  moe_router_padding_for_fp8: False  # Deprecated, use moe_router_padding_for_quantization
  moe_router_num_groups: null
  moe_router_group_topk: null
  moe_router_pre_softmax: False
  moe_router_topk_scaling_factor: null
  moe_router_score_function: "softmax"  # Options: "softmax", "sigmoid"
  moe_router_dtype: null  # Options: null, "fp32", "fp64"
  moe_router_enable_expert_bias: False
  moe_router_bias_update_rate: 1.e-3
  moe_router_force_load_balancing: False
  moe_grouped_gemm: False
  moe_use_legacy_grouped_gemm: False
  moe_aux_loss_coeff: 0  # 1e-2 would be a good start value for load balance loss
  moe_z_loss_coeff: null  # 1e-3 would be a good start value for z-loss
  moe_input_jitter_eps: null
  moe_token_dropping: False
  moe_token_dispatcher_type: "allgather"  # Options: "allgather", "alltoall", "flex"
  moe_enable_deepep: False  # Deprecated, use moe_flex_dispatcher_backend
  moe_flex_dispatcher_backend: "deepep"  # Options: "deepep", "hybridep"
  moe_per_layer_logging: False
  moe_expert_capacity_factor: null
  moe_pad_expert_input_to_capacity: False
  moe_token_drop_policy: "probs"  # Options: "probs", "position"
  moe_layer_recompute: False  # Deprecated
  moe_permute_fusion: False
  moe_router_fusion: False
  moe_apply_probs_on_input: False
  moe_latent_size: null
  moe_deepep_num_sms: 20
  moe_hybridep_num_sms: 16

  ##################
  # Context Parallel
  ##################
  cp_comm_type: null  # Options: null, "p2p", "all_gather", "a2a", "a2a+p2p", or list

  ##################
  # Cuda Graphs
  ##################
  enable_cuda_graph: False  # Deprecated, use cuda_graph_impl
  cuda_graph_use_single_mempool: False
  cuda_graph_retain_backward_graph: False
  cuda_graph_warmup_steps: 3
  external_cuda_graph: False  # Deprecated, use cuda_graph_impl
  cuda_graph_impl: "none"  # Options: "none", "local", "transformer_engine"
  cuda_graph_scope: "full"  # Options: "full", "attn", "mlp", "moe", "moe_router", "moe_preprocess", "mamba", "full_iteration"

  ####################
  # miscellaneous
  ####################
  clone_scatter_output_in_embedding: True
  disable_parameter_transpose_cache: False
  config_logger_dir: ""
  flash_decode: False
  batch_invariant_mode: False
  use_te_activation_func: False
  use_te_rng_tracker: False
  inference_rng_tracker: False
  inference_sampling_seed: 42
  symmetric_ar_type: null
  use_inference_optimized_layers: False
  inference_fuse_tp_communication: False
  mrope_section: null
  is_hybrid_model: False
  mamba_state_dim: 128
  mamba_head_dim: 64
  mamba_num_groups: 8
  mamba_num_heads: null
  use_mamba_mem_eff_path: True
  mlp_chunks_for_prefill: 1
  heterogeneous_block_specs: False
  hetereogenous_dist_checkpoint: False

  ####################
  # Quantization
  ####################
  quant_recipe: null

  transformer_impl: local  # Options: "transformer_engine", "local", "inference_optimized"

  #####################################
  # Fine-grained Activation Offloading
  #####################################
  fine_grained_activation_offloading: False
  offload_modules: null  # Options: "attn_norm", "qkv_linear", "core_attn", "attn_proj", "mlp_norm", "expert_fc1", "moe_act"
  min_offloaded_tensor_size: 1048576  # 1024 * 1024

model_parallel:
  ###################
  # Model parallelism
  ###################
  tensor_model_parallel_size: 1
  pipeline_model_parallel_comm_backend: "nccl"  # Options: nccl, ucc, or null for default
  pipeline_model_parallel_size: 1
  virtual_pipeline_model_parallel_size: null
  sequence_parallel: False
  context_parallel_size: 1
  hierarchical_context_parallel_sizes: [1]
  max_seqlen_per_dp_cp_rank: null
  hybrid_context_parallel: False
  expert_model_parallel_size: 1
  expert_tensor_parallel_size: null  # Defaults to tensor_model_parallel_size if null
  moe_extended_tp: False  # Deprecated from MCore v0.10

  ###################
  # Initialization
  ###################
  perform_initialization: True
  use_cpu_initialization: False

  ###################
  # Training
  ###################
  fp16: False
  bf16: True
  params_dtype: null
  timers: null
  finalize_model_grads_func: null
  grad_scale_func: null
  no_sync_func: null
  grad_sync_func: null
  param_sync_func: null
  deterministic_mode: False
  enable_autocast: False
  autocast_dtype: null
  num_microbatches_with_partial_activation_checkpoints: null

  ###################
  # Optimizations
  ###################
  gradient_accumulation_fusion: True
  async_tensor_model_parallel_allreduce: True  # Deprecated
  tp_comm_overlap: False
  tp_comm_bulk_wgrad: True
  tp_comm_bulk_dgrad: True
  tp_comm_overlap_ag: True
  tp_comm_overlap_rs: True
  tp_comm_overlap_rs_dgrad: False
  tp_comm_split_ag: True  # Deprecated from TE v1.6.0
  tp_comm_atomic_ag: False  # Deprecated from TE v1.6.0
  tp_comm_split_rs: True  # Deprecated from TE v1.6.0
  tp_comm_atomic_rs: False  # Deprecated from TE v1.6.0
  cross_entropy_loss_fusion: False
  cross_entropy_fusion_impl: 'native'  # Options: 'native', 'te'
  tp_comm_overlap_disable_qkv: False
  tp_comm_overlap_disable_fc1: False
  tp_comm_bootstrap_backend: 'nccl'  # Options: 'nccl', 'mpi', 'gloo'
  overlap_moe_expert_parallel_comm: False
  delay_wgrad_compute: False
  ep_overlap_early_attn_memory_release: False

  ###################
  # Pipeline Parallel
  ###################
  pipeline_dtype: null
  variable_seq_lengths: False
  overlap_p2p_comm: False
  batch_p2p_comm: True
  batch_p2p_sync: True
  use_ring_exchange_p2p: False
  deallocate_pipeline_outputs: False
  defer_embedding_wgrad_compute: False
  wgrad_deferral_limit: 0
  overlap_p2p_comm_warmup_flush: False
  microbatch_group_size_per_vp_stage: null
  mtp_standalone: False

  ###################
  # CPU Offloading
  ###################
  cpu_offloading: False
  cpu_offloading_num_layers: 0
  _cpu_offloading_context: null
  cpu_offloading_activations: True
  cpu_offloading_weights: False
  cpu_offloading_double_buffering: False

  ###################
  # Timing
  ###################
  barrier_with_L1_time: True

# training:
use_legacy_models: False
# spec should be a tuple: [module_path, object_name]
# Example: ["megatron.core.models.gpt.gpt_layer_specs", "get_gpt_layer_local_spec"]
# Example: ["megatron.core.models.gpt.gpt_layer_specs", "get_gpt_layer_with_transformer_engine_spec"]
spec: ["models.bidirectional.bidirGPT", "get_bidirectinoal_gpt_layer_with_transformer_engine_spec"]
micro_batch_size: 2
global_batch_size: 128
rampup_batch_size: [32, 32, 65324160]
check_for_nan_in_loss_and_grad: True
num_layers_per_virtual_pipeline_stage: null

encoder_num_layers: null
decoder_num_layers: null
rotary_seq_len_interpolation_factor: null
add_position_embedding: False
make_vocab_size_divisible_by: 128
group_query_attention: False

exit_signal_handler: False
exit_duration_in_mins: null
exit_interval: null

untie_embeddings_and_output_weights: True
position_embedding_type: rope
rotary_percent: 0.5
openai_gelu: False
squared_relu: False
swiglu: True
onnx_safe: null
bert_binary_head: True
max_position_embeddings: 4096

use_flash_attn: False
seed: 1234
data_parallel_random_init: False

# Optimizer
optimizer: adam
lr: 2.5e-4
lr_decay_style: cosine
lr_decay_iters: null
lr_decay_samples: 255126953
lr_warmup_fraction: null
lr_warmup_iters: 0
lr_warmup_samples: 81381
lr_warmup_init: 0.0
min_lr: 2.5e-5
weight_decay: 0.1
start_weight_decay: null
end_weight_decay: null
weight_decay_incr_style: constant
clip_grad: 1.0
adam_beta1: 0.9
adam_beta2: 0.95
adam_eps: 1.e-08
sgd_momentum: 0.9
override_opt_param_scheduler: False
use_checkpoint_opt_param_scheduler: False

# checkpointing arguments
save: null
save_interval: 20000
no_save_optim: null
no_save_rng: null
load: null
no_load_optim: null
no_load_rng: null
finetune: False
use_checkpoint_args: False
exit_on_missing_checkpoint: False

# loss arguments
loss_scale: null
initial_loss_scale: 4294967296
min_loss_scale: 1.0
loss_scale_window: 1000
hysteresis: 2
accumulate_allreduce_grads_in_fp32: False
fp16_lm_cross_entropy: False

# distributed arguments
distributed_backend: nccl
distributed_timeout_minutes: 10
overlap_grad_reduce: False
align_grad_reduce: True
overlap_param_gather: False
align_param_gather: False
scatter_gather_tensors_in_pipeline: True
local_rank: null
lazy_mpu_init: null
empty_unused_memory_level: 0
standalone_embedding_stage: False
use_distributed_optimizer: False
nccl_communicator_config_path: null

train_iters: null
eval_iters: 32
eval_interval: 2000
skip_train: False

adlr_autoresume: False
adlr_autoresume_interval: 1000

# garbage collection
manual_gc: False
manual_gc_interval: 0
manual_gc_eval: True

tp_comm_overlap_cfg: null

#data
data_path: null
split: '99,1,0'
train_data_path: null
valid_data_path: null
test_data_path: null
data_cache_path: null
mock_data: False
vocab_size: null
vocab_file: null
merge_file: null
vocab_extra_ids: 0
seq_length: 1024
encoder_seq_length: null
decoder_seq_length: null
retriever_seq_length: 256
sample_rate: 1.0
mask_prob: 0.15
short_seq_prob: 0.1
num_workers: 2
tokenizer_type: HuggingFaceTokenizer
tokenizer_model: NousResearch/Meta-Llama-3-8B-Alternate-Tokenizer
reset_position_ids: False
reset_attention_mask: False
eod_mask_loss: False
train_samples: 268554688
dataloader_type: null

#profile:
profile: False
profile_ranks: [0]
profile_step_end: 12
profile_step_start: 10

#logging:
log_params_norm: True
log_num_zeros_in_grad: True
log_throughput: False
log_progress: False
timing_log_level: 0
timing_log_option: minmax
tensorboard_log_interval: 1
tensorboard_queue_size: 1000
log_timers_to_tensorboard: False
log_validation_ppl_to_tensorboard: False
log_memory_to_tensorboard: False
log_world_size_to_tensorboard: False
log_loss_scale_to_tensorboard: True
wandb_project: ''
wandb_exp_name: ''
wandb_save_dir: ''
enable_one_logger: True
one_logger_project: megatron-lm
one_logger_run_name: null
log_interval: 100
tensorboard_dir: null