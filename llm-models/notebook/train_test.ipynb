{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52a5e7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "import os\n",
    "from typing import Iterator\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import load_from_disk, Dataset as HFDataset # Rename to avoid clash\n",
    "\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82e05893-495d-48aa-bd37-d9e2bbc59ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 0.0 MB\n",
      "Reserved: 0.0 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"Allocated:\", torch.cuda.memory_allocated() / 1024**2, \"MB\")\n",
    "print(\"Reserved:\", torch.cuda.memory_reserved() / 1024**2, \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbd25c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, importlib\n",
    "current = os.getcwd()\n",
    "parent = os.path.dirname(current)\n",
    "sys.path.append(parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58dc61ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'models.deepseek.data' from '/home/ubuntu/projects/llm-models/llm-models/models/deepseek/data.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.deepseek import model\n",
    "importlib.reload(model)\n",
    "\n",
    "from models.deepseek import train\n",
    "importlib.reload(train)\n",
    "\n",
    "from models.deepseek import data\n",
    "importlib.reload(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b78f56e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(data)\n",
    "\n",
    "from torch.utils.data import IterableDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2fc0706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/projects/llm-models/llm-models/notebook'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad85232d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'models.deepseek.data' has no attribute 'TinyStoriesDataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m tinydataset = \u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTinyStoriesDataset\u001b[49m(data_dir=\u001b[33m'\u001b[39m\u001b[33m../../data\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'models.deepseek.data' has no attribute 'TinyStoriesDataset'"
     ]
    }
   ],
   "source": [
    "tinydataset = data.TinyStoriesDataset(data_dir='../../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f4e3558",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(tinydataset, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f700f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\\nLily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\\nTogether, they shared the needle and sewed the button on Lily\\'s shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.\\n<|endoftext|>\\nOnce upon a time, there was a little car named Beep. Beep loved to go fast and play in the sun. Beep was a healthy car because he always had good fuel. Good fuel made Beep happy and strong.\\nOne day, Beep was driving in the park when he saw a big tree. The tree had many leaves that were falling. Beep liked how the leaves fall and wanted to play with them. Beep drove under the tree and watched the leaves fall on him. He laughed and beeped his horn.\\nBeep played with the falling leaves all day. When it was time to go home, Beep knew he needed more fuel. He went to the fuel place and got more healthy fuel. Now, Beep was ready to go fast and play again the next day. And Beep lived happily ever after.\\n<|endoftext|>\\nOne day, a little fish named Fin was swimming near the shore. He saw a big crab and wanted to be friends. \"Hi, I am Fin. Do you want to play?\" asked the little fish. The crab looked at Fin and said, \"No, I don\\'t want to play. I am cold and I don\\'t feel fine.\"\\nFin felt sad but wanted to help the crab feel better. He swam away and thought of a plan. He remembered that the sun could make things warm. So, Fin swam to the top of the water and called to the sun, \"Please, sun, help my new friend feel fine and not freeze!\"\\nThe sun heard Fin\\'s call and shone its warm light on the shore. The crab started to feel better and not so cold. He saw Fin and said, \"Thank you, little fish, for making me feel fine. I don\\'t feel like I will freeze now. Let\\'s play together!\" And so, Fin and the crab played and became good friends.\\n<|endoftext|>\\nOnce upon a time, in a land full of trees, there was a little cherry tree. The cherry tree was very sad because it did not have any friends. All the other trees were big and strong, but the cherry tree was small and weak. The cherry tree was envious of the big trees.\\nOne day, the cherry tree felt a tickle in its branches. It was a little spring wind. The wind told the cherry tree not to be sad. The wind said, \"You are special because you have sweet cherries that everyone loves.\" The cherry tree started to feel a little better.\\nAs time went on, the cherry tree grew more and more cherries. All the animals in the land came to eat the cherries and play under the cherry tree. The cherry tree was happy because it had many friends now. The cherry tree learned that being different can be a good thing. And they all lived happily ever after.\\n<|endoftext|>\\nOnce upon a time, there was a little girl named Lily. Lily liked to pretend she was a popular princess. She lived in a big castle with her best friends, a cat and a dog.\\nOne day, while playing in the castle, Lily found a big cobweb. The cobweb was in the way of her fun game. She wanted to get rid of it, but she was scared of the spider that lived there.\\nLily asked her friends, the cat and the dog, to help her. They all worked together to clean the cobweb. The spider was sad, but it found a new home outside. Lily, the cat, and the dog were happy they could play without the cobweb in the way. And they all lived happily ever after.\\n<|endoftext|>\\nOnce upon a time, in a big lake, there was a brown kayak. The brown kayak liked to roll in the water all day long. It was very happy when it could roll and splash in the lake.\\nOne day, a little boy named Tim came to play with the brown kayak. Tim and the brown kayak rolled in the water together. They laughed and had a lot of fun. The sun was shining, and the water was warm.\\nAfter a while, it was time for Tim to go home. He said goodbye to the brown kayak and gave it a big hug. The brown kayak was sad to see Tim go, but it knew they would play together again soon. So, the brown kayak kept rolling in the water, waiting for the next fun day with Tim.\\n<|endoftext|>\\nOnce upon a time, in a small town, there was a troubled little girl named Lily. She was always sad because she lost her favorite toy, a triangle. She looked everywhere in her house but could not find it.\\nOne sunny day, Lily went to the park to play. She saw a big puddle of water and thought her triangle might be there. She put her hand in the water to soak it and looked for her toy. She felt something at the bottom of the puddle.\\nLily pulled it out and saw that it was her triangle! She was so happy that she found it. From that day on, Lily was never troubled again. She played with her triangle every day and always kept it close to her. And when she saw puddles, she would smile and remember how she found her toy.\\n<|endoftext|>\\nOnce upon a time, in a peaceful town, there lived a little boy named Tim. Tim loved to run and play outside. One day, Tim saw a race in the park. He was excited and wanted to join the race.\\nTim went to his friend, Sarah, and said, \"Let\\'s start the race!\" Sarah smiled and said, \"Yes, let\\'s go!\" They lined up with the other kids and waited for the race to begin. When they heard the word \"Go!\", they started running as fast as they could.\\nTim and Sarah ran with all their speed, laughing and having fun. They could feel the wind in their hair as they raced to the finish line. In the end, Tim won the race and Sarah came in second. They were both so happy and proud of themselves. They celebrated with their friends and had a great day at the park.\\n<|endoftext|>\\nOnce upon a time, there was a clever little dog named Max. Max loved to run and play with his friends in the park. One day, Max was running very fast when he fell and hurt his knee.\\nMax went to his friend, the wise old owl, and said, \"Owl, my knee hurts. What can I do?\" The owl thought for a moment and said, \"Max, you should test your knee. Try to walk slowly and see if it still hurts.\"\\nSo Max tested his knee by walking slowly. At first, it hurt a little, but soon Max felt better. He said, \"Thank you, Owl, for your help. Now I can play with my friends again.\"\\nMax was so happy that he could play with his friends without pain. He learned that sometimes, it was good to slow down and listen to his body. And Max and his friends played happily in the park ever after.\\n<|endoftext|>\\nOne day, a fast driver named Tim went for a ride in his loud car. He loved to speed down the street and feel the wind in his hair. As he drove, he saw his friend, Sam, standing by the road.\\n\"Hi, Sam!\" Tim called out. \"Do you want to go for a ride?\"\\n\"Yes, please!\" Sam said, and he got in the car. They drove around the town, going fast and having fun. The car was very loud, and everyone could hear them coming.\\nAt last, they stopped at the park to play. They ran and laughed until it was time to go home. Tim and Sam had a great day together, speeding in the loud car and playing in the park.\\n<|endoftext|>\\nOnce upon a time, there was a big car named Dependable. He had a very important job. Dependable would take a family to the park every day. The family had a mom, dad, and a little girl named Lily. They all had a lot of love for each other.\\nOne day, when they got to the park, they saw a big sign that said, \"Fun Race Today!\" The family was very excited. They knew that Dependable was very fast and could win the race. So, they decided to join the race.\\nThe race started, and Dependable went very fast. The other cars tried to catch up, but Dependable was too quick. In the end, Dependable won the race! The family was so happy and proud of their car. They knew that their love for each other and their trust in Dependable made them win the race. And from that day on, they had even more fun at the park, knowing that they had the fastest and most dependable car around.\\n<|endoftext|>\\nOnce upon a time, there was a boy named Tim. He liked to wear a big, dark hat. The hat was his favorite thing to wear. Tim wore the hat everywhere he went.\\nOne day, Tim found a pencil on the ground. The pencil was small and yellow. Tim liked the pencil a lot. He put the pencil in his hat and took it with him.\\nTim drew pictures with the pencil. He drew a sun, a tree, and a cat. Tim was very happy with his new pencil. He wore his dark hat and drew pictures every day.\\n<|endoftext|>\\nOne day, a girl named Mia went for a walk. She saw a big, scary house. It had a tall door and small windows. Mia was brave, so she went inside the house.\\nIn the house, Mia saw a birdcage. Inside the birdcage, there was a little bird. The bird was sad. It wanted to fly and be free. Mia wanted to help the bird.\\nMia opened the birdcage door. The bird flew out and was happy. It was not scary anymore. Mia and the bird were friends. They played and had fun all day.\\n<|endoftext|>\\nOnce upon a time, in a small house, there lived a little girl named Amy. Amy was very sleepy. She put on her pajamas and went to bed.\\nIn the middle of the night, Amy heard a soft sound. She opened her eyes and saw a friendly ghost. The ghost said, \"Hello, Amy! I am here to help you.\"\\nAmy asked the ghost, \"Can you help me pick a dream?\" The ghost smiled and said, \"Of course! Let\\'s pick a happy dream for you.\" Together, they picked a dream about playing in a big park with lots of friends.\\nAmy said, \"Thank you, ghost!\" The ghost smiled and said, \"You\\'re welcome. Now, go back to sleep and enjoy your dream.\" Amy closed her eyes, feeling happy and safe with her new friend.\\nAnd so, Amy had a wonderful dream, thanks to the kind ghost. From that night on, the ghost would always visit Amy and help her pick the best dreams, and they became the best of friends.\\n<|endoftext|>\\nOnce upon a time, in a big forest, there was a tiny mushroom. It was all alone. The sun was very harsh, and the mushroom did not like it. It wanted to find a friend to play with and to help it hide from the sun.\\nOne day, a little bunny came hopping by. The mushroom called out, \"Hello, bunny! Will you be my friend?\" The bunny looked at the mushroom and smiled. \"Sure, I will be your friend. Let\\'s play together!\" The bunny and the mushroom played all day, and they were very happy.\\nAs they played, the bunny realized that the mushroom needed help to hide from the harsh sun. So, the bunny dug a hole in the ground and put the mushroom inside. Now, the mushroom was safe and cool. The mushroom and the bunny were the best of friends, and they played in the forest every day.\\n<|endoftext|>\\nOnce upon a time, there was a queen. She was a very nice queen. She had a big, pretty castle. The queen had a lot of work to do every day. But today, she wanted to relax.\\nThe queen went to the park to relax. She sat on a soft, green grass. The queen saw a bug. The bug was disgusting. The queen did not like the disgusting bug.\\nThe queen went back to her castle. She was happy to be away from the disgusting bug. Now, the queen could relax in her big, pretty castle. The queen smiled and had a good day.\\n<|endoftext|>\\nOnce upon a time, there was a little white cat named Fluffy. Fluffy loved to play with her best friend, a small boy named Timmy. They played outside in the sun every day. Fluffy liked to chase Timmy, and Timmy liked to run.\\nOne day, Timmy learned a new word at school. He wanted to teach Fluffy the word too. Timmy said, \"Fluffy, the word is \\'repeat\\'. Can you say \\'repeat\\'?\" Fluffy looked at Timmy and said, \"Meow.\" Timmy laughed and said, \"No, Fluffy, say \\'repeat\\'.\"\\nFluffy tried again and said, \"Meow-peat.\" Timmy clapped his hands and said, \"Good job, Fluffy! You said the word!\" Fluffy was very happy. She liked learning new words with Timmy. From that day on, Fluffy and Timmy played a game where they would teach each other new words. They had lots of fun together, and they lived happily ever after.\\n<|endoftext|>\\nOnce upon a time, there was a big octopus. He lived in the deep blue sea. He had a friend, a reliable fish. They played together every day.\\nOne day, the octopus and the fish found a big jug. They wanted to pour water on their friends for fun. But they couldn\\'t agree on who would pour the water. The octopus said, \"I want to pour the water!\" The fish said, \"No, I want to pour the water!\" They were not happy.\\nThen, the octopus had an idea. He said, \"Let\\'s both pour the water!\" The fish liked the idea. They picked up the big jug together and poured water on all their friends. Everyone laughed and had fun. The octopus and the fish were happy again. They learned to share and play together. And they lived happily ever after.\\n<|endoftext|>\\nOne day, a silly cat named Tom found a hoop in the yard. He wanted to play with it, but it was too big. Tom thought very hard about how to make the hoop smaller.\\nTom had an idea. He would stretch the hoop to make it smaller. He put his paws on the hoop and pulled as hard as he could. The hoop started to stretch! It got smaller and smaller.\\nNow, the hoop was just the right size for Tom to play with. He jumped through the hoop and chased it around the yard. Tom had so much fun playing with his new toy. And that is the story of the silly cat and the hoop.\\n<|endoftext|>\\nOnce upon a time, there was a polite crab. The crab lived in the sea. The crab had many friends.\\nOne day, the crab met a new friend. The new friend did not know how to play. The crab wanted to help the new friend.\\nThe polite crab showed the new friend how to play. They played all day. They had lots of fun. The new friend was happy. The polite crab was happy too. They were best friends forever.\\n<|endoftext|>\\nOnce, there was a boy named Tim. Tim liked to fish. He had a long pole to catch fish. One day, his friend Sam came to play. Sam saw the pole and asked, \"Can you lend me your pole?\" Tim said, \"Yes, but be careful!\"\\nSam took the pole and went to the river. He tried to catch a fish. He saw a big, yummy fish. He wanted to catch it. He pulled the pole very hard. But the pole broke! Sam felt sad.\\nSam went back to Tim with the broken pole. He said, \"I am sorry, I broke your pole.\" Tim was very sad. Now, he could not catch yummy fish. They both learned to be more careful with their things.\\n<|endoftext|>\\nOnce upon a time, in a small house, there lived a boy named Tim. Tim was a selfish boy. He did not like to share his things with others. One day, his mom brought home a big bag of vegetables. She wanted to explain to Tim why it was important to share.\\n\"Tim,\" said Mom, \"we need to share these vegetables with our neighbors. It is not good to be selfish.\" Tim did not want to share the vegetables. He wanted to keep them all for himself. But Mom told him, \"If you share, you will make others happy.\"\\nTim thought about it and then said, \"No, I don\\'t want to share.\" So, he took the vegetables and hid them in his room. The neighbors were sad because they had no food to eat. They knocked on Tim\\'s door and asked if he had any food to share. Tim did not want to share, so he said no.\\nThat night, Tim ate all the vegetables by himself. He felt very full and sick. He wished he had listened to his mom and shared with the neighbors. But now, it was too late. The vegetables were gone, and Tim felt very bad.\\n<|endoftext|>\\nOnce upon a time, there was a girl named Sue. Sue had long hair that would remind her of a rainbow. She would play with her hair all day.\\nOne day, Sue saw a slow snail in her garden. The snail had a pretty shell. Sue wanted to be friends with the snail. She named the snail Sam.\\nSue and Sam played together every day. They would race, but Sam was always slow. Sue would laugh and clap her hands. Sue and Sam were very happy.\\n<|endoftext|>\\nOnce upon a time, there was a helpful girl named Lily. She loved to read books and spend time with her friends. One day, she found a new book at school. The book was about a kind bear who helped everyone in the forest.\\nLily read the book to her friends. They all liked the story. They wanted to be like the kind bear in the book. So, they decided to help others too. They helped their teacher clean the classroom. They also helped their friends when they fell down or needed a friend to play with.\\nAs they spent more time being helpful, Lily and her friends felt happy. They learned that being kind and helpful to others made them happy too. The moral of the story is to be kind and helpful, just like the kind bear in the book.\\n<|endoftext|>\\nOnce upon a time, there was a little boy named Tim. Tim wanted to build something special for his mom\\'s birthday. He thought and thought about what to make. Then, he had an idea! He would build a big present for her.\\nTim was worried. He didn\\'t know if he could build the present all by himself. He asked his dad for help. Together, they found some big boxes, pretty paper, and a big bow. They worked together to make the present look nice.\\nWhen Tim\\'s mom saw the present, she was so happy! She gave Tim a big hug and thanked him for the special gift. Tim felt proud that he could build something so nice for his mom. And he was no longer worried.\\n<|endoftext|>\\nOne day, a big whale was swimming in the deep blue sea. The whale was very mysterious. It had a big smile on its face and liked to play with the little fish.\\nThe mysterious whale loved to relax in the warm water. It would lay on its back and let the waves rock it like a baby. The little fish would swim around the whale, and they all felt happy and safe.\\nOne day, the mysterious whale and the little fish found a beautiful place to relax. It was a quiet spot with lots of colorful plants and pretty shells. They all had a fun day, playing and relaxing together in their new secret place.\\n<|endoftext|>\\nOnce upon a time, there was a smelly old tree. By the tree, there was a big hole. In the hole, there was a shiny coin. A boy named Tim saw the coin and wanted it.\\nTim said to his friend, \"I want that coin, but the tree is smelly and rot.\" His friend said, \"Be brave, Tim! You can get the coin.\" So, Tim went to the smelly tree and tried to get the coin from the hole.\\nAs Tim got closer to the hole, he saw a little mouse. The mouse said, \"I will help you get the coin, but you must help me too.\" Tim agreed, and they got the coin together. Tim was happy, and the mouse was happy too. And they both learned that even in smelly places, good things can happen.\\n<|endoftext|>\\nOnce upon a time, there was a little boy named Tim. Tim loved going to the park to play. One day, Tim went to the park with his mom and dad. He was very happy.\\nAt the park, Tim saw a big tree. He wanted to give the tree a hug. So, he hugged the tree and felt good. Tim liked the tree a lot. He played with his ball, ran around, and had a lot of fun.\\nTim had a successful day at the park. He played and laughed a lot. When it was time to go home, Tim felt tired but happy. He couldn\\'t wait to come back to the park again.\\n<|endoftext|>\\nOne sunny day, a pretty boat was on the water. The boat was red and blue. A girl named Lily and her dog, Spot, were on the boat. They liked to play and have fun together.\\nLily saw a big fish jump out of the water. She said, \"Wow, Spot! Did you see that big fish?\" Spot barked and wagged his tail. They both laughed and clapped their hands.\\nLily picked up a hose and started to spray water. She sprayed the water on the boat and on Spot. Spot jumped and barked, trying to catch the water. They played and sprayed water until the sun went down. Then, they went home, happy and wet.\\n<|endoftext|>\\nOnce upon a time, there was a girl named Lily. She had a flute that she loved to play. One day, she met a deaf cat who could not hear her play the flute. Lily wanted to help the cat hear her music.\\nLily had an idea. She would stretch a big string from her flute to the cat\\'s ear. The cat could feel the music through the string. The cat was happy and started to dance.\\nLily and the deaf cat became best friends. They played and danced together every day. The cat loved to feel the music from Lily\\'s flute. They were happy and had lots of fun.\\n<|endoftext|>\\nOnce upon a time, there was a boy named Tom. He loved gum. One day, he found a thin piece of gum on the ground. He thought it was very special. He put the gum in his pocket to save it for later.\\nTom went to play with his friend, Sam. They played a game where they had to escape from a pretend monster. They ran and hid, but the monster always found them. Tom felt scared, but he remembered his special gum. He thought it could help them win the game.\\nTom took out the thin gum and shared it with Sam. They both chewed the gum and started to run from the monster again. But this time, they did not escape. The gum made them feel sick and slow. The monster caught them, and they lost the game. Tom and Sam were very sad. They learned never to pick up gum from the ground again.\\n<|endoftext|>\\nOnce upon a time, there was a little boy named Tim. Tim had a favorite vest that he loved to wear. It was a pretty vest with many colors. He wore it all the time.\\nOne day, it started to rain when Tim was playing outside. The rain made Tim and his vest very wet. Tim did not like being wet, so he went back inside his house. His mom saw his wet vest and told him to take it off.\\nTim took off his wet vest and started to weep. He was sad because his favorite vest was wet. His mom saw him weep and hugged him. She told him not to worry, she would dry the vest. Tim felt better and played with his toys while he waited for his vest to dry.\\n<|endoftext|>\\nOnce upon a time, there was a little boat. The boat was blue and it liked to float on the water. One day, the sun was very hot and the water began to dry up. The boat was sad because it could not float anymore.\\nA bird saw the boat and asked, \"Why are you sad, little boat?\" The boat said, \"The water is dry and I cannot float. I miss the water.\" The bird wanted to help the boat, so it thought of an easy plan.\\nThe bird told the boat, \"I will find more water for you. Then you can float again!\" The boat was happy and thanked the bird. The bird flew away and soon found a big pond. It came back and told the boat about the pond.\\nTogether, the bird and the boat went to the big pond. The boat was able to float again and it was very happy. The bird and the boat played together in the water all day long. And they became the best of friends.\\n<|endoftext|>\\nOnce upon a time, there was an old lady who lived in a small house. She loved to mix things in her big pot. One day, she decided to mix some yummy soup for her lunch.\\nAs she mixed the soup, she saw some smoke come out of her pot. She knew that meant the soup was very hot. The old lady was very careful not to touch the smoke or the pot.\\nShe let the soup cool down and then she ate it. The old lady thought it was the best soup she ever had. She was very happy and full. From that day on, she always mixed yummy soup for her lunch.\\n<|endoftext|>\\nOnce upon a time, there was a little boy named Tim. Tim was very nervous. He had lost his toy razor. He loved to pretend to shave like his dad. Every day, he would play with his toy razor. But now, it was gone.\\nOne day, Tim went to the park with his mom. He thought maybe he left his toy razor there. He looked and looked, but he could not find it. Tim was very sad. He missed his toy razor so much. He wanted to find it and play with it again.\\nThen, Tim saw a girl playing with a toy. He looked closer and recognized it. It was his toy razor! Tim was so happy. He went to the girl and told her that it was his toy. The girl was nice and gave it back to him. After that, Tim was not nervous anymore. He had his toy razor back and could play with it again. And they all lived happily ever after.\\n<|endoftext|>\\nOnce upon a time, there was a big gray elephant named Elly. Elly was sad because she thought she was ugly. She had big ears and a long nose, and she felt different from the other animals.\\nOne day, a little bird named Benny flew to Elly. Benny saw that Elly was sad and asked, \"Why are you sad, Elly?\" Elly told Benny that she felt ugly and did not like her big ears and long nose. Benny looked at Elly and said, \"I think you are not ugly. You are special.\"\\nBenny promised to show Elly how her big ears and long nose made her special. They went to the river, and Benny asked Elly to use her long nose to spray water on the dry plants. Elly did and the plants grew happy and green. Then, Benny asked Elly to use her big ears to fan the hot animals. Elly did, and all the animals felt cool and happy.\\nElly learned that being different was not ugly. She was special and had her own way to help others. Elly and Benny became best friends, and they lived happily ever after.\\n<|endoftext|>\\nOnce upon a time, there was a little dog named Zigzag. Zigzag loved to run and play in the park. One day, Zigzag saw a big twisty slide. He wanted to try it, so he ran up to the slide.\\nZigzag twisted and turned as he went down the slide. He went so fast that he fell off at the end. He felt embarrassed because all his friends saw him fall. But his friends did not laugh at him. They ran to help Zigzag get up.\\nZigzag\\'s friends told him not to be embarrassed. They all tried the twisty slide, too. They all twisted and turned, just like Zigzag. They had so much fun playing together. And Zigzag learned that it\\'s okay to make mistakes, as long as you have good friends by your side.\\n<|endoftext|>\\nOnce upon a time, in a small house, there was a cat and a dog. They liked to play all day. One day, they saw a shiny chain on the floor. They both wanted it.\\nThe cat had an idea. She rubbed her head on the dog\\'s leg. The dog felt happy and closed his eyes. The cat took the chain and ran away. The dog felt sad and guilty.\\nLater, the cat felt bad. She went back to the dog and gave him the chain. They both played with the chain and were happy friends again.\\n<|endoftext|>\\nOne day, a little boy named Tim went to play outside. He saw a big box near the tree. Tim was very curious. He wanted to know what was inside. So, he went closer to the box and took a peek.\\nInside the box, Tim found a toy gun. He was so happy! He picked up the gun and started to play with it. He pretended to be a superhero, saving the world from bad guys. Tim ran around the yard, laughing and having fun.\\nAfter playing for a while, Tim started to feel hungry. He went back to his house and saw his mom in the kitchen. She had made some yummy cookies for him. Tim ate the cookies and told his mom about the toy gun and his adventures. His mom smiled and gave him a big hug.\\n<|endoftext|>\\nOne day, a boy named Tim found a big hammer in his toy box. He took the hammer and went to play in the deep mud outside. While he was playing, he saw a little bug stuck in the mud.\\n\"Help me, please!\" said the bug. Tim wanted to help, but he didn\\'t want to delay his playtime. The bug looked very sad, so Tim decided to help.\\nTim used his big hammer to make a path in the mud. The bug was able to crawl out and said, \"Thank you, Tim!\" Tim felt happy that he helped the bug. After that, he continued playing with his hammer, but now he also had a new friend to play with.\\n<|endoftext|>\\nOnce upon a time, in a small house, there was a big box. The box was empty. A little boy named Tim wanted to find something to put in the box. He started to search his room.\\nTim looked under his bed and found a soft toy. He pushed the toy into the box. Next, he searched in his closet and found a ball. He pushed the ball into the box too. Tim was happy to see his box filling up.\\nAfter a while, the box was full. Tim felt proud of his search. He showed his mom and dad the full box. They smiled and clapped. Tim had turned the empty box into a box filled with fun things.\\n<|endoftext|>\\nOnce upon a time, in a big, green park, there was a small, messy dog named Binky. Binky had a long, waggy tail and loved to play with his friends. One sunny day, Binky and his friends were playing catch with a big, red ball.\\nAs they played, Binky\\'s tail got caught in a bush. \"Ouch!\" he cried. Binky\\'s friends heard him cry and came running to help. \"What\\'s wrong, Binky?\" asked his friend, a cute, fluffy bunny named Lulu.\\n\"My tail is stuck!\" Binky said with a sad face. Lulu and the other friends tried to help Binky get his tail out of the bush. They pulled and pulled, but it was still stuck.\\nThen, a wise, old owl named Oliver flew down from a tall tree. \"Don\\'t cry, Binky,\" he said. \"I can help you get your tail out.\" Oliver used his sharp beak to carefully cut the bush and free Binky\\'s tail.\\nBinky was so happy! He wagged his tail and said, \"Thank you, Oliver!\" All the friends cheered and clapped. Then, they went back to playing with the big, red ball, laughing and having fun together.\\n<|endoftext|>\\nOnce upon a time, in a small town, there was a little boy named Tim. Tim loved to speak and play with his friends. One day, Tim found a big, pink gum on the ground. It looked amazing. Tim picked it up and put it in his pocket. He thought, \"I will show this to my friends later.\"\\nTim went to the park to play with his friends. He saw them playing and running. Tim took the gum out of his pocket and showed it to his friends. They all wanted to see the amazing gum. Tim told them, \"I found it on the ground. It looks amazing, right?\" His friends nodded and smiled.\\nLater, Tim and his friends sat under a big tree. They all looked at the gum and talked about how amazing it was. Then, they decided to share the gum. They broke it into small pieces and each took a piece. They chewed the gum and it was very yummy. They all laughed and spoke about how great the gum was. Tim was happy that he found the amazing gum and could share it with his friends.\\n<|endoftext|>\\nOnce upon a time, there was a little boy named Tim. Tim loved pumpkins. He loved their round shape and bright orange color. One day, Tim saw a small pumpkin in the garden. It was not clear if the pumpkin was ready to be picked.\\nTim asked his mom, \"Can I receive this pumpkin, please?\" His mom looked at the pumpkin and said, \"It is not ready yet. You must wait for it to grow bigger.\"\\nTim waited and watched the pumpkin grow. As it grew, he learned to be patient. He knew that good things come to those who wait. Finally, the day came when the pumpkin was big and ready to be picked.\\nTim\\'s mom said, \"Now you can receive the pumpkin, Tim.\" Tim was so happy! He had waited and now he had a big, beautiful pumpkin. The moral of the story is that patience is a good thing. When we wait for the right time, we can receive the best things in life.\\n<|endoftext|>\\nOnce upon a time, there was a little girl named Lily. She lived in a small house with her mom and dad. One day, her mom asked her to go to the park and play. Lily was very happy and ran to the park.\\nAt the park, Lily saw a wide tree. She went to the tree and saw a faucet. She turned the faucet and water came out. Lily was thirsty, so she drank some water. Then, she saw a little boy who was also thirsty. Lily said, \"Hi, I am Lily. Let\\'s be friends!\" The little boy smiled and said, \"Hi, I am Tom. Nice to meet you!\" They both drank water and played together.\\nLily and Tom played and had fun all day. They learned that it is good to share and help others. When they went home, they told their moms and dads about their new friend. Lily and Tom were happy because they learned that sharing and helping others makes everyone happy.\\n<|endoftext|>\\nOnce upon a time, there was a little flower named Bloom. Bloom lived in a big garden with many other flowers. Bloom had a goal to be the most beautiful flower in the garden.\\nOne sunny day, Bloom saw a dangerous bug coming near the flowers. Bloom was scared and said to the other flowers, \"Watch out! Dangerous bug is coming!\" The other flowers thanked Bloom for the warning, and they all hid from the bug.\\nAfter the bug left, the flowers were safe and happy. Because Bloom warned them, they all agreed that Bloom was the most beautiful flower in the garden. Bloom\\'s goal came true, and they all lived happily ever after.\\n<|endoftext|>\\nOnce upon a time, in a small town, there lived a humble girl named Lily. She loved to give and help others. One day, she saw an old lady who looked sad.\\nLily went to the lady and said, \"Why are you sad?\" The old lady said, \"I am hungry, and I have no food.\" Lily wanted to help her, so she ran home to get some food.\\nLily put some food on a plate and went back to the old lady. She gave the plate to the lady and said, \"Here, eat this.\" The old lady smiled and said, \"Thank you, Lily. You are very kind.\"\\nThe old lady was happy, and Lily felt good that she could help. From that day on, Lily always looked for ways to help others and share what she had. And everyone in the town loved the humble girl who loved to give.\\n<|endoftext|>\\nOnce upon a time, there was a little girl named Sue. Sue had a big bedroom with fancy things in it. She loved her room very much. One day, she saw a small flower in her room. The flower was trying to bloom.\\nSue said to the flower, \"Hello, little flower! Why are you trying to bloom in my bedroom?\" The flower said, \"I want to be a fancy flower in your room.\" Sue liked the idea, but she knew that flowers needed sun and water to grow.\\nSue tried to help the flower. She gave it water and put it near the window. But the flower did not get enough sun. It tried very hard to bloom, but it could not. The flower became sad and said, \"I am sorry, Sue. I cannot be a fancy flower in your room.\"\\nIn the end, the flower did not bloom. It stayed small and sad. Sue felt bad for the flower. She wished she could help it more. But she knew that flowers needed sun and water to grow, and her bedroom was not the best place for a flower to live.\\n<|endoftext|>\\nOnce upon a time, there was a little infant named Tim. He liked to play with his toys and watch the birds outside his window. Tim had a big, soft teddy bear that he loved very much.\\nOne day, Tim\\'s teddy bear was sad. Tim did not know why his teddy bear was sad. He tried to make his teddy bear happy by giving it a hug and a kiss.\\nTim decided to take his sad teddy bear outside to watch the birds with him. They sat under a big tree and watched the birds sing and fly. Soon, the teddy bear was not sad anymore. Tim and his teddy bear were very happy watching the birds together.\\n<|endoftext|>\\nOnce upon a time, in a small town, there was a little boy named Tim. Tim loved ice-cream so much. One day, he saw an ancient ice-cream truck. The truck was very old and slow, but it had a big sign that said \"Ice-Cream\".\\nTim went to the truck and said, \"Hi, can I have ice-cream, please?\" The ice-cream man looked at him and said, \"Sure, little boy. Here you go.\" Tim took the ice-cream and started to eat it. But it did not taste good at all. It tasted very bad.\\nTim screamed, \"Yuck! This ice-cream is bad!\" The ice-cream man just laughed and said, \"I am sorry, little boy. The ice-cream is too old.\" Tim was very sad and walked away. He did not get to enjoy his ice-cream that day.\\n<|endoftext|>', 'Once upon a time, there were two best friends, Joe and Grace. They were always playing together and having so much fun. One day, Joe and Grace decided to go to the town\\'s marketplace.\\nWhen they got there, they saw a vendor with lots of colorful balloons. Joe wanted one so badly, but he didn\\'t have enough money. Grace saw how sad Joe was and wanted to help. She asked her mom to bring some money so Joe could buy a balloon.\\nWhen Grace\\'s mom arrived she saw that Joe wanted a deep red balloon. Joe was so relieved to be able to buy it after his friend helped him. Grace said to Joe, \"Remember, if you help a friend in need, you will always be rewarded!\"\\nJoe and Grace smiled and hugged each other, walked away with the deep red balloon, proud of the lesson they had learnt.\\n<|endoftext|>']\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cf85d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = model.ModelArgs(expert_type=model.ExpertType(2),\n",
    "                             dim=512, inter_dim=1365, moe_inter_dim=256,\n",
    "                             kv_lora_rank=128, qk_nope_head_dim=32, qk_rope_head_dim=16, \n",
    "                             v_head_dim=32, \n",
    "                             n_routed_experts=16, n_layers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cc67c82-99c9-49e1-9489-4c7dada2c5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = model.ModelArgs(expert_type=model.ExpertType(2),\n",
    "                             dim=128, inter_dim=341, moe_inter_dim=64,\n",
    "                             kv_lora_rank=32, qk_nope_head_dim=8, qk_rope_head_dim=4, \n",
    "                             v_head_dim=8, \n",
    "                             n_routed_experts=16, n_layers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0679f0bd-282e-4f77-a5ae-a11ab1d9ec42",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = model.ModelArgs(expert_type=model.ExpertType(1),\n",
    "                             dim=128, inter_dim=341, moe_inter_dim=64,\n",
    "                             kv_lora_rank=32, qk_nope_head_dim=8, qk_rope_head_dim=4, \n",
    "                             v_head_dim=8, \n",
    "                             n_routed_experts=16, n_layers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f454f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = train.TrainingArgs(\n",
    "    model_args=model_args,\n",
    "    batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    max_steps=100000,\n",
    "    learning_rate=3e-4,\n",
    "    checkpoint_dir=\"./checkpoints\",\n",
    "    data_dir=\"./data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77dae5e1-0c53-4cfb-9ff5-c4d5e657a97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 33,000,576\n",
      "Trainable parameters: 33,000,576\n",
      "Loading *tokenized* TinyStories dataset split from: './processed_tinystories/train'...\n",
      "Tokenized dataset loaded. Total chunks: 459757\n",
      "Loading *tokenized* TinyStories dataset split from: './processed_tinystories/validation'...\n",
      "Tokenized dataset loaded. Total chunks: 4619\n"
     ]
    }
   ],
   "source": [
    "trainer = train.Trainer(training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82d7e8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a11eb58d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '12355'\n",
    "dist.init_process_group(backend='nccl', rank=0, world_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b86b86e5-a3cd-4ee3-ac33-52f0b0f382e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, asdict\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "00597c75-9b8e-4253-8be0-6623ae96be82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for step, batch in enumerate(trainer.train_loader):\n",
    "    count += 1\n",
    "    if count > 4:\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0db892e5-2483-4ef3-8a5e-e83c3a1911d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1024])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a0825809-1f27-4a13-89e4-239eb25ed312",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.randint(0, 10000, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "798f7ebf-1520-4b89-876f-d4b9d6d5bd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = batch\n",
    "x = x.to(trainer.device, non_blocking=True)\n",
    "y = y.to(trainer.device, non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a1dd78d0-e9cb-4dba-823a-6e5f2071b79c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[60]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      1\u001b[39m logits = trainer.model(x, start_pos=\u001b[32m0\u001b[39m)\n\u001b[32m      3\u001b[39m loss = F.cross_entropy(\n\u001b[32m      4\u001b[39m                 logits.view(-\u001b[32m1\u001b[39m, logits.size(-\u001b[32m1\u001b[39m)),\n\u001b[32m      5\u001b[39m                 y.view(-\u001b[32m1\u001b[39m),\n\u001b[32m      6\u001b[39m                 ignore_index=-\u001b[32m1\u001b[39m\n\u001b[32m      7\u001b[39m             )\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/llm/lib/python3.12/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/llm/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/llm/lib/python3.12/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "logits = trainer.model(x, start_pos=0)\n",
    "\n",
    "loss = F.cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)),\n",
    "                y.view(-1),\n",
    "                ignore_index=-1\n",
    "            )\n",
    "\n",
    "loss.backward(retain_graph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "49540cc7-8e7a-4eb0-a896-2c2b5f15ec8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = F.cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)),\n",
    "                y.view(-1),\n",
    "                ignore_index=-1\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9cd151d4-3f47-4cb7-ae39-64021879eb2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/llm/lib/python3.12/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/llm/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/llm/lib/python3.12/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff01bd6f-6e24-4b3f-89cb-18b22881b702",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def enum_to_value(obj):\n",
    "    if isinstance(obj, Enum):\n",
    "        return obj.value\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: enum_to_value(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [enum_to_value(v) for v in obj]\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c330e33-890e-4a06-92af-ac000ab30b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 1263,    13, 40355,  ...,   465,   835,   290],\n",
      "        [  284, 13502,  2405,  ...,   635, 11040,    13],\n",
      "        [  356,   460,   526,  ...,   257,  7812,  7838],\n",
      "        [   11,   644,   460,  ...,   284,  1441,   262]]), tensor([[ 1263,    13, 40355,  ...,   465,   835,   290],\n",
      "        [  284, 13502,  2405,  ...,   635, 11040,    13],\n",
      "        [  356,   460,   526,  ...,   257,  7812,  7838],\n",
      "        [   11,   644,   460,  ...,   284,  1441,   262]])]\n"
     ]
    }
   ],
   "source": [
    "for step, batch in enumerate(trainer.train_loader):\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40ce698d-5631-493f-af49-b6f9d913e269",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = batch\n",
    "x = x.to(trainer.device, non_blocking=True)\n",
    "y = y.to(trainer.device, non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b257830e-f70f-44d5-a13a-2ac4c7672617",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mlogits\u001b[49m.shape\n",
      "\u001b[31mNameError\u001b[39m: name 'logits' is not defined"
     ]
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "418c1d04-2cbf-4e54-affe-cb0dfce3a650",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits1 = trainer.model(x, start_pos=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8bf8d128-c5fe-461b-b2c4-1828c64add4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss1 = F.cross_entropy(\n",
    "                logits1.view(-1, logits1.size(-1)),\n",
    "                y.view(-1),\n",
    "                ignore_index=-1\n",
    "            )\n",
    "\n",
    "loss1.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e0820a8-0fd4-448e-ac1b-7dc5dee595c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits2 = trainer.model(x, start_pos=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52e158dc-e744-487c-b57b-71d0597c71f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m loss2 = F.cross_entropy(\n\u001b[32m      2\u001b[39m                 logits2.view(-\u001b[32m1\u001b[39m, logits2.size(-\u001b[32m1\u001b[39m)),\n\u001b[32m      3\u001b[39m                 y.view(-\u001b[32m1\u001b[39m),\n\u001b[32m      4\u001b[39m                 ignore_index=-\u001b[32m1\u001b[39m\n\u001b[32m      5\u001b[39m             )\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mloss2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/llm/lib/python3.12/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/llm/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/llm/lib/python3.12/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "loss2 = F.cross_entropy(\n",
    "                logits2.view(-1, logits2.size(-1)),\n",
    "                y.view(-1),\n",
    "                ignore_index=-1\n",
    "            )\n",
    "\n",
    "loss2.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "103b6f98-f6ac-4ad9-a909-757c743b4d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss1 = F.cross_entropy(\n",
    "                logits1.view(-1, logits1.size(-1)),\n",
    "                y.view(-1),\n",
    "                ignore_index=-1\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c82d0631-c2c2-4ed0-98f2-72e95e19f086",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (embed): ParallelEmbedding()\n",
       "  (layers): ModuleList(\n",
       "    (0): Block(\n",
       "      (attn): MLA(\n",
       "        (wq): ColumnParallelLinear()\n",
       "        (wkv_a): Linear()\n",
       "        (kv_norm): RMSNorm()\n",
       "        (wkv_b): ColumnParallelLinear()\n",
       "        (wo): RowParallelLinear()\n",
       "      )\n",
       "      (ffn): MLP(\n",
       "        (w1): ColumnParallelLinear()\n",
       "        (w2): RowParallelLinear()\n",
       "        (w3): ColumnParallelLinear()\n",
       "      )\n",
       "      (ffn_norm): RMSNorm()\n",
       "      (attn_norm): RMSNorm()\n",
       "    )\n",
       "    (1-3): 3 x Block(\n",
       "      (attn): MLA(\n",
       "        (wq): ColumnParallelLinear()\n",
       "        (wkv_a): Linear()\n",
       "        (kv_norm): RMSNorm()\n",
       "        (wkv_b): ColumnParallelLinear()\n",
       "        (wo): RowParallelLinear()\n",
       "      )\n",
       "      (ffn): MoE(\n",
       "        (gate): Gate()\n",
       "        (experts): ModuleList(\n",
       "          (0-15): 16 x Expert(\n",
       "            (w1): Linear()\n",
       "            (w2): Linear()\n",
       "            (w3): Linear()\n",
       "          )\n",
       "        )\n",
       "        (shared_experts): MLP(\n",
       "          (w1): ColumnParallelLinear()\n",
       "          (w2): RowParallelLinear()\n",
       "          (w3): ColumnParallelLinear()\n",
       "        )\n",
       "      )\n",
       "      (ffn_norm): RMSNorm()\n",
       "      (attn_norm): RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm): RMSNorm()\n",
       "  (head): ColumnParallelLinear()\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcf78ba-8b33-4d97-8ff8-e66e93ac19b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a0d55f-e649-44e2-afb0-5942ff819292",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9268ca1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 33,000,576\n",
      "Trainable parameters: 33,000,576\n",
      "Loading *tokenized* TinyStories dataset split from: './processed_tinystories/train'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/projects/llm-models/llm-models/models/deepseek/train.py:165: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(enabled=self.args.use_amp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized dataset loaded. Total chunks: 459757\n",
      "Loading *tokenized* TinyStories dataset split from: './processed_tinystories/validation'...\n",
      "Tokenized dataset loaded. Total chunks: 4619\n"
     ]
    }
   ],
   "source": [
    "trainer = train.Trainer(training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b554b58-4a1e-4f42-a509-e80d6d944445",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# trainer_model = model.Transformer(trainer.args.model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b8d7efc-a3ed-4b34-b1fc-e3a51be6c80a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (embed): ParallelEmbedding()\n",
       "  (layers): ModuleList(\n",
       "    (0): Block(\n",
       "      (attn): MLA(\n",
       "        (wq): ColumnParallelLinear()\n",
       "        (wkv_a): Linear()\n",
       "        (kv_norm): RMSNorm()\n",
       "        (wkv_b): ColumnParallelLinear()\n",
       "        (wo): RowParallelLinear()\n",
       "      )\n",
       "      (ffn): MLP(\n",
       "        (w1): ColumnParallelLinear()\n",
       "        (w2): RowParallelLinear()\n",
       "        (w3): ColumnParallelLinear()\n",
       "      )\n",
       "      (ffn_norm): RMSNorm()\n",
       "      (attn_norm): RMSNorm()\n",
       "    )\n",
       "    (1-3): 3 x Block(\n",
       "      (attn): MLA(\n",
       "        (wq): ColumnParallelLinear()\n",
       "        (wkv_a): Linear()\n",
       "        (kv_norm): RMSNorm()\n",
       "        (wkv_b): ColumnParallelLinear()\n",
       "        (wo): RowParallelLinear()\n",
       "      )\n",
       "      (ffn): MoE(\n",
       "        (gate): Gate()\n",
       "        (experts): ModuleList(\n",
       "          (0-15): 16 x Expert(\n",
       "            (w1): Linear()\n",
       "            (w2): Linear()\n",
       "            (w3): Linear()\n",
       "          )\n",
       "        )\n",
       "        (shared_experts): MLP(\n",
       "          (w1): ColumnParallelLinear()\n",
       "          (w2): RowParallelLinear()\n",
       "          (w3): ColumnParallelLinear()\n",
       "        )\n",
       "      )\n",
       "      (ffn_norm): RMSNorm()\n",
       "      (attn_norm): RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm): RMSNorm()\n",
       "  (head): ColumnParallelLinear()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eaa806ef-c4da-43e7-8e0d-94a2896bb0b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "selected index k out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtopk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n",
      "\u001b[31mRuntimeError\u001b[39m: selected index k out of range"
     ]
    }
   ],
   "source": [
    "torch.topk(scores, 6, dim=-1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c643bc3c-f014-4a88-b0fc-73af1cb6b58a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/projects/llm-models/llm-models/models/deepseek/train.py:218: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=self.args.use_amp, dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss\n",
      "loss\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/llm-models/llm-models/models/deepseek/train.py:290\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    287\u001b[39m     param_group[\u001b[33m'\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m'\u001b[39m] = lr\n\u001b[32m    289\u001b[39m \u001b[38;5;66;03m# Training step\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m290\u001b[39m loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    291\u001b[39m running_loss += loss\n\u001b[32m    293\u001b[39m \u001b[38;5;66;03m# Update weights after accumulation steps\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/llm-models/llm-models/models/deepseek/train.py:229\u001b[39m, in \u001b[36mTrainer.train_step\u001b[39m\u001b[34m(self, batch)\u001b[39m\n\u001b[32m    227\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    228\u001b[39m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m229\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/llm/lib/python3.12/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/llm/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/llm/lib/python3.12/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66bda053-d219-4393-9cb7-d0b4074d1fcf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AcceleratorError",
     "evalue": "CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAcceleratorError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m trainer.model.named_parameters():\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/llm-models/llm/lib/python3.12/site-packages/torch/nn/parameter.py:74\u001b[39m, in \u001b[36mParameter.__repr__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m__repr__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mParameter containing:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__repr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/llm-models/llm/lib/python3.12/site-packages/torch/_tensor.py:590\u001b[39m, in \u001b[36mTensor.__repr__\u001b[39m\u001b[34m(self, tensor_contents)\u001b[39m\n\u001b[32m    586\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    587\u001b[39m         Tensor.\u001b[34m__repr__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, tensor_contents=tensor_contents\n\u001b[32m    588\u001b[39m     )\n\u001b[32m    589\u001b[39m \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m590\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_tensor_str\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/llm-models/llm/lib/python3.12/site-packages/torch/_tensor_str.py:726\u001b[39m, in \u001b[36m_str\u001b[39m\u001b[34m(self, tensor_contents)\u001b[39m\n\u001b[32m    724\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad(), torch.utils._python_dispatch._disable_current_modes():\n\u001b[32m    725\u001b[39m     guard = torch._C._DisableFuncTorch()  \u001b[38;5;66;03m# noqa: F841\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m726\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_str_intern\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/llm-models/llm/lib/python3.12/site-packages/torch/_tensor_str.py:647\u001b[39m, in \u001b[36m_str_intern\u001b[39m\u001b[34m(inp, tensor_contents)\u001b[39m\n\u001b[32m    645\u001b[39m                     tensor_str = _tensor_str(\u001b[38;5;28mself\u001b[39m.to_dense(), indent)\n\u001b[32m    646\u001b[39m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m                     tensor_str = \u001b[43m_tensor_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layout != torch.strided:\n\u001b[32m    650\u001b[39m     suffixes.append(\u001b[33m\"\u001b[39m\u001b[33mlayout=\u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.layout))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/llm-models/llm/lib/python3.12/site-packages/torch/_tensor_str.py:379\u001b[39m, in \u001b[36m_tensor_str\u001b[39m\u001b[34m(self, indent)\u001b[39m\n\u001b[32m    375\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\n\u001b[32m    376\u001b[39m         \u001b[38;5;28mself\u001b[39m, indent, summarize, real_formatter, imag_formatter\n\u001b[32m    377\u001b[39m     )\n\u001b[32m    378\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m379\u001b[39m     formatter = _Formatter(\u001b[43mget_summarized_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m summarize \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    380\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[38;5;28mself\u001b[39m, indent, summarize, formatter)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/llm-models/llm/lib/python3.12/site-packages/torch/_tensor_str.py:415\u001b[39m, in \u001b[36mget_summarized_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    413\u001b[39m     start = [\u001b[38;5;28mself\u001b[39m[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, PRINT_OPTS.edgeitems)]\n\u001b[32m    414\u001b[39m     end = [\u001b[38;5;28mself\u001b[39m[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) - PRINT_OPTS.edgeitems, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m))]\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.stack([\u001b[43mget_summarized_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m (start + end)])\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    417\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.stack([get_summarized_data(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/llm-models/llm/lib/python3.12/site-packages/torch/_tensor_str.py:405\u001b[39m, in \u001b[36mget_summarized_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    403\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dim == \u001b[32m1\u001b[39m:\n\u001b[32m    404\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.size(\u001b[32m0\u001b[39m) > \u001b[32m2\u001b[39m * PRINT_OPTS.edgeitems:\n\u001b[32m--> \u001b[39m\u001b[32m405\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m            \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mPRINT_OPTS\u001b[49m\u001b[43m.\u001b[49m\u001b[43medgeitems\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[43mPRINT_OPTS\u001b[49m\u001b[43m.\u001b[49m\u001b[43medgeitems\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    408\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    409\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[31mAcceleratorError\u001b[39m: CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "for name, param in trainer.model.named_parameters():\n",
    "    print(param)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffba93cf-b986-4805-b93d-640f10ae7bf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "273394814022960"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(trainer.optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665e74d4-d8b6-4858-9abd-b6035c5e580f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc992e6-7653-48f6-8eb1-d8d259a69c9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd79a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# --- Configuration ---\n",
    "MODEL_NAME = \"gpt2\"\n",
    "CONTEXT_LENGTH = 1024\n",
    "PROCESSED_DATA_DIR = \"./processed_tinystories\"\n",
    "\n",
    "def tokenize_and_group_dataset(split: str):\n",
    "    \"\"\"Loads a split, tokenizes it, and groups it into fixed-size chunks.\"\"\"\n",
    "    \n",
    "    # 1. Load Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "    # 2. Load Raw Dataset\n",
    "    raw_dataset = load_dataset(\"roneneldan/TinyStories\", split=split)\n",
    "\n",
    "    # 3. Tokenization\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"], truncation=False) # No truncation yet\n",
    "    \n",
    "    tokenized_datasets = raw_dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=[\"text\"],\n",
    "        num_proc=os.cpu_count(),\n",
    "    )\n",
    "\n",
    "    # 4. Grouping and Chunking\n",
    "    def group_texts(examples):\n",
    "        # Concatenate all lists of token IDs in the batch\n",
    "        concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "        total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "        \n",
    "        # Drop the last chunk\n",
    "        total_length = (total_length // CONTEXT_LENGTH) * CONTEXT_LENGTH\n",
    "        \n",
    "        # Split the concatenated list into chunks of CONTEXT_LENGTH\n",
    "        result = {\n",
    "            k: [t[i : i + CONTEXT_LENGTH] for i in range(0, total_length, CONTEXT_LENGTH)]\n",
    "            for k, t in concatenated_examples.items()\n",
    "        }\n",
    "        # For CLM, labels are the input IDs\n",
    "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "        return result\n",
    "\n",
    "    lm_dataset = tokenized_datasets.map(\n",
    "        group_texts,\n",
    "        batched=True,\n",
    "        num_proc=os.cpu_count(),\n",
    "    )\n",
    "    \n",
    "    # 5. Save the processed dataset\n",
    "    output_path = os.path.join(PROCESSED_DATA_DIR, split)\n",
    "    lm_dataset.save_to_disk(output_path)\n",
    "    print(f\"Processed dataset saved to: {output_path}\")\n",
    "\n",
    "    return lm_dataset\n",
    "\n",
    "# Run this block once to create the processed data files\n",
    "if not os.path.exists(PROCESSED_DATA_DIR):\n",
    "    os.makedirs(PROCESSED_DATA_DIR)\n",
    "    \n",
    "    print(\"\\n--- Starting Pre-processing for all splits ---\")\n",
    "    tokenize_and_group_dataset(\"train\")\n",
    "    tokenize_and_group_dataset(\"validation\")\n",
    "    # TinyStories also has a 'test' split if needed: tokenize_and_group_dataset(\"test\")\n",
    "    print(\"--- Pre-processing Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3aa0b0fe-4e36-4ec0-a40a-2d4858accf7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/projects/llm-models/llm/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2c61a58-c6de-43a2-93f3-74f94cc7ddb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'gpt2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "split = 'train'\n",
    "\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "# 2. Load Raw Dataset\n",
    "raw_dataset = load_dataset(\"roneneldan/TinyStories\", split=split)\n",
    "\n",
    "# 3. Tokenization\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=False) # No truncation yet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca4c8bbc-a0e8-4875-91cf-4cf7bb28926e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenized_datasets = raw_dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=[\"text\"],\n",
    "        num_proc=os.cpu_count(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30ea7c11-8c19-4a03-bbb2-080df53d78cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/projects/llm/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "MODEL_NAME = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "065dcc23-73fb-428c-a417-77ccbd5da200",
   "metadata": {},
   "outputs": [
    {
     "ename": "RemoteEntryNotFoundError",
     "evalue": "404 Client Error. (Request ID: Root=1-6939c3ad-41a5479f633cd8c13bea7ca3;2614ec0e-0f21-41dd-ae9f-1d4b9b16b6fb)\n\nEntry Not Found for url: https://huggingface.co/api/models/openai-community/gpt2/tree/main/additional_chat_templates?recursive=false&expand=false.\nadditional_chat_templates does not exist on \"main\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPStatusError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/llm/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:657\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/llm/lib/python3.12/site-packages/httpx/_models.py:829\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    828\u001b[39m message = message.format(\u001b[38;5;28mself\u001b[39m, error_type=error_type)\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request=request, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPStatusError\u001b[39m: Client error '404 Not Found' for url 'https://huggingface.co/api/models/openai-community/gpt2/tree/main/additional_chat_templates?recursive=false&expand=false'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRemoteEntryNotFoundError\u001b[39m                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m ds = \u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtokenize_and_group_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/llm-models/llm-models/models/deepseek/data.py:28\u001b[39m, in \u001b[36mtokenize_and_group_dataset\u001b[39m\u001b[34m(split)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Loads a split, tokenizes it, and groups it into fixed-size chunks.\"\"\"\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# 1. Load Tokenizer\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m tokenizer = \u001b[43mAutoTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tokenizer.pad_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     30\u001b[39m     tokenizer.pad_token = tokenizer.eos_token\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/llm/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:1175\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m   1172\u001b[39m tokenizer_class_py, tokenizer_class_fast = TOKENIZER_MAPPING[\u001b[38;5;28mtype\u001b[39m(config)]\n\u001b[32m   1174\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m1175\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class_fast\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1177\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/llm/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2039\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2035\u001b[39m             vocab_files[\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mchat_template_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemplate_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m] = (\n\u001b[32m   2036\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCHAT_TEMPLATE_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemplate_file.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   2037\u001b[39m             )\n\u001b[32m   2038\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2039\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m template \u001b[38;5;129;01min\u001b[39;00m \u001b[43mlist_repo_templates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2040\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2041\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2042\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2043\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2044\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2045\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   2046\u001b[39m         template = template.removesuffix(\u001b[33m\"\u001b[39m\u001b[33m.jinja\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2047\u001b[39m         vocab_files[\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mchat_template_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemplate\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m] = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCHAT_TEMPLATE_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemplate\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.jinja\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/llm/lib/python3.12/site-packages/transformers/utils/hub.py:167\u001b[39m, in \u001b[36mlist_repo_templates\u001b[39m\u001b[34m(repo_id, local_files_only, revision, cache_dir, token)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m local_files_only:\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m            \u001b[49m\u001b[43mentry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mremoveprefix\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mCHAT_TEMPLATE_DIR\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mentry\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlist_repo_tree\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43mpath_in_repo\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCHAT_TEMPLATE_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mentry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mendswith\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.jinja\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (GatedRepoError, RepositoryNotFoundError, RevisionNotFoundError):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m  \u001b[38;5;66;03m# valid errors => do not catch\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/llm/lib/python3.12/site-packages/huggingface_hub/hf_api.py:3156\u001b[39m, in \u001b[36mlist_repo_tree\u001b[39m\u001b[34m(self, repo_id, path_in_repo, recursive, expand, revision, repo_type, token)\u001b[39m\n\u001b[32m   3062\u001b[39m \u001b[38;5;129m@validate_hf_hub_args\u001b[39m\n\u001b[32m   3063\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlist_repo_tree\u001b[39m(\n\u001b[32m   3064\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3072\u001b[39m     token: Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   3073\u001b[39m ) -> Iterable[Union[RepoFile, RepoFolder]]:\n\u001b[32m   3074\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3075\u001b[39m \u001b[33;03m    List a repo tree's files and folders and get information about them.\u001b[39;00m\n\u001b[32m   3076\u001b[39m \n\u001b[32m   3077\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   3078\u001b[39m \u001b[33;03m        repo_id (`str`):\u001b[39;00m\n\u001b[32m   3079\u001b[39m \u001b[33;03m            A namespace (user or an organization) and a repo name separated by a `/`.\u001b[39;00m\n\u001b[32m   3080\u001b[39m \u001b[33;03m        path_in_repo (`str`, *optional*):\u001b[39;00m\n\u001b[32m   3081\u001b[39m \u001b[33;03m            Relative path of the tree (folder) in the repo, for example:\u001b[39;00m\n\u001b[32m   3082\u001b[39m \u001b[33;03m            `\"checkpoints/1fec34a/results\"`. Will default to the root tree (folder) of the repository.\u001b[39;00m\n\u001b[32m   3083\u001b[39m \u001b[33;03m        recursive (`bool`, *optional*, defaults to `False`):\u001b[39;00m\n\u001b[32m   3084\u001b[39m \u001b[33;03m            Whether to list tree's files and folders recursively.\u001b[39;00m\n\u001b[32m   3085\u001b[39m \u001b[33;03m        expand (`bool`, *optional*, defaults to `False`):\u001b[39;00m\n\u001b[32m   3086\u001b[39m \u001b[33;03m            Whether to fetch more information about the tree's files and folders (e.g. last commit and files' security scan results). This\u001b[39;00m\n\u001b[32m   3087\u001b[39m \u001b[33;03m            operation is more expensive for the server so only 50 results are returned per page (instead of 1000).\u001b[39;00m\n\u001b[32m   3088\u001b[39m \u001b[33;03m            As pagination is implemented in `huggingface_hub`, this is transparent for you except for the time it\u001b[39;00m\n\u001b[32m   3089\u001b[39m \u001b[33;03m            takes to get the results.\u001b[39;00m\n\u001b[32m   3090\u001b[39m \u001b[33;03m        revision (`str`, *optional*):\u001b[39;00m\n\u001b[32m   3091\u001b[39m \u001b[33;03m            The revision of the repository from which to get the tree. Defaults to `\"main\"` branch.\u001b[39;00m\n\u001b[32m   3092\u001b[39m \u001b[33;03m        repo_type (`str`, *optional*):\u001b[39;00m\n\u001b[32m   3093\u001b[39m \u001b[33;03m            The type of the repository from which to get the tree (`\"model\"`, `\"dataset\"` or `\"space\"`.\u001b[39;00m\n\u001b[32m   3094\u001b[39m \u001b[33;03m            Defaults to `\"model\"`.\u001b[39;00m\n\u001b[32m   3095\u001b[39m \u001b[33;03m        token (`bool` or `str`, *optional*):\u001b[39;00m\n\u001b[32m   3096\u001b[39m \u001b[33;03m            A valid user access token (string). Defaults to the locally saved\u001b[39;00m\n\u001b[32m   3097\u001b[39m \u001b[33;03m            token, which is the recommended method for authentication (see\u001b[39;00m\n\u001b[32m   3098\u001b[39m \u001b[33;03m            https://huggingface.co/docs/huggingface_hub/quick-start#authentication).\u001b[39;00m\n\u001b[32m   3099\u001b[39m \u001b[33;03m            To disable authentication, pass `False`.\u001b[39;00m\n\u001b[32m   3100\u001b[39m \n\u001b[32m   3101\u001b[39m \u001b[33;03m    Returns:\u001b[39;00m\n\u001b[32m   3102\u001b[39m \u001b[33;03m        `Iterable[Union[RepoFile, RepoFolder]]`:\u001b[39;00m\n\u001b[32m   3103\u001b[39m \u001b[33;03m            The information about the tree's files and folders, as an iterable of [`RepoFile`] and [`RepoFolder`] objects. The order of the files and folders is\u001b[39;00m\n\u001b[32m   3104\u001b[39m \u001b[33;03m            not guaranteed.\u001b[39;00m\n\u001b[32m   3105\u001b[39m \n\u001b[32m   3106\u001b[39m \u001b[33;03m    Raises:\u001b[39;00m\n\u001b[32m   3107\u001b[39m \u001b[33;03m        [`~utils.RepositoryNotFoundError`]:\u001b[39;00m\n\u001b[32m   3108\u001b[39m \u001b[33;03m            If repository is not found (error 404): wrong repo_id/repo_type, private but not authenticated or repo\u001b[39;00m\n\u001b[32m   3109\u001b[39m \u001b[33;03m            does not exist.\u001b[39;00m\n\u001b[32m   3110\u001b[39m \u001b[33;03m        [`~utils.RevisionNotFoundError`]:\u001b[39;00m\n\u001b[32m   3111\u001b[39m \u001b[33;03m            If revision is not found (error 404) on the repo.\u001b[39;00m\n\u001b[32m   3112\u001b[39m \u001b[33;03m        [`~utils.EntryNotFoundError`]:\u001b[39;00m\n\u001b[32m   3113\u001b[39m \u001b[33;03m            If the tree (folder) does not exist (error 404) on the repo.\u001b[39;00m\n\u001b[32m   3114\u001b[39m \n\u001b[32m   3115\u001b[39m \u001b[33;03m    Examples:\u001b[39;00m\n\u001b[32m   3116\u001b[39m \n\u001b[32m   3117\u001b[39m \u001b[33;03m        Get information about a repo's tree.\u001b[39;00m\n\u001b[32m   3118\u001b[39m \u001b[33;03m        ```py\u001b[39;00m\n\u001b[32m   3119\u001b[39m \u001b[33;03m        >>> from huggingface_hub import list_repo_tree\u001b[39;00m\n\u001b[32m   3120\u001b[39m \u001b[33;03m        >>> repo_tree = list_repo_tree(\"lysandre/arxiv-nlp\")\u001b[39;00m\n\u001b[32m   3121\u001b[39m \u001b[33;03m        >>> repo_tree\u001b[39;00m\n\u001b[32m   3122\u001b[39m \u001b[33;03m        <generator object HfApi.list_repo_tree at 0x7fa4088e1ac0>\u001b[39;00m\n\u001b[32m   3123\u001b[39m \u001b[33;03m        >>> list(repo_tree)\u001b[39;00m\n\u001b[32m   3124\u001b[39m \u001b[33;03m        [\u001b[39;00m\n\u001b[32m   3125\u001b[39m \u001b[33;03m            RepoFile(path='.gitattributes', size=391, blob_id='ae8c63daedbd4206d7d40126955d4e6ab1c80f8f', lfs=None, last_commit=None, security=None),\u001b[39;00m\n\u001b[32m   3126\u001b[39m \u001b[33;03m            RepoFile(path='README.md', size=391, blob_id='43bd404b159de6fba7c2f4d3264347668d43af25', lfs=None, last_commit=None, security=None),\u001b[39;00m\n\u001b[32m   3127\u001b[39m \u001b[33;03m            RepoFile(path='config.json', size=554, blob_id='2f9618c3a19b9a61add74f70bfb121335aeef666', lfs=None, last_commit=None, security=None),\u001b[39;00m\n\u001b[32m   3128\u001b[39m \u001b[33;03m            RepoFile(\u001b[39;00m\n\u001b[32m   3129\u001b[39m \u001b[33;03m                path='flax_model.msgpack', size=497764107, blob_id='8095a62ccb4d806da7666fcda07467e2d150218e',\u001b[39;00m\n\u001b[32m   3130\u001b[39m \u001b[33;03m                lfs={'size': 497764107, 'sha256': 'd88b0d6a6ff9c3f8151f9d3228f57092aaea997f09af009eefd7373a77b5abb9', 'pointer_size': 134}, last_commit=None, security=None\u001b[39;00m\n\u001b[32m   3131\u001b[39m \u001b[33;03m            ),\u001b[39;00m\n\u001b[32m   3132\u001b[39m \u001b[33;03m            RepoFile(path='merges.txt', size=456318, blob_id='226b0752cac7789c48f0cb3ec53eda48b7be36cc', lfs=None, last_commit=None, security=None),\u001b[39;00m\n\u001b[32m   3133\u001b[39m \u001b[33;03m            RepoFile(\u001b[39;00m\n\u001b[32m   3134\u001b[39m \u001b[33;03m                path='pytorch_model.bin', size=548123560, blob_id='64eaa9c526867e404b68f2c5d66fd78e27026523',\u001b[39;00m\n\u001b[32m   3135\u001b[39m \u001b[33;03m                lfs={'size': 548123560, 'sha256': '9be78edb5b928eba33aa88f431551348f7466ba9f5ef3daf1d552398722a5436', 'pointer_size': 134}, last_commit=None, security=None\u001b[39;00m\n\u001b[32m   3136\u001b[39m \u001b[33;03m            ),\u001b[39;00m\n\u001b[32m   3137\u001b[39m \u001b[33;03m            RepoFile(path='vocab.json', size=898669, blob_id='b00361fece0387ca34b4b8b8539ed830d644dbeb', lfs=None, last_commit=None, security=None)]\u001b[39;00m\n\u001b[32m   3138\u001b[39m \u001b[33;03m        ]\u001b[39;00m\n\u001b[32m   3139\u001b[39m \u001b[33;03m        ```\u001b[39;00m\n\u001b[32m   3140\u001b[39m \n\u001b[32m   3141\u001b[39m \u001b[33;03m        Get even more information about a repo's tree (last commit and files' security scan results)\u001b[39;00m\n\u001b[32m   3142\u001b[39m \u001b[33;03m        ```py\u001b[39;00m\n\u001b[32m   3143\u001b[39m \u001b[33;03m        >>> from huggingface_hub import list_repo_tree\u001b[39;00m\n\u001b[32m   3144\u001b[39m \u001b[33;03m        >>> repo_tree = list_repo_tree(\"prompthero/openjourney-v4\", expand=True)\u001b[39;00m\n\u001b[32m   3145\u001b[39m \u001b[33;03m        >>> list(repo_tree)\u001b[39;00m\n\u001b[32m   3146\u001b[39m \u001b[33;03m        [\u001b[39;00m\n\u001b[32m   3147\u001b[39m \u001b[33;03m            RepoFolder(\u001b[39;00m\n\u001b[32m   3148\u001b[39m \u001b[33;03m                path='feature_extractor',\u001b[39;00m\n\u001b[32m   3149\u001b[39m \u001b[33;03m                tree_id='aa536c4ea18073388b5b0bc791057a7296a00398',\u001b[39;00m\n\u001b[32m   3150\u001b[39m \u001b[33;03m                last_commit={\u001b[39;00m\n\u001b[32m   3151\u001b[39m \u001b[33;03m                    'oid': '47b62b20b20e06b9de610e840282b7e6c3d51190',\u001b[39;00m\n\u001b[32m   3152\u001b[39m \u001b[33;03m                    'title': 'Upload diffusers weights (#48)',\u001b[39;00m\n\u001b[32m   3153\u001b[39m \u001b[33;03m                    'date': datetime.datetime(2023, 3, 21, 9, 5, 27, tzinfo=datetime.timezone.utc)\u001b[39;00m\n\u001b[32m   3154\u001b[39m \u001b[33;03m                }\u001b[39;00m\n\u001b[32m   3155\u001b[39m \u001b[33;03m            ),\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3156\u001b[39m \u001b[33;03m            RepoFolder(\u001b[39;00m\n\u001b[32m   3157\u001b[39m \u001b[33;03m                path='safety_checker',\u001b[39;00m\n\u001b[32m   3158\u001b[39m \u001b[33;03m                tree_id='65aef9d787e5557373fdf714d6c34d4fcdd70440',\u001b[39;00m\n\u001b[32m   3159\u001b[39m \u001b[33;03m                last_commit={\u001b[39;00m\n\u001b[32m   3160\u001b[39m \u001b[33;03m                    'oid': '47b62b20b20e06b9de610e840282b7e6c3d51190',\u001b[39;00m\n\u001b[32m   3161\u001b[39m \u001b[33;03m                    'title': 'Upload diffusers weights (#48)',\u001b[39;00m\n\u001b[32m   3162\u001b[39m \u001b[33;03m                    'date': datetime.datetime(2023, 3, 21, 9, 5, 27, tzinfo=datetime.timezone.utc)\u001b[39;00m\n\u001b[32m   3163\u001b[39m \u001b[33;03m                }\u001b[39;00m\n\u001b[32m   3164\u001b[39m \u001b[33;03m            ),\u001b[39;00m\n\u001b[32m   3165\u001b[39m \u001b[33;03m            RepoFile(\u001b[39;00m\n\u001b[32m   3166\u001b[39m \u001b[33;03m                path='model_index.json',\u001b[39;00m\n\u001b[32m   3167\u001b[39m \u001b[33;03m                size=582,\u001b[39;00m\n\u001b[32m   3168\u001b[39m \u001b[33;03m                blob_id='d3d7c1e8c3e78eeb1640b8e2041ee256e24c9ee1',\u001b[39;00m\n\u001b[32m   3169\u001b[39m \u001b[33;03m                lfs=None,\u001b[39;00m\n\u001b[32m   3170\u001b[39m \u001b[33;03m                last_commit={\u001b[39;00m\n\u001b[32m   3171\u001b[39m \u001b[33;03m                    'oid': 'b195ed2d503f3eb29637050a886d77bd81d35f0e',\u001b[39;00m\n\u001b[32m   3172\u001b[39m \u001b[33;03m                    'title': 'Fix deprecation warning by changing `CLIPFeatureExtractor` to `CLIPImageProcessor`. (#54)',\u001b[39;00m\n\u001b[32m   3173\u001b[39m \u001b[33;03m                    'date': datetime.datetime(2023, 5, 15, 21, 41, 59, tzinfo=datetime.timezone.utc)\u001b[39;00m\n\u001b[32m   3174\u001b[39m \u001b[33;03m                },\u001b[39;00m\n\u001b[32m   3175\u001b[39m \u001b[33;03m                security={\u001b[39;00m\n\u001b[32m   3176\u001b[39m \u001b[33;03m                    'safe': True,\u001b[39;00m\n\u001b[32m   3177\u001b[39m \u001b[33;03m                    'av_scan': {'virusFound': False, 'virusNames': None},\u001b[39;00m\n\u001b[32m   3178\u001b[39m \u001b[33;03m                    'pickle_import_scan': None\u001b[39;00m\n\u001b[32m   3179\u001b[39m \u001b[33;03m                }\u001b[39;00m\n\u001b[32m   3180\u001b[39m \u001b[33;03m            )\u001b[39;00m\n\u001b[32m   3181\u001b[39m \u001b[33;03m            ...\u001b[39;00m\n\u001b[32m   3182\u001b[39m \u001b[33;03m        ]\u001b[39;00m\n\u001b[32m   3183\u001b[39m \u001b[33;03m        ```\u001b[39;00m\n\u001b[32m   3184\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   3185\u001b[39m     repo_type = repo_type \u001b[38;5;129;01mor\u001b[39;00m constants.REPO_TYPE_MODEL\n\u001b[32m   3186\u001b[39m     revision = quote(revision, safe=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m revision \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m constants.DEFAULT_REVISION\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/llm/lib/python3.12/site-packages/huggingface_hub/utils/_pagination.py:37\u001b[39m, in \u001b[36mpaginate\u001b[39m\u001b[34m(path, params, headers)\u001b[39m\n\u001b[32m     35\u001b[39m session = get_session()\n\u001b[32m     36\u001b[39m r = session.get(path, params=params, headers=headers)\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m r.json()\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Follow pages\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Next link already contains query params\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/llm/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:671\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[31mRemoteEntryNotFoundError\u001b[39m: 404 Client Error. (Request ID: Root=1-6939c3ad-41a5479f633cd8c13bea7ca3;2614ec0e-0f21-41dd-ae9f-1d4b9b16b6fb)\n\nEntry Not Found for url: https://huggingface.co/api/models/openai-community/gpt2/tree/main/additional_chat_templates?recursive=false&expand=false.\nadditional_chat_templates does not exist on \"main\""
     ]
    }
   ],
   "source": [
    "ds = data.tokenize_and_group_dataset('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de4dba0d-b99a-42d0-9f34-5e14ffc19a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_LENGTH = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58ad0d12-9a87-4b92-a9be-0f40d701e537",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|| 2119719/2119719 [12:34<00:00, 2807.67 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# 4. Grouping and Chunking\n",
    "def group_texts(examples):\n",
    "    # Concatenate all lists of token IDs in the batch\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    \n",
    "    # Drop the last chunk\n",
    "    total_length = (total_length // CONTEXT_LENGTH) * CONTEXT_LENGTH\n",
    "    \n",
    "    # Split the concatenated list into chunks of CONTEXT_LENGTH\n",
    "    result = {\n",
    "        k: [t[i : i + CONTEXT_LENGTH] for i in range(0, total_length, CONTEXT_LENGTH)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # For CLM, labels are the input IDs\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_dataset = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    num_proc=os.cpu_count(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d171e30e-8817-41ef-a574-37cd21a89a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_dataset = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    num_proc=os.cpu_count(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bed3a74-7266-413e-9678-327e6fa5fb3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\\n\\nLily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\\n\\nTogether, they shared the needle and sewed the button on Lily\\'s shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.'}\n",
      "{'text': 'Once upon a time, there was a little car named Beep. Beep loved to go fast and play in the sun. Beep was a healthy car because he always had good fuel. Good fuel made Beep happy and strong.\\n\\nOne day, Beep was driving in the park when he saw a big tree. The tree had many leaves that were falling. Beep liked how the leaves fall and wanted to play with them. Beep drove under the tree and watched the leaves fall on him. He laughed and beeped his horn.\\n\\nBeep played with the falling leaves all day. When it was time to go home, Beep knew he needed more fuel. He went to the fuel place and got more healthy fuel. Now, Beep was ready to go fast and play again the next day. And Beep lived happily ever after.'}\n",
      "{'text': 'One day, a little fish named Fin was swimming near the shore. He saw a big crab and wanted to be friends. \"Hi, I am Fin. Do you want to play?\" asked the little fish. The crab looked at Fin and said, \"No, I don\\'t want to play. I am cold and I don\\'t feel fine.\"\\n\\nFin felt sad but wanted to help the crab feel better. He swam away and thought of a plan. He remembered that the sun could make things warm. So, Fin swam to the top of the water and called to the sun, \"Please, sun, help my new friend feel fine and not freeze!\"\\n\\nThe sun heard Fin\\'s call and shone its warm light on the shore. The crab started to feel better and not so cold. He saw Fin and said, \"Thank you, little fish, for making me feel fine. I don\\'t feel like I will freeze now. Let\\'s play together!\" And so, Fin and the crab played and became good friends.'}\n",
      "{'text': 'Once upon a time, in a land full of trees, there was a little cherry tree. The cherry tree was very sad because it did not have any friends. All the other trees were big and strong, but the cherry tree was small and weak. The cherry tree was envious of the big trees.\\n\\nOne day, the cherry tree felt a tickle in its branches. It was a little spring wind. The wind told the cherry tree not to be sad. The wind said, \"You are special because you have sweet cherries that everyone loves.\" The cherry tree started to feel a little better.\\n\\nAs time went on, the cherry tree grew more and more cherries. All the animals in the land came to eat the cherries and play under the cherry tree. The cherry tree was happy because it had many friends now. The cherry tree learned that being different can be a good thing. And they all lived happily ever after.'}\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for text in raw_dataset:\n",
    "    print(text)\n",
    "    batch_data = text\n",
    "    i += 1\n",
    "    if i > 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "873ae25f-d84e-4c7d-8825-d1368e597e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    \"\"\"\n",
    "    Concatenates all texts and splits them into fixed-size chunks. \n",
    "    This is standard practice for CLM training (e.g., training a GPT-2 model).\n",
    "    \"\"\"\n",
    "    # Concatenate all lists of token IDs in the batch\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    \n",
    "    # Drop the last chunk if it's smaller than the context length\n",
    "    total_length = (total_length // CONTEXT_LENGTH) * CONTEXT_LENGTH\n",
    "    \n",
    "    # Split the concatenated list into chunks of CONTEXT_LENGTH\n",
    "    result = {\n",
    "        k: [t[i : i + CONTEXT_LENGTH] for i in range(0, total_length, CONTEXT_LENGTH)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    \n",
    "    # For Causal Language Modeling (CLM), the 'labels' are the 'input_ids'.\n",
    "    # The Hugging Face Trainer handles the required shifting for loss calculation.\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe05d39b-a259-452e-8db1-141e6e21897a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['text'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "800e9da9-1b68-460f-920e-8f07230cba23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyStoriesTokenizedDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset class that loads the *pre-processed*, tokenized, \n",
    "    and chunked TinyStories dataset from disk.\n",
    "    \n",
    "    It returns a dictionary suitable for a Hugging Face Trainer or custom PyTorch loop.\n",
    "    \"\"\"\n",
    "    def __init__(self, split: str = \"train\", processed_data_dir: str = \"./processed_tinystories\"):\n",
    "        \"\"\"\n",
    "        Initializes the dataset by loading the specified split from a local directory.\n",
    "\n",
    "        Args:\n",
    "            split (str): The dataset split to load ('train', 'validation').\n",
    "            processed_data_dir (str): The directory where the pre-processed data is saved.\n",
    "        \"\"\"\n",
    "        file_path = os.path.join(processed_data_dir, split)\n",
    "        print(f\"Loading *tokenized* TinyStories dataset split from: '{file_path}'...\")\n",
    "        \n",
    "        # Load the pre-processed dataset from disk\n",
    "        try:\n",
    "            # We load the processed Arrow Table (Dataset)\n",
    "            self.dataset = lm_dataset\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\n",
    "                f\"Could not load processed dataset split '{split}'. \"\n",
    "                f\"Did you run the pre-processing script first? Error: {e}\"\n",
    "            )\n",
    "\n",
    "        self.split = split\n",
    "        print(f\"Tokenized dataset loaded. Total chunks: {len(self.dataset)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of fixed-length chunks (examples) in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> dict:\n",
    "        \"\"\"\n",
    "        Retrieves the token IDs and labels for the chunk at the given index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the chunk to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing 'input_ids' and 'labels' tensors.\n",
    "        \"\"\"\n",
    "        # Retrieve the example from the Hugging Face Dataset\n",
    "        example = self.dataset[idx]\n",
    "        \n",
    "        # Convert the lists of integers (token IDs) into PyTorch Tensors\n",
    "        # These are the necessary keys for Causal Language Modeling training\n",
    "        return {\n",
    "            'input_ids': torch.tensor(example['input_ids'], dtype=torch.long),\n",
    "            'labels': torch.tensor(example['labels'], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "02b55f16-908f-48c9-bda9-2813c0cd2ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading *tokenized* TinyStories dataset split from: './processed_tinystories/train'...\n",
      "Tokenized dataset loaded. Total chunks: 459751\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TinyStoriesTokenizedDataset('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0813d9e6-ae19-491a-9ad4-201b47c0f7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "14c8e964-240f-4f7c-8113-79338760848f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = DistributedSampler(\n",
    "            train_dataset,\n",
    "            num_replicas=1,\n",
    "            rank=0,\n",
    "            shuffle=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b91caf75-4229-4167-bb4c-d78a000ba617",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/projects/llm-models/llm/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=8,\n",
    "            sampler=train_sampler,\n",
    "            shuffle=(train_sampler is None),\n",
    "            num_workers=8,\n",
    "            pin_memory=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "116955b2-efeb-4f0d-9fad-37bf52f6eb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[20037,  1816,   284,  ...,   640,    11,   612],\n",
      "        [ 2613,    13,  1119,  ...,   606,   290,   531],\n",
      "        [  290,   766,   508,  ...,   739,   257, 24484],\n",
      "        ...,\n",
      "        [  257,  3704,    13,  ...,   389,   407,  3148],\n",
      "        [  290,  3835,   290,  ..., 35113,   526,   198],\n",
      "        [10718,  1306,   284,  ...,    11, 11735,    13]]), 'labels': tensor([[20037,  1816,   284,  ...,   640,    11,   612],\n",
      "        [ 2613,    13,  1119,  ...,   606,   290,   531],\n",
      "        [  290,   766,   508,  ...,   739,   257, 24484],\n",
      "        ...,\n",
      "        [  257,  3704,    13,  ...,   389,   407,  3148],\n",
      "        [  290,  3835,   290,  ..., 35113,   526,   198],\n",
      "        [10718,  1306,   284,  ...,    11, 11735,    13]])}\n"
     ]
    }
   ],
   "source": [
    "for t in train_loader:\n",
    "    print(t)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fdafa1-468d-4e85-887c-9ec86d204efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
