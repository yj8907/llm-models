{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35fa4e02-4c9c-4ad4-b966-4d796e47b54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b27ef10-2df8-4c8f-b393-7adb9c069d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/projects/llm-models/llm-models/notebook'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab2b628a-bd0a-444b-86bf-39b7c2afced8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.deepseek.bidirGPT import get_bidirectinoal_gpt_layer_with_transformer_engine_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e2fa393-0c06-47e9-a492-aa7b3803f2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/ubuntu/projects/llm-dev/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'models.deepseek.model' from '/home/ubuntu/projects/llm-models/llm-models/models/deepseek/model.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib, os\n",
    "\n",
    "from models.deepseek import model\n",
    "importlib.reload(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e646f0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib, os\n",
    "\n",
    "from models.deepseek import model\n",
    "importlib.reload(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7f17d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/home/ubuntu/projects/Megatron-LM\")\n",
    "from gpt_builders import gpt_builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb9dd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributed as dist\n",
    "\n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '12356'\n",
    "dist.init_process_group(backend='nccl', rank=0, world_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc13f93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from megatron.training.yaml_arguments import load_yaml, core_transformer_config_from_yaml\n",
    "from megatron.training import get_args\n",
    "from types import SimpleNamespace\n",
    "\n",
    "from megatron.training.initialize import initialize_megatron\n",
    "\n",
    "# Load YAML and build transformer config once\n",
    "args_from_yaml = load_yaml(\"/home/ubuntu/projects/llm-models/llm-models/models/bidirectional/gpt_config.yaml\")\n",
    "transformer_config = core_transformer_config_from_yaml(args_from_yaml, \"language_model\")\n",
    "\n",
    "transfomer_key = 'language_model'\n",
    "# 1. Convert the base YAML object to a dictionary\n",
    "args_dict = vars(args_from_yaml).copy()\n",
    "\n",
    "# 2. Safely merge the nested dictionaries\n",
    "# Using .update() ensures that if a key exists in both, the nested one takes precedence\n",
    "transfomer_key = 'language_model'\n",
    "if hasattr(args_from_yaml, transfomer_key):\n",
    "    args_dict.update(vars(getattr(args_from_yaml, transfomer_key)))\n",
    "\n",
    "if hasattr(args_from_yaml, 'model_parallel'):\n",
    "    args_dict.update(vars(args_from_yaml.model_parallel))\n",
    "\n",
    "# 3. Create the namespace from the merged dictionary\n",
    "args = SimpleNamespace(**args_dict)\n",
    "\n",
    "initialize_megatron(\n",
    "    extra_args_provider=None,\n",
    "    args_defaults=vars(args),\n",
    "    ignore_unknown_args=True # Useful if your YAML has extra fields\n",
    ")\n",
    "\n",
    "# Provide gpt_builder with that config (config=... skips internal config loading)\n",
    "def model_provider_with_yaml_config(pre_process=True, post_process=True, vp_stage=None):\n",
    "    # You must still have get_args() returning the full args (e.g. from same YAML)\n",
    "    # args = get_args()\n",
    "\n",
    "        # Initalize and get arguments, timers, and Tensorboard writer.\n",
    "    extra_args_provider, args_defaults, get_embedding_ranks, get_position_embedding_ranks = None, None, None, None\n",
    "    store = None\n",
    "    \n",
    "    return gpt_builder(args, pre_process, post_process, vp_stage, config=transformer_config)\n",
    "\n",
    "model = model_provider_with_yaml_config()\n",
    "# Then in pretrain:\n",
    "# pretrain(..., partial(model_provider, model_provider_with_yaml_config), ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2f6865",
   "metadata": {},
   "outputs": [],
   "source": [
    "from megatron.core import parallel_state\n",
    "\n",
    "parallel_state.model_parallel_is_initialized()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9543a86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from megatron.core.models.gpt import GPTModel\n",
    "from megatron.core.transformer.spec_utils import import_module\n",
    "\n",
    "import megatron.core.models.gpt as gpt\n",
    "import importlib\n",
    "importlib.reload(gpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47fbb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_process = True\n",
    "post_process = True\n",
    "mtp_block_spec = None\n",
    "vp_stage = None\n",
    "\n",
    "model = gpt.GPTModel(\n",
    "    config=config,\n",
    "    transformer_layer_spec=transformer_layer_spec,\n",
    "    vocab_size=args.padded_vocab_size,\n",
    "    max_sequence_length=args.max_position_embeddings,\n",
    "    pre_process=pre_process,\n",
    "    post_process=post_process,\n",
    "    fp16_lm_cross_entropy=args.fp16_lm_cross_entropy,\n",
    "    parallel_output=True,\n",
    "    share_embeddings_and_output_weights=not args.untie_embeddings_and_output_weights,\n",
    "    position_embedding_type=args.position_embedding_type,\n",
    "    rotary_percent=args.rotary_percent,\n",
    "    rotary_base=args.rotary_base,\n",
    "    rope_scaling=args.use_rope_scaling,\n",
    "    mtp_block_spec=mtp_block_spec,\n",
    "    vp_stage=vp_stage,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29dc659",
   "metadata": {},
   "outputs": [],
   "source": [
    "from megatron.training import get_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764b7cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from megatron.training.yaml_arguments import load_yaml, core_transformer_config_from_yaml\n",
    "\n",
    "# Load YAML and build transformer config once\n",
    "args_from_yaml = load_yaml(\"/home/ubuntu/projects/llm-models/llm-models/models/bidirectional/gpt_config.yaml\")\n",
    "transformer_config = core_transformer_config_from_yaml(args_from_yaml, \"language_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2c3e62-422c-48ce-ba90-f4704da8eef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from megatron.training.yaml_arguments import load_yaml, core_transformer_config_from_yaml\n",
    "\n",
    "# Load YAML and build transformer config once\n",
    "args_from_yaml = load_yaml(\"/home/ubuntu/projects/llm-models/llm-models/models/bidirectional/gpt_config.yaml\")\n",
    "transformer_config = core_transformer_config_from_yaml(args_from_yaml, \"language_model\")\n",
    "\n",
    "# Provide gpt_builder with that config (config=... skips internal config loading)\n",
    "def model_provider_with_yaml_config(pre_process=True, post_process=True, vp_stage=None):\n",
    "    # You must still have get_args() returning the full args (e.g. from same YAML)\n",
    "    args = get_args()\n",
    "    return gpt_builder(args, pre_process, post_process, vp_stage, config=transformer_config)\n",
    "\n",
    "# Then in pretrain:\n",
    "pretrain(..., partial(model_provider, model_provider_with_yaml_config), ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a505a7-8599-41bb-b010-3f9a4fdc5c0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
