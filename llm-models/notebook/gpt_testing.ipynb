{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35fa4e02-4c9c-4ad4-b966-4d796e47b54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b27ef10-2df8-4c8f-b393-7adb9c069d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/projects/llm-models/llm-models/notebook'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab2b628a-bd0a-444b-86bf-39b7c2afced8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import models.bidirectional.bidirGPT as bidirGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e2fa393-0c06-47e9-a492-aa7b3803f2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/ubuntu/projects/llm-dev/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'models.deepseek.model' from '/home/ubuntu/projects/llm-models/llm-models/models/deepseek/model.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib, os\n",
    "\n",
    "from models.deepseek import model\n",
    "importlib.reload(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e646f0a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'models.deepseek.model' from '/home/ubuntu/projects/llm-models/llm-models/models/deepseek/model.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib, os\n",
    "\n",
    "from models.deepseek import model\n",
    "importlib.reload(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d7f17d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/home/ubuntu/projects/Megatron-LM\")\n",
    "from gpt_builders import gpt_builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bb9dd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributed as dist\n",
    "\n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '12356'\n",
    "dist.init_process_group(backend='nccl', rank=0, world_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae42302d-560f-44b7-96cd-d160d657e8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# args = args_from_yaml\n",
    "# args = SimpleNamespace(**vars(getattr(args, 'language_model')), **vars(args.model_parallel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e14fff-c973-4bd7-b611-6d2690658240",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb864950-9d23-4b6e-8f75-bfc573cb7cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getattr(args, 'scaled_init_method')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc13f93b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning using experimental yaml arguments feature, argparse arguments will be ignored\n",
      "test\n",
      "True\n",
      "using world size: 1, data-parallel size: 1, context-parallel size: 1, hierarchical context-parallel sizes: None, tensor-model-parallel size: 1, pipeline-model-parallel size: 1\n",
      "Number of virtual stages per pipeline stage: None\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  _cpu_offloading_context ......................... None\n",
      "  account_for_embedding_in_pipeline_split ......... False\n",
      "  account_for_loss_in_pipeline_split .............. False\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activation_func ................................. swiglu\n",
      "  activation_func_clamp_value ..................... None\n",
      "  activation_func_fp8_input_store ................. False\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  add_bias_linear ................................. True\n",
      "  add_position_embedding .......................... True\n",
      "  add_qkv_bias .................................... True\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  align_grad_reduce ............................... True\n",
      "  align_param_gather .............................. False\n",
      "  allow_ambiguous_pad_tokens ...................... False\n",
      "  app_tag_run_name ................................ None\n",
      "  app_tag_run_version ............................. 0.0.0\n",
      "  apply_layernorm_1p .............................. False\n",
      "  apply_query_key_layer_scaling ................... False\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  apply_rope_fusion ............................... False\n",
      "  apply_wd_to_qk_layernorm ........................ False\n",
      "  async_save ...................................... False\n",
      "  async_tensor_model_parallel_allreduce ........... True\n",
      "  attention_backend ............................... AttnBackend.auto\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_output_gate ........................... False\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  auto_detect_ckpt_format ......................... False\n",
      "  autocast_dtype .................................. None\n",
      "  barrier_with_L1_time ............................ True\n",
      "  batch_invariant_mode ............................ False\n",
      "  batch_p2p_comm .................................. True\n",
      "  batch_p2p_sync .................................. True\n",
      "  bert_binary_head ................................ True\n",
      "  bert_embedder_type .............................. megatron\n",
      "  bert_load ....................................... None\n",
      "  beta_fast ....................................... 32\n",
      "  beta_slow ....................................... 1\n",
      "  bf16 ............................................ False\n",
      "  bias_activation_fusion .......................... False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  bias_swiglu_fusion .............................. True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  cache_mla_latents ............................... False\n",
      "  calc_ft_timeouts ................................ False\n",
      "  calculate_per_token_loss ........................ False\n",
      "  check_for_large_grads ........................... False\n",
      "  check_for_nan_in_loss_and_grad .................. True\n",
      "  check_for_spiky_loss ............................ False\n",
      "  check_weight_hash_across_dp_replicas_interval ... None\n",
      "  ckpt_assume_constant_structure .................. False\n",
      "  ckpt_convert_format ............................. None\n",
      "  ckpt_convert_save ............................... None\n",
      "  ckpt_convert_update_legacy_dist_opt_format ...... False\n",
      "  ckpt_format ..................................... torch_dist\n",
      "  ckpt_fully_parallel_load ........................ False\n",
      "  ckpt_fully_parallel_save ........................ True\n",
      "  ckpt_fully_parallel_save_deprecated ............. False\n",
      "  ckpt_step ....................................... None\n",
      "  classes_fraction ................................ 1.0\n",
      "  clip_grad ....................................... 1.0\n",
      "  clone_scatter_output_in_embedding ............... True\n",
      "  config_logger_dir ............................... \n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  context_parallel_size ........................... 1\n",
      "  cp_comm_type .................................... ['p2p']\n",
      "  cpu_offloading .................................. False\n",
      "  cpu_offloading_activations ...................... True\n",
      "  cpu_offloading_double_buffering ................. False\n",
      "  cpu_offloading_num_layers ....................... 0\n",
      "  cpu_offloading_weights .......................... False\n",
      "  create_all_gather_group ......................... False\n",
      "  create_attention_mask_in_dataloader ............. True\n",
      "  cross_entropy_fusion_impl ....................... native\n",
      "  cross_entropy_loss_fusion ....................... False\n",
      "  cuda_graph_impl ................................. none\n",
      "  cuda_graph_retain_backward_graph ................ False\n",
      "  cuda_graph_scope ................................ []\n",
      "  cuda_graph_use_single_mempool ................... False\n",
      "  cuda_graph_warmup_steps ......................... 3\n",
      "  data_args_path .................................. None\n",
      "  data_cache_path ................................. None\n",
      "  data_parallel_random_init ....................... False\n",
      "  data_parallel_sharding_strategy ................. no_shard\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... None\n",
      "  data_per_class_fraction ......................... 1.0\n",
      "  data_sharding ................................... True\n",
      "  dataloader_defer_npy_index_mmap ................. False\n",
      "  dataloader_fast_cache_load ...................... False\n",
      "  dataloader_type ................................. single\n",
      "  ddp_average_in_collective ....................... False\n",
      "  ddp_bucket_size ................................. None\n",
      "  ddp_num_buckets ................................. None\n",
      "  ddp_pad_buckets_for_high_nccl_busbw ............. False\n",
      "  ddp_reduce_scatter_with_fp32_accumulation ....... False\n",
      "  deallocate_pipeline_outputs ..................... False\n",
      "  decode_only_cuda_graphs ......................... False\n",
      "  decoder_first_pipeline_num_layers ............... None\n",
      "  decoder_last_pipeline_num_layers ................ None\n",
      "  decoder_num_layers .............................. None\n",
      "  decoder_seq_length .............................. None\n",
      "  decoupled_lr .................................... None\n",
      "  decoupled_min_lr ................................ None\n",
      "  decrease_batch_size_if_needed ................... False\n",
      "  defer_embedding_wgrad_compute ................... False\n",
      "  delay_wgrad_compute ............................. False\n",
      "  deprecated_use_mcore_models ..................... False\n",
      "  deterministic_mode .............................. False\n",
      "  dino_bottleneck_size ............................ 256\n",
      "  dino_freeze_last_layer .......................... 1\n",
      "  dino_head_hidden_size ........................... 2048\n",
      "  dino_local_crops_number ......................... 10\n",
      "  dino_local_img_size ............................. 96\n",
      "  dino_norm_last_layer ............................ False\n",
      "  dino_teacher_temp ............................... 0.07\n",
      "  dino_warmup_teacher_temp ........................ 0.04\n",
      "  dino_warmup_teacher_temp_epochs ................. 30\n",
      "  disable_bf16_reduced_precision_matmul ........... False\n",
      "  disable_chunked_prefill ......................... True\n",
      "  disable_jit_fuser ............................... False\n",
      "  disable_mamba_mem_eff_path ...................... False\n",
      "  disable_parameter_transpose_cache ............... False\n",
      "  disable_straggler_on_startup .................... False\n",
      "  disable_symmetric_registration .................. False\n",
      "  dist_ckpt_format_deprecated ..................... None\n",
      "  dist_ckpt_optim_fully_reshardable ............... False\n",
      "  dist_ckpt_save_pre_mcore_014 .................... False\n",
      "  dist_ckpt_strictness ............................ assume_ok_unexpected\n",
      "  distrib_optim_fully_reshardable_mem_efficient ... False\n",
      "  distribute_saved_activations .................... False\n",
      "  distributed_backend ............................. nccl\n",
      "  distributed_timeout_minutes ..................... 10\n",
      "  distributed_timeout_seconds_after_init .......... None\n",
      "  dsa_indexer_head_dim ............................ None\n",
      "  dsa_indexer_loss_coeff .......................... None\n",
      "  dsa_indexer_n_heads ............................. None\n",
      "  dsa_indexer_topk ................................ None\n",
      "  dsa_indexer_use_sparse_loss ..................... False\n",
      "  dump_param_to_param_group_map ................... None\n",
      "  embedding_init_method ........................... None\n",
      "  embedding_init_method_std ....................... None\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_autocast ................................. False\n",
      "  enable_cuda_graph ............................... False\n",
      "  enable_experimental ............................. False\n",
      "  enable_ft_package ............................... False\n",
      "  enable_full_sharding_in_hsdp .................... False\n",
      "  enable_gloo_process_groups ...................... True\n",
      "  enable_msc ...................................... True\n",
      "  enable_one_logger ............................... True\n",
      "  encoder_num_layers .............................. 4\n",
      "  encoder_seq_length .............................. 1024\n",
      "  end_weight_decay ................................ 0.01\n",
      "  eod_mask_loss ................................... False\n",
      "  ep_overlap_early_attn_memory_release ............ False\n",
      "  error_injection_rate ............................ 0\n",
      "  error_injection_type ............................ transient_error\n",
      "  eval_interval ................................... 2000\n",
      "  eval_iters ...................................... 100\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... None\n",
      "  exit_on_missing_checkpoint ...................... False\n",
      "  exit_signal ..................................... 15\n",
      "  exit_signal_handler ............................. False\n",
      "  exit_signal_handler_for_dataloader .............. False\n",
      "  exp_avg_dtype ................................... torch.float32\n",
      "  exp_avg_sq_dtype ................................ torch.float32\n",
      "  experimental_attention_variant .................. None\n",
      "  expert_model_parallel_size ...................... 1\n",
      "  expert_tensor_parallel_size ..................... 1\n",
      "  external_cuda_graph ............................. False\n",
      "  fake_process_group .............................. False\n",
      "  ffn_hidden_size ................................. 512\n",
      "  fim_data ........................................ False\n",
      "  fim_eod_token ................................... <|endoftext|>\n",
      "  fim_fragment_rate ............................... None\n",
      "  fim_middle_token ................................ <fim_middle>\n",
      "  fim_no_prefix ................................... None\n",
      "  fim_pad_token ................................... <fim_pad>\n",
      "  fim_prefix_token ................................ <fim_prefix>\n",
      "  fim_rate ........................................ 0.5\n",
      "  fim_split_sample ................................ None\n",
      "  fim_spm_rate .................................... 0.5\n",
      "  fim_suffix_token ................................ <fim_suffix>\n",
      "  finalize_model_grads_func ....................... None\n",
      "  fine_grained_activation_offloading .............. False\n",
      "  finetune ........................................ False\n",
      "  first_last_layers_bf16 .......................... False\n",
      "  flash_decode .................................... False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  fp4 ............................................. None\n",
      "  fp4_param ....................................... False\n",
      "  fp4_quantizer_factory ........................... None\n",
      "  fp4_recipe ...................................... nvfp4\n",
      "  fp8 ............................................. None\n",
      "  fp8_amax_compute_algo ........................... most_recent\n",
      "  fp8_amax_history_len ............................ 1\n",
      "  fp8_dot_product_attention ....................... False\n",
      "  fp8_interval .................................... 1\n",
      "  fp8_margin ...................................... 0\n",
      "  fp8_multi_head_attention ........................ False\n",
      "  fp8_param ....................................... False\n",
      "  fp8_param_gather ................................ False\n",
      "  fp8_quantizer_factory ........................... None\n",
      "  fp8_recipe ...................................... delayed\n",
      "  fp8_wgrad ....................................... True\n",
      "  fsdp_double_buffer .............................. False\n",
      "  fsdp_manual_registration ........................ False\n",
      "  ft_num_warmup_iters ............................. 5\n",
      "  full_validation ................................. False\n",
      "  fused_single_qkv_rope ........................... False\n",
      "  gated_linear_unit ............................... False\n",
      "  global_batch_size ............................... 128\n",
      "  glu_linear_offset ............................... 0.0\n",
      "  grad_reduce_in_bf16 ............................. False\n",
      "  grad_scale_func ................................. None\n",
      "  grad_sync_func .................................. None\n",
      "  gradient_accumulation_fusion .................... True\n",
      "  gradient_reduce_div_fusion ...................... True\n",
      "  group_query_attention ........................... False\n",
      "  grpo_clamp_eps_lower ............................ 0.01\n",
      "  grpo_clamp_eps_upper ............................ 0.01\n",
      "  grpo_entropy_term_weight ........................ 0.0\n",
      "  grpo_filter_groups_with_same_reward ............. False\n",
      "  grpo_group_size ................................. 2\n",
      "  grpo_iterations ................................. 2\n",
      "  grpo_kl_beta .................................... 0.001\n",
      "  grpo_prompts_per_step ........................... 32\n",
      "  head_lr_mult .................................... 1.0\n",
      "  hetereogenous_dist_checkpoint ................... False\n",
      "  heterogeneous_block_specs ....................... False\n",
      "  heterogeneous_layers_config_encoded_json ........ None\n",
      "  heterogeneous_layers_config_path ................ None\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 128\n",
      "  hierarchical_context_parallel_sizes ............. [1]\n",
      "  high_priority_stream_groups ..................... []\n",
      "  hybrid_attention_ratio .......................... 0.0\n",
      "  hybrid_context_parallel ......................... False\n",
      "  hybrid_mlp_ratio ................................ 0.0\n",
      "  hybrid_override_pattern ......................... None\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_h ........................................... 224\n",
      "  img_w ........................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  inference_batch_times_seqlen_threshold .......... -1\n",
      "  inference_coordinator_port ...................... 12346\n",
      "  inference_dynamic_batching ...................... False\n",
      "  inference_dynamic_batching_block_size ........... 256\n",
      "  inference_dynamic_batching_buffer_size_gb ....... 40.0\n",
      "  inference_dynamic_batching_cuda_graph_max_tokens  16384\n",
      "  inference_dynamic_batching_cuda_graph_mixed_prefill_count  16\n",
      "  inference_dynamic_batching_max_requests ......... None\n",
      "  inference_dynamic_batching_max_tokens ........... None\n",
      "  inference_dynamic_batching_num_cuda_graphs ...... 16\n",
      "  inference_dynamic_batching_paused_buffer_size_gb  None\n",
      "  inference_dynamic_batching_track_paused_request_events  False\n",
      "  inference_dynamic_batching_unified_memory_level . 0\n",
      "  inference_fuse_tp_communication ................. False\n",
      "  inference_logging_step_interval ................. 0\n",
      "  inference_max_batch_size ........................ 8\n",
      "  inference_max_seq_length ........................ 2560\n",
      "  inference_rng_tracker ........................... False\n",
      "  inference_sampling_seed ......................... 42\n",
      "  inference_wandb_logging ......................... False\n",
      "  init_method ..................................... kaiming_uniform\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  init_model_with_meta_device ..................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  inprocess_active_world_size ..................... 1\n",
      "  inprocess_barrier_timeout ....................... 120\n",
      "  inprocess_completion_timeout .................... 120\n",
      "  inprocess_empty_cuda_cache ...................... False\n",
      "  inprocess_granularity ........................... node\n",
      "  inprocess_hard_timeout .......................... 90\n",
      "  inprocess_heartbeat_interval .................... 30\n",
      "  inprocess_heartbeat_timeout ..................... 60\n",
      "  inprocess_last_call_wait ........................ 1\n",
      "  inprocess_max_iterations ........................ None\n",
      "  inprocess_monitor_process_interval .............. 1.0\n",
      "  inprocess_monitor_thread_interval ............... 1.0\n",
      "  inprocess_progress_watchdog_interval ............ 1.0\n",
      "  inprocess_restart ............................... False\n",
      "  inprocess_soft_timeout .......................... 60\n",
      "  inprocess_termination_grace_time ................ 1\n",
      "  is_hybrid_model ................................. False\n",
      "  iter_per_epoch .................................. 1250\n",
      "  iterations_to_skip .............................. []\n",
      "  keep_fp8_transpose_cache ........................ False\n",
      "  kitchen_attention_backend ....................... sdpa\n",
      "  kv_channels ..................................... 16\n",
      "  kv_lora_rank .................................... 32\n",
      "  langrl_env_config ............................... None\n",
      "  langrl_external_server .......................... False\n",
      "  langrl_inference_server_conversation_template ... None\n",
      "  langrl_inference_server_type .................... inplace_megatron\n",
      "  language_model .................................. namespace(num_layers=4, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, pipeline_model_parallel_layout=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=128, num_attention_heads=8, attention_backend='auto', softmax_scale=None, softmax_type='vanilla', num_query_groups=None, ffn_hidden_size=None, kv_channels=None, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-05, layernorm_zero_centered_gamma=True, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=False, activation_func='swiglu', activation_func_fp8_input_store=False, glu_linear_offset=0.0, activation_func_clamp_value=None, num_moe_experts=None, rotary_interleaved=False, window_size=None, window_attn_skip_freq=None, normalization='RMSNorm', qk_layernorm=False, qk_l2_norm=False, qk_clip=False, qk_clip_alpha=0.5, qk_clip_threshold=100, log_max_attention_logit=False, attention_output_gate=False, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=True, no_rope_freq=None, use_legacy_models=False, padded_vocab_size=100048, use_rope_scaling=False, q_lora_rank=64, kv_lora_rank=64, qk_head_dim=32, qk_pos_emb_head_dim=32, v_head_dim=32, rope_type='yarn', rotary_base=10000, rotary_percent=1.0, rotary_scaling_factor=40, original_max_position_embeddings=4096, beta_fast=32, beta_slow=1, mscale=1.0, mscale_all_dim=0.0, cache_mla_latents=False, experimental_attention_variant=None, dsa_indexer_n_heads=None, dsa_indexer_head_dim=None, dsa_indexer_topk=None, dsa_indexer_loss_coeff=None, dsa_indexer_use_sparse_loss=False, linear_attention_freq=None, linear_conv_kernel_dim=None, linear_key_head_dim=None, linear_value_head_dim=None, linear_num_key_heads=None, linear_num_value_heads=None, init_method='kaiming_uniform', output_layer_init_method='kaiming_uniform', init_method_std=0.02, embedding_init_method=None, embedding_init_method_std=None, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, bias_swiglu_fusion=True, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=True, apply_rope_fusion=True, use_fused_weighted_squared_relu=False, fused_single_qkv_rope=False, recompute_granularity=None, recompute_method=None, recompute_num_layers=None, distribute_saved_activations=None, recompute_modules=None, fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_quantizer_factory=None, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, use_kitchen=False, use_kitchen_attention=False, kitchen_attention_backend='sdpa', fp4=None, fp4_recipe='nvfp4', fp4_param=False, fp4_quantizer_factory=None, moe_shared_expert_intermediate_size=None, moe_shared_expert_gate=False, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=None, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_enable_routing_replay=False, moe_router_topk_limited_devices=None, moe_router_padding_for_quantization=False, moe_router_padding_for_fp8=False, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_router_force_load_balancing=False, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='allgather', moe_enable_deepep=False, moe_flex_dispatcher_backend='deepep', moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_router_fusion=False, moe_apply_probs_on_input=False, moe_latent_size=None, moe_deepep_num_sms=20, moe_hybridep_num_sms=16, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_impl='none', cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, batch_invariant_mode=False, use_te_activation_func=False, use_te_rng_tracker=False, inference_rng_tracker=False, inference_sampling_seed=42, symmetric_ar_type=None, use_inference_optimized_layers=False, inference_fuse_tp_communication=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, mamba_num_heads=None, use_mamba_mem_eff_path=True, mlp_chunks_for_prefill=1, heterogeneous_block_specs=False, hetereogenous_dist_checkpoint=False, quant_recipe=None, transformer_impl='local', fine_grained_activation_offloading=False, offload_modules=None, min_offloaded_tensor_size=1048576)\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  layernorm_zero_centered_gamma ................... True\n",
      "  lazy_mpu_init ................................... None\n",
      "  legacy_tokenizer ................................ False\n",
      "  linear_attention_freq ........................... None\n",
      "  linear_conv_kernel_dim .......................... 4\n",
      "  linear_key_head_dim ............................. 128\n",
      "  linear_num_key_heads ............................ 16\n",
      "  linear_num_value_heads .......................... 32\n",
      "  linear_value_head_dim ........................... 128\n",
      "  load ............................................ None\n",
      "  load_main_params_from_ckpt ...................... False\n",
      "  local_rank ...................................... 0\n",
      "  log_device_memory_used .......................... False\n",
      "  log_energy ...................................... False\n",
      "  log_interval .................................... 100\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_max_attention_logit ......................... False\n",
      "  log_memory_interval ............................. None\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_progress .................................... False\n",
      "  log_straggler ................................... False\n",
      "  log_throughput .................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  log_world_size_to_tensorboard ................... False\n",
      "  logging_level ................................... None\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00025\n",
      "  lr_decay_iters .................................. None\n",
      "  lr_decay_samples ................................ 255126953\n",
      "  lr_decay_style .................................. linear\n",
      "  lr_warmup_fraction .............................. None\n",
      "  lr_warmup_init .................................. 0.0\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  lr_wsd_decay_iters .............................. None\n",
      "  lr_wsd_decay_samples ............................ None\n",
      "  lr_wsd_decay_style .............................. exponential\n",
      "  main_grads_dtype ................................ torch.float32\n",
      "  main_params_dtype ............................... torch.float32\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mamba_head_dim .................................. 64\n",
      "  mamba_num_groups ................................ 8\n",
      "  mamba_num_heads ................................. None\n",
      "  mamba_state_dim ................................. 128\n",
      "  manual_gc ....................................... False\n",
      "  manual_gc_eval .................................. True\n",
      "  manual_gc_interval .............................. 0\n",
      "  mask_factor ..................................... 1.0\n",
      "  mask_prob ....................................... 0.15\n",
      "  mask_type ....................................... random\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 4096\n",
      "  max_seqlen_per_dp_cp_rank ....................... None\n",
      "  max_tokens_to_oom ............................... 12000\n",
      "  memory_efficient_layer_norm ..................... False\n",
      "  memory_snapshot_path ............................ snapshot.pickle\n",
      "  merge_file ...................................... None\n",
      "  micro_batch_size ................................ 2\n",
      "  microbatch_group_size_per_vp_stage .............. None\n",
      "  mid_level_dataset_surplus ....................... 0.005\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 0.0\n",
      "  min_offloaded_tensor_size ....................... 1048576\n",
      "  mlp_chunks_for_prefill .......................... 1\n",
      "  mmap_bin_files .................................. True\n",
      "  mock_data ....................................... False\n",
      "  model_parallel .................................. namespace(tensor_model_parallel_size=1, pipeline_model_parallel_comm_backend='nccl', pipeline_model_parallel_size=1, virtual_pipeline_model_parallel_size=None, sequence_parallel=False, context_parallel_size=1, hierarchical_context_parallel_sizes=[1], max_seqlen_per_dp_cp_rank=None, hybrid_context_parallel=False, expert_model_parallel_size=1, expert_tensor_parallel_size=None, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=None, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=None, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=True, async_tensor_model_parallel_allreduce=True, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', overlap_moe_expert_parallel_comm=False, delay_wgrad_compute=False, ep_overlap_early_attn_memory_release=False, pipeline_dtype=None, variable_seq_lengths=False, overlap_p2p_comm=False, batch_p2p_comm=True, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=None, mtp_standalone=False, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=False, cpu_offloading_double_buffering=False, barrier_with_L1_time=True)\n",
      "  moe_apply_probs_on_input ........................ False\n",
      "  moe_aux_loss_coeff .............................. 0.0\n",
      "  moe_deepep_num_sms .............................. 20\n",
      "  moe_enable_deepep ............................... False\n",
      "  moe_enable_routing_replay ....................... False\n",
      "  moe_expert_capacity_factor ...................... None\n",
      "  moe_extended_tp ................................. False\n",
      "  moe_ffn_hidden_size ............................. None\n",
      "  moe_flex_dispatcher_backend ..................... deepep\n",
      "  moe_grouped_gemm ................................ False\n",
      "  moe_hybridep_num_sms ............................ 16\n",
      "  moe_input_jitter_eps ............................ None\n",
      "  moe_latent_size ................................. None\n",
      "  moe_layer_freq .................................. 1\n",
      "  moe_layer_recompute ............................. False\n",
      "  moe_pad_expert_input_to_capacity ................ False\n",
      "  moe_pad_experts_for_cuda_graph_inference ........ False\n",
      "  moe_per_layer_logging ........................... False\n",
      "  moe_permute_fusion .............................. False\n",
      "  moe_router_bias_update_rate ..................... 0.001\n",
      "  moe_router_dtype ................................ None\n",
      "  moe_router_enable_expert_bias ................... False\n",
      "  moe_router_force_load_balancing ................. False\n",
      "  moe_router_fusion ............................... False\n",
      "  moe_router_group_topk ........................... None\n",
      "  moe_router_load_balancing_type .................. aux_loss\n",
      "  moe_router_num_groups ........................... None\n",
      "  moe_router_padding_for_fp8 ...................... False\n",
      "  moe_router_padding_for_quantization ............. False\n",
      "  moe_router_pre_softmax .......................... False\n",
      "  moe_router_score_function ....................... softmax\n",
      "  moe_router_topk ................................. 2\n",
      "  moe_router_topk_limited_devices ................. None\n",
      "  moe_router_topk_scaling_factor .................. None\n",
      "  moe_shared_expert_gate .......................... False\n",
      "  moe_shared_expert_intermediate_size ............. None\n",
      "  moe_shared_expert_overlap ....................... False\n",
      "  moe_token_dispatcher_type ....................... allgather\n",
      "  moe_token_drop_policy ........................... probs\n",
      "  moe_token_dropping .............................. False\n",
      "  moe_upcycling_granularity ....................... 1\n",
      "  moe_use_legacy_grouped_gemm ..................... False\n",
      "  moe_use_upcycling ............................... False\n",
      "  moe_z_loss_coeff ................................ None\n",
      "  mrope_section ................................... None\n",
      "  mscale .......................................... 1.0\n",
      "  mscale_all_dim .................................. 0.0\n",
      "  mtp_loss_scaling_factor ......................... 0.1\n",
      "  mtp_num_layers .................................. None\n",
      "  mtp_standalone .................................. False\n",
      "  multi_latent_attention .......................... False\n",
      "  multiple_validation_sets ........................ False\n",
      "  muon_extra_scale_factor ......................... 1.0\n",
      "  muon_fp32_matmul_prec ........................... medium\n",
      "  muon_momentum ................................... 0.9\n",
      "  muon_num_ns_steps ............................... 5\n",
      "  muon_scale_mode ................................. spectral\n",
      "  muon_split_qkv .................................. True\n",
      "  muon_tp_mode .................................... blockwise\n",
      "  muon_use_nesterov ............................... False\n",
      "  nccl_all_reduce_for_prefill ..................... False\n",
      "  nccl_communicator_config_path ................... None\n",
      "  nccl_ub ......................................... False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_persist_layer_norm ........................... False\n",
      "  no_rope_freq .................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  no_sync_func .................................... None\n",
      "  no_weight_decay_cond_type ....................... None\n",
      "  non_persistent_ckpt_type ........................ None\n",
      "  non_persistent_global_ckpt_dir .................. None\n",
      "  non_persistent_local_ckpt_algo .................. fully_parallel\n",
      "  non_persistent_local_ckpt_dir ................... None\n",
      "  non_persistent_save_interval .................... None\n",
      "  norm_epsilon .................................... 1e-05\n",
      "  normalization ................................... LayerNorm\n",
      "  num_attention_heads ............................. 8\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_dataset_builder_threads ..................... 1\n",
      "  num_distributed_optimizer_instances ............. 1\n",
      "  num_experts ..................................... None\n",
      "  num_layers ...................................... 4\n",
      "  num_layers_at_end_in_bf16 ....................... 1\n",
      "  num_layers_at_start_in_bf16 ..................... 1\n",
      "  num_layers_in_first_pipeline_stage .............. None\n",
      "  num_layers_in_last_pipeline_stage ............... None\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_microbatches_with_partial_activation_checkpoints  None\n",
      "  num_moe_experts ................................. None\n",
      "  num_query_groups ................................ 1\n",
      "  num_virtual_stages_per_pipeline_rank ............ None\n",
      "  num_workers ..................................... 2\n",
      "  object_storage_cache_path ....................... None\n",
      "  offload_modules ................................. []\n",
      "  one_logger_async ................................ False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/projects/Megatron-LM/megatron/core/transformer/transformer_config.py:1495: UserWarning: apply_rope_fusion for multi-latent attention only supports training. It is experimental and may change in future versions.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/core/transformer/transformer_config.py:1668: UserWarning: full scope is deprecated. Use empty cuda_graph_scope to capture the whole layer.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for use_legacy_models:False with use_legacy_models:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for check_for_nan_in_loss_and_grad:True with check_for_nan_in_loss_and_grad:True\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for add_position_embedding:False with add_position_embedding:True\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for make_vocab_size_divisible_by:128 with make_vocab_size_divisible_by:128\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for group_query_attention:False with group_query_attention:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for exit_signal_handler:False with exit_signal_handler:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for untie_embeddings_and_output_weights:True with untie_embeddings_and_output_weights:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for position_embedding_type:rope with position_embedding_type:learned_absolute\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for rotary_percent:1.0 with rotary_percent:1.0\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for openai_gelu:False with openai_gelu:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for squared_relu:False with squared_relu:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for swiglu:True with swiglu:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for bert_binary_head:True with bert_binary_head:True\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for use_flash_attn:False with use_flash_attn:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for seed:1234 with seed:1234\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for data_parallel_random_init:False with data_parallel_random_init:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for optimizer:adam with optimizer:adam\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for lr_decay_style:cosine with lr_decay_style:linear\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for lr_warmup_iters:0 with lr_warmup_iters:0\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for lr_warmup_samples:81381 with lr_warmup_samples:0\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for lr_warmup_init:0.0 with lr_warmup_init:0.0\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for min_lr:2.5e-05 with min_lr:0.0\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for weight_decay:0.1 with weight_decay:0.01\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for weight_decay_incr_style:constant with weight_decay_incr_style:constant\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for clip_grad:1.0 with clip_grad:1.0\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for adam_beta1:0.9 with adam_beta1:0.9\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for adam_beta2:0.95 with adam_beta2:0.999\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for adam_eps:1e-08 with adam_eps:1e-08\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for sgd_momentum:0.9 with sgd_momentum:0.9\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for override_opt_param_scheduler:False with override_opt_param_scheduler:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for use_checkpoint_opt_param_scheduler:False with use_checkpoint_opt_param_scheduler:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for finetune:False with finetune:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for use_checkpoint_args:False with use_checkpoint_args:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for exit_on_missing_checkpoint:False with exit_on_missing_checkpoint:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for initial_loss_scale:4294967296 with initial_loss_scale:4294967296\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for min_loss_scale:1.0 with min_loss_scale:1.0\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for loss_scale_window:1000 with loss_scale_window:1000\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for hysteresis:2 with hysteresis:2\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for accumulate_allreduce_grads_in_fp32:False with accumulate_allreduce_grads_in_fp32:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for fp16_lm_cross_entropy:False with fp16_lm_cross_entropy:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for distributed_backend:nccl with distributed_backend:nccl\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for distributed_timeout_minutes:10 with distributed_timeout_minutes:10\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for overlap_grad_reduce:False with overlap_grad_reduce:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for align_grad_reduce:True with align_grad_reduce:True\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for overlap_param_gather:False with overlap_param_gather:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for align_param_gather:False with align_param_gather:True\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for scatter_gather_tensors_in_pipeline:True with scatter_gather_tensors_in_pipeline:True\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for local_rank:None with local_rank:0\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for empty_unused_memory_level:0 with empty_unused_memory_level:0\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for use_distributed_optimizer:False with use_distributed_optimizer:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for eval_iters:32 with eval_iters:100\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for skip_train:False with skip_train:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for adlr_autoresume:False with adlr_autoresume:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for adlr_autoresume_interval:1000 with adlr_autoresume_interval:1000\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for manual_gc:False with manual_gc:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for manual_gc_interval:0 with manual_gc_interval:0\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for manual_gc_eval:True with manual_gc_eval:True\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for mock_data:False with mock_data:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for vocab_extra_ids:0 with vocab_extra_ids:0\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for sample_rate:1.0 with sample_rate:1.0\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for mask_prob:0.15 with mask_prob:0.15\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for short_seq_prob:0.1 with short_seq_prob:0.1\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for num_workers:2 with num_workers:2\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for reset_position_ids:False with reset_position_ids:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for reset_attention_mask:False with reset_attention_mask:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for eod_mask_loss:False with eod_mask_loss:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for profile:False with profile:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for profile_ranks:[0] with profile_ranks:[0]\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for profile_step_end:12 with profile_step_end:12\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for profile_step_start:10 with profile_step_start:10\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for log_params_norm:True with log_params_norm:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for log_num_zeros_in_grad:True with log_num_zeros_in_grad:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for log_throughput:False with log_throughput:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for log_progress:False with log_progress:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for timing_log_level:0 with timing_log_level:0\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for timing_log_option:minmax with timing_log_option:minmax\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for tensorboard_log_interval:1 with tensorboard_log_interval:1\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for tensorboard_queue_size:1000 with tensorboard_queue_size:1000\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for log_timers_to_tensorboard:False with log_timers_to_tensorboard:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for log_validation_ppl_to_tensorboard:False with log_validation_ppl_to_tensorboard:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for log_memory_to_tensorboard:False with log_memory_to_tensorboard:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for log_world_size_to_tensorboard:False with log_world_size_to_tensorboard:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for log_loss_scale_to_tensorboard:True with log_loss_scale_to_tensorboard:True\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for enable_one_logger:True with enable_one_logger:True\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for one_logger_project:megatron-lm with one_logger_project:megatron-lm\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for log_interval:100 with log_interval:100\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for mtp_loss_scaling_factor:None with mtp_loss_scaling_factor:0.1\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for account_for_embedding_in_pipeline_split:False with account_for_embedding_in_pipeline_split:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for account_for_loss_in_pipeline_split:False with account_for_loss_in_pipeline_split:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for attention_backend:auto with attention_backend:AttnBackend.auto\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for softmax_type:vanilla with softmax_type:vanilla\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for num_query_groups:None with num_query_groups:1\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for hidden_dropout:0.0 with hidden_dropout:0.1\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for attention_dropout:0.0 with attention_dropout:0.1\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for fp32_residual_connection:False with fp32_residual_connection:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for apply_residual_connection_post_layernorm:False with apply_residual_connection_post_layernorm:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for add_bias_linear:False with add_bias_linear:True\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for add_qkv_bias:False with add_qkv_bias:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for glu_linear_offset:0.0 with glu_linear_offset:0.0\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for rotary_interleaved:False with rotary_interleaved:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for normalization:RMSNorm with normalization:LayerNorm\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for qk_layernorm:False with qk_layernorm:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for qk_l2_norm:False with qk_l2_norm:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for qk_clip:False with qk_clip:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for qk_clip_alpha:0.5 with qk_clip_alpha:0.5\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for qk_clip_threshold:100 with qk_clip_threshold:100\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for log_max_attention_logit:False with log_max_attention_logit:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for attention_output_gate:False with attention_output_gate:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for test_mode:False with test_mode:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for calculate_per_token_loss:False with calculate_per_token_loss:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for multi_latent_attention:True with multi_latent_attention:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for use_rope_scaling:False with use_rope_scaling:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for kv_lora_rank:64 with kv_lora_rank:32\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for qk_head_dim:32 with qk_head_dim:128\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for qk_pos_emb_head_dim:32 with qk_pos_emb_head_dim:64\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for v_head_dim:32 with v_head_dim:128\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for rotary_base:10000 with rotary_base:10000\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for rotary_scaling_factor:40 with rotary_scaling_factor:1.0\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for mscale:1.0 with mscale:1.0\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for mscale_all_dim:0.0 with mscale_all_dim:0.0\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for cache_mla_latents:False with cache_mla_latents:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for dsa_indexer_use_sparse_loss:False with dsa_indexer_use_sparse_loss:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for linear_conv_kernel_dim:None with linear_conv_kernel_dim:4\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for linear_key_head_dim:None with linear_key_head_dim:128\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for linear_value_head_dim:None with linear_value_head_dim:128\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for linear_num_key_heads:None with linear_num_key_heads:16\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for linear_num_value_heads:None with linear_num_value_heads:32\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for init_method_std:0.02 with init_method_std:0.02\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for init_model_with_meta_device:False with init_model_with_meta_device:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for apply_query_key_layer_scaling:False with apply_query_key_layer_scaling:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for attention_softmax_in_fp32:True with attention_softmax_in_fp32:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for disable_bf16_reduced_precision_matmul:False with disable_bf16_reduced_precision_matmul:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for bias_swiglu_fusion:True with bias_swiglu_fusion:True\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for masked_softmax_fusion:True with masked_softmax_fusion:True\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for bias_dropout_fusion:True with bias_dropout_fusion:True\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for apply_rope_fusion:True with apply_rope_fusion:True\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for use_fused_weighted_squared_relu:False with use_fused_weighted_squared_relu:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for distribute_saved_activations:None with distribute_saved_activations:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for fp8_recipe:delayed with fp8_recipe:delayed\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for fp8_margin:0 with fp8_margin:0\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for fp8_interval:1 with fp8_interval:1\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for fp8_amax_history_len:1 with fp8_amax_history_len:1\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for fp8_amax_compute_algo:most_recent with fp8_amax_compute_algo:most_recent\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for fp8_wgrad:True with fp8_wgrad:True\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for first_last_layers_bf16:False with first_last_layers_bf16:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for num_layers_at_start_in_bf16:1 with num_layers_at_start_in_bf16:1\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for num_layers_at_end_in_bf16:1 with num_layers_at_end_in_bf16:1\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for fp4_recipe:nvfp4 with fp4_recipe:nvfp4\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for fp4_param:False with fp4_param:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for moe_shared_expert_gate:False with moe_shared_expert_gate:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for moe_shared_expert_overlap:False with moe_shared_expert_overlap:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for moe_layer_freq:1 with moe_layer_freq:1\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for moe_router_load_balancing_type:aux_loss with moe_router_load_balancing_type:aux_loss\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for moe_router_topk:2 with moe_router_topk:2\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for moe_enable_routing_replay:False with moe_enable_routing_replay:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for moe_router_padding_for_quantization:False with moe_router_padding_for_quantization:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for moe_router_padding_for_fp8:False with moe_router_padding_for_fp8:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for moe_router_pre_softmax:False with moe_router_pre_softmax:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for moe_router_score_function:softmax with moe_router_score_function:softmax\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for moe_router_enable_expert_bias:False with moe_router_enable_expert_bias:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for moe_router_bias_update_rate:0.001 with moe_router_bias_update_rate:0.001\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for moe_router_force_load_balancing:False with moe_router_force_load_balancing:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for moe_grouped_gemm:False with moe_grouped_gemm:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for moe_use_legacy_grouped_gemm:False with moe_use_legacy_grouped_gemm:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for moe_aux_loss_coeff:0 with moe_aux_loss_coeff:0.0\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for moe_token_dispatcher_type:allgather with moe_token_dispatcher_type:allgather\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for moe_enable_deepep:False with moe_enable_deepep:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for moe_flex_dispatcher_backend:deepep with moe_flex_dispatcher_backend:deepep\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for moe_per_layer_logging:False with moe_per_layer_logging:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for moe_pad_expert_input_to_capacity:False with moe_pad_expert_input_to_capacity:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for moe_token_drop_policy:probs with moe_token_drop_policy:probs\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for moe_layer_recompute:False with moe_layer_recompute:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for moe_permute_fusion:False with moe_permute_fusion:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for moe_router_fusion:False with moe_router_fusion:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for moe_apply_probs_on_input:False with moe_apply_probs_on_input:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for moe_deepep_num_sms:20 with moe_deepep_num_sms:20\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for moe_hybridep_num_sms:16 with moe_hybridep_num_sms:16\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for cp_comm_type:None with cp_comm_type:['p2p']\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for enable_cuda_graph:False with enable_cuda_graph:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for cuda_graph_warmup_steps:3 with cuda_graph_warmup_steps:3\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for external_cuda_graph:False with external_cuda_graph:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for cuda_graph_impl:none with cuda_graph_impl:none\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for cuda_graph_scope:full with cuda_graph_scope:[]\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for clone_scatter_output_in_embedding:True with clone_scatter_output_in_embedding:True\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for config_logger_dir: with config_logger_dir:\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for flash_decode:False with flash_decode:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for batch_invariant_mode:False with batch_invariant_mode:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for use_te_activation_func:False with use_te_activation_func:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for inference_rng_tracker:False with inference_rng_tracker:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for inference_fuse_tp_communication:False with inference_fuse_tp_communication:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for is_hybrid_model:False with is_hybrid_model:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for mamba_state_dim:128 with mamba_state_dim:128\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for mamba_head_dim:64 with mamba_head_dim:64\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for mamba_num_groups:8 with mamba_num_groups:8\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for mlp_chunks_for_prefill:1 with mlp_chunks_for_prefill:1\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for transformer_impl:local with transformer_impl:transformer_engine\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for fine_grained_activation_offloading:False with fine_grained_activation_offloading:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for offload_modules:None with offload_modules:[]\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for min_offloaded_tensor_size:1048576 with min_offloaded_tensor_size:1048576\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for tensor_model_parallel_size:1 with tensor_model_parallel_size:1\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for pipeline_model_parallel_size:1 with pipeline_model_parallel_size:1\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for sequence_parallel:False with sequence_parallel:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for context_parallel_size:1 with context_parallel_size:1\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for hybrid_context_parallel:False with hybrid_context_parallel:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for expert_model_parallel_size:1 with expert_model_parallel_size:1\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for expert_tensor_parallel_size:None with expert_tensor_parallel_size:1\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for moe_extended_tp:False with moe_extended_tp:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for perform_initialization:True with perform_initialization:True\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for fp16:False with fp16:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for bf16:True with bf16:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for deterministic_mode:False with deterministic_mode:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for gradient_accumulation_fusion:True with gradient_accumulation_fusion:True\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for async_tensor_model_parallel_allreduce:True with async_tensor_model_parallel_allreduce:True\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for tp_comm_overlap:False with tp_comm_overlap:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for tp_comm_bulk_wgrad:True with tp_comm_bulk_wgrad:True\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for tp_comm_bulk_dgrad:True with tp_comm_bulk_dgrad:True\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for tp_comm_overlap_ag:True with tp_comm_overlap_ag:True\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for tp_comm_overlap_rs:True with tp_comm_overlap_rs:True\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for tp_comm_overlap_rs_dgrad:False with tp_comm_overlap_rs_dgrad:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for tp_comm_split_ag:True with tp_comm_split_ag:True\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for tp_comm_split_rs:True with tp_comm_split_rs:True\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for cross_entropy_loss_fusion:False with cross_entropy_loss_fusion:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for cross_entropy_fusion_impl:native with cross_entropy_fusion_impl:native\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for tp_comm_bootstrap_backend:nccl with tp_comm_bootstrap_backend:nccl\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for overlap_moe_expert_parallel_comm:False with overlap_moe_expert_parallel_comm:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for delay_wgrad_compute:False with delay_wgrad_compute:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for ep_overlap_early_attn_memory_release:False with ep_overlap_early_attn_memory_release:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for overlap_p2p_comm:False with overlap_p2p_comm:True\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for use_ring_exchange_p2p:False with use_ring_exchange_p2p:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for defer_embedding_wgrad_compute:False with defer_embedding_wgrad_compute:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for wgrad_deferral_limit:0 with wgrad_deferral_limit:0\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for overlap_p2p_comm_warmup_flush:False with overlap_p2p_comm_warmup_flush:False\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for cpu_offloading_num_layers:0 with cpu_offloading_num_layers:0\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Overriding default arguments for barrier_with_L1_time:True with barrier_with_L1_time:True\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  one_logger_project .............................. megatron-lm\n",
      "  one_logger_run_name ............................. None\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  optimizer_cpu_offload ........................... False\n",
      "  optimizer_offload_fraction ...................... 1.0\n",
      "  original_max_position_embeddings ................ 4096\n",
      "  outer_dp_sharding_strategy ...................... no_shard\n",
      "  output_bert_embeddings .......................... False\n",
      "  output_layer_init_method ........................ kaiming_uniform\n",
      "  overlap_cpu_optimizer_d2h_h2d ................... False\n",
      "  overlap_grad_reduce ............................. False\n",
      "  overlap_moe_expert_parallel_comm ................ False\n",
      "  overlap_p2p_comm ................................ False\n",
      "  overlap_p2p_comm_warmup_flush ................... False\n",
      "  overlap_param_gather ............................ False\n",
      "  overlap_param_gather_with_optimizer_step ........ False\n",
      "  override_opt_param_scheduler .................... False\n",
      "  padded_vocab_size ............................... 100048\n",
      "  param_sync_func ................................. None\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  per_dataset_sequences_path ...................... None\n",
      "  per_split_data_args_path ........................ None\n",
      "  perform_initialization .......................... True\n",
      "  perform_rl_step ................................. False\n",
      "  persist_layer_norm .............................. False\n",
      "  phase_transition_iterations ..................... None\n",
      "  pin_cpu_grads ................................... True\n",
      "  pin_cpu_params .................................. True\n",
      "  pipeline_dtype .................................. None\n",
      "  pipeline_model_parallel_comm_backend ............ nccl\n",
      "  pipeline_model_parallel_layout .................. None\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  position_embedding_type ......................... learned_absolute\n",
      "  pretrained_checkpoint ........................... None\n",
      "  profile ......................................... False\n",
      "  profile_ranks ................................... [0]\n",
      "  profile_step_end ................................ 12\n",
      "  profile_step_start .............................. 10\n",
      "  q_lora_rank ..................................... 64\n",
      "  qk_clip ......................................... False\n",
      "  qk_clip_alpha ................................... 0.5\n",
      "  qk_clip_threshold ............................... 100\n",
      "  qk_head_dim ..................................... 128\n",
      "  qk_l2_norm ...................................... False\n",
      "  qk_layernorm .................................... False\n",
      "  qk_pos_emb_head_dim ............................. 64\n",
      "  quant_recipe .................................... None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  quick_geglu ..................................... False\n",
      "  rampup_batch_size ............................... [32, 32, 65324160]\n",
      "  rank ............................................ 0\n",
      "  recompute_granularity ........................... None\n",
      "  recompute_method ................................ None\n",
      "  recompute_modules ............................... None\n",
      "  recompute_num_layers ............................ None\n",
      "  record_memory_history ........................... False\n",
      "  refit_method .................................... gloo\n",
      "  relative_attention_max_distance ................. 128\n",
      "  relative_attention_num_buckets .................. 32\n",
      "  replication ..................................... False\n",
      "  replication_factor .............................. 2\n",
      "  replication_jump ................................ None\n",
      "  rerun_mode ...................................... validate_results\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  result_rejected_tracker_filename ................ None\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  reuse_grad_buf_for_mxfp8_param_ag ............... False\n",
      "  rl_calculate_intra_group_similarity ............. False\n",
      "  rl_default_temperature .......................... 1.0\n",
      "  rl_default_top_k ................................ -1\n",
      "  rl_default_top_p ................................ 0\n",
      "  rl_importance_sampling_truncation_coef .......... None\n",
      "  rl_inference_expert_model_parallel_size ......... None\n",
      "  rl_inference_expert_tensor_model_parallel_size .. None\n",
      "  rl_inference_logprobs_is_correction ............. False\n",
      "  rl_inference_model_unified_memory_level ......... 0\n",
      "  rl_inference_pipeline_model_parallel_size ....... None\n",
      "  rl_inference_tensor_model_parallel_size ......... None\n",
      "  rl_offload_inference_model_weights_when_idle .... False\n",
      "  rl_offload_kv_cache_during_training ............. False\n",
      "  rl_offload_optimizer_during_inference ........... False\n",
      "  rl_parallel_generation_tasks .................... 512\n",
      "  rl_partial_rollouts ............................. False\n",
      "  rl_prompts_per_eval ............................. 32\n",
      "  rl_remove_kv_cache_during_training .............. False\n",
      "  rl_reset_cuda_graphs ............................ False\n",
      "  rl_sequence_packing_algo ........................ fifo\n",
      "  rl_sequence_packing_max_sequences_per_bin ....... 50\n",
      "  rl_skip_bos_token ............................... False\n",
      "  rl_training_cuda_graphs ......................... False\n",
      "  rl_use_sequence_packing ......................... False\n",
      "  rl_verify_model_weights_swap .................... False\n",
      "  rope_scaling_factor ............................. 8.0\n",
      "  rope_type ....................................... yarn\n",
      "  rotary_base ..................................... 10000\n",
      "  rotary_interleaved .............................. False\n",
      "  rotary_percent .................................. 1.0\n",
      "  rotary_scaling_factor ........................... 1.0\n",
      "  rotary_seq_len_interpolation_factor ............. None\n",
      "  run_workload_inspector_server ................... False\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ None\n",
      "  save_dgrads_interval ............................ None\n",
      "  save_interval ................................... 20000\n",
      "  save_retain_interval ............................ None\n",
      "  save_wgrads_interval ............................ None\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sequence_parallel ............................... False\n",
      "  sft ............................................. False\n",
      "  sft_tokenizer_prompt_format ..................... nemotron-h-aligned\n",
      "  sgd_momentum .................................... 0.9\n",
      "  sharp_enabled_group ............................. None\n",
      "  short_seq_prob .................................. 0.1\n",
      "  skip_train ...................................... False\n",
      "  skipped_train_samples ........................... 0\n",
      "  softmax_scale ................................... None\n",
      "  softmax_type .................................... vanilla\n",
      "  spec ............................................ ['models.bidirectional.bidirGPT', 'get_bidirectinoal_gpt_layer_with_transformer_engine_spec']\n",
      "  split ........................................... 99,1,0\n",
      "  squared_relu .................................... False\n",
      "  standalone_embedding_stage ...................... False\n",
      "  start_weight_decay .............................. 0.01\n",
      "  straggler_ctrlr_port ............................ 65535\n",
      "  straggler_minmax_count .......................... 1\n",
      "  strict_fsdp_dtensor_load ........................ True\n",
      "  suggested_communication_unit_size ............... None\n",
      "  swiglu .......................................... False\n",
      "  swin_backbone_type .............................. tiny\n",
      "  symmetric_ar_type ............................... None\n",
      "  te_precision_config_file ........................ None\n",
      "  te_rng_tracker .................................. False\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  test_data_path .................................. None\n",
      "  test_mode ....................................... False\n",
      "  tiktoken_num_special_tokens ..................... 1000\n",
      "  tiktoken_pattern ................................ None\n",
      "  tiktoken_special_tokens ......................... None\n",
      "  timers .......................................... None\n",
      "  timing_log_level ................................ 0\n",
      "  timing_log_option ............................... minmax\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_hf_include_special_tokens ............. False\n",
      "  tokenizer_hf_use_fast ........................... False\n",
      "  tokenizer_metadata .............................. None\n",
      "  tokenizer_model ................................. NousResearch/Meta-Llama-3-8B-Alternate-Tokenizer\n",
      "  tokenizer_sentencepiece_legacy .................. False\n",
      "  tokenizer_special_tokens ........................ None\n",
      "  tokenizer_type .................................. HuggingFaceTokenizer\n",
      "  torch_fsdp2_reshard_after_forward ............... True\n",
      "  tp_comm_atomic_ag ............................... False\n",
      "  tp_comm_atomic_rs ............................... False\n",
      "  tp_comm_bootstrap_backend ....................... nccl\n",
      "  tp_comm_bulk_dgrad .............................. True\n",
      "  tp_comm_bulk_wgrad .............................. True\n",
      "  tp_comm_overlap ................................. False\n",
      "  tp_comm_overlap_ag .............................. True\n",
      "  tp_comm_overlap_cfg ............................. None\n",
      "  tp_comm_overlap_disable_fc1 ..................... False\n",
      "  tp_comm_overlap_disable_qkv ..................... False\n",
      "  tp_comm_overlap_rs .............................. True\n",
      "  tp_comm_overlap_rs_dgrad ........................ False\n",
      "  tp_comm_split_ag ................................ True\n",
      "  tp_comm_split_rs ................................ True\n",
      "  tp_only_amax_red ................................ False\n",
      "  train_data_path ................................. None\n",
      "  train_iters ..................................... None\n",
      "  train_samples ................................... 268554688\n",
      "  train_sync_interval ............................. None\n",
      "  transformer_impl ................................ transformer_engine\n",
      "  transformer_pipeline_model_parallel_size ........ 1\n",
      "  trust_remote_code ............................... False\n",
      "  untie_embeddings_and_output_weights ............. False\n",
      "  use_checkpoint_args ............................. False\n",
      "  use_checkpoint_opt_param_scheduler .............. False\n",
      "  use_cpu_initialization .......................... False\n",
      "  use_dist_ckpt ................................... True\n",
      "  use_dist_ckpt_deprecated ........................ False\n",
      "  use_distributed_optimizer ....................... False\n",
      "  use_flash_attn .................................. False\n",
      "  use_fused_weighted_squared_relu ................. False\n",
      "  use_inference_optimized_layers .................. False\n",
      "  use_kitchen ..................................... False\n",
      "  use_kitchen_attention ........................... False\n",
      "  use_legacy_models ............................... False\n",
      "  use_legacy_static_engine ........................ False\n",
      "  use_mamba_mem_eff_path .......................... True\n",
      "  use_megatron_fsdp ............................... False\n",
      "  use_mp_args_from_checkpoint_args ................ False\n",
      "  use_one_sent_docs ............................... False\n",
      "  use_persistent_ckpt_worker ...................... False\n",
      "  use_precision_aware_optimizer ................... False\n",
      "  use_pytorch_profiler ............................ False\n",
      "  use_ring_exchange_p2p ........................... False\n",
      "  use_rope_scaling ................................ False\n",
      "  use_rotary_position_embeddings .................. False\n",
      "  use_sharp ....................................... False\n",
      "  use_te_activation_func .......................... False\n",
      "  use_te_rng_tracker .............................. False\n",
      "  use_tokenizer_model_from_checkpoint_args ........ True\n",
      "  use_torch_fsdp2 ................................. False\n",
      "  use_torch_optimizer_for_cpu_offload ............. False\n",
      "  use_tp_pp_dp_mapping ............................ False\n",
      "  v_head_dim ...................................... 128\n",
      "  valid_data_path ................................. None\n",
      "  variable_seq_lengths ............................ False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vision_backbone_type ............................ vit\n",
      "  vision_pretraining .............................. False\n",
      "  vision_pretraining_type ......................... classify\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... None\n",
      "  vocab_size ...................................... 100042\n",
      "  wandb_entity .................................... None\n",
      "  wandb_exp_name .................................. \n",
      "  wandb_project ................................... \n",
      "  wandb_save_dir .................................. \n",
      "  weight_decay .................................... 0.01\n",
      "  weight_decay_incr_style ......................... constant\n",
      "  wgrad_deferral_limit ............................ 0\n",
      "  window_attn_skip_freq ........................... None\n",
      "  window_size ..................................... None\n",
      "  world_size ...................................... 1\n",
      "  yaml_cfg ........................................ /home/ubuntu/projects/llm-models/llm-models/models/bidirectional/gpt_config.yaml\n",
      "-------------------- end of arguments ---------------------\n",
      "INFO:megatron.core.num_microbatches_calculator:will use batch size rampup starting from global batch size 32 to global batch size 128 with batchsize increments 32 over 65324160 samples.\n",
      "INFO:megatron.core.num_microbatches_calculator:setting initial batch size to 32\n",
      "> building HuggingFaceTokenizer tokenizer ...\n",
      "WARNING:megatron.core.datasets.megatron_tokenizer:You're using the legacy tokenizer system, which is deprecated and will be removed in a future release. Please migrate to the new tokenizer system (`megatron.core.tokenizers.MegatronTokenizer`).\n",
      "WARNING: one_logger package is required to enable e2e metrics tracking. please go to https://confluence.nvidia.com/display/MLWFO/Package+Repositories for details to install it\n",
      "WARNING:megatron.core.rerun_state_machine:RerunStateMachine initialized in mode RerunMode.VALIDATE_RESULTS\n",
      "torch distributed is already initialized, skipping initialization ...\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "> initialized tensor model parallel with size 1\n",
      "> initialized pipeline model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> compiling dataset index builder ...\n",
      "make: Entering directory '/home/ubuntu/projects/Megatron-LM/megatron/core/datasets'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/home/ubuntu/projects/Megatron-LM/megatron/core/datasets'\n",
      ">>> done with dataset index builder. Compilation time: 0.002 seconds\n",
      "> compiling and loading fused kernels ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "make: python3-config: No such file or directory\n",
      "make: python3-config: No such file or directory\n",
      "/home/ubuntu/projects/Megatron-LM/megatron/training/utils.py:410: UserWarning: Constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.\n",
      "  warnings.warn(message)\n",
      "/home/ubuntu/projects/llm-dev/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
      "  warnings.warn(  # warn only once\n",
      "[rank0]:[W203 08:40:22.340820603 ProcessGroupNCCL.cpp:5068] Guessing device ID based on global rank. This can cause a hang if rank to GPU mapping is heterogeneous. You can specify device_id in init_process_group()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> done with compiling and loading fused kernels. Compilation time: 0.345 seconds\n",
      "building GPT model ...\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from megatron.training.yaml_arguments import load_yaml, core_transformer_config_from_yaml\n",
    "from megatron.training import get_args\n",
    "from types import SimpleNamespace\n",
    "\n",
    "from megatron.training.initialize import initialize_megatron\n",
    "\n",
    "# Load YAML and build transformer config once\n",
    "args_from_yaml = load_yaml(\"/home/ubuntu/projects/llm-models/llm-models/models/bidirectional/gpt_config.yaml\")\n",
    "# args_dict = vars(args_from_yaml).copy()\n",
    "\n",
    "transformer_config = core_transformer_config_from_yaml(args_from_yaml, \"language_model\")\n",
    "\n",
    "transfomer_key = 'language_model'\n",
    "# 1. Convert the base YAML object to a dictionary\n",
    "args_dict = vars(args_from_yaml).copy()\n",
    "\n",
    "# 2. Safely merge the nested dictionaries\n",
    "# Using .update() ensures that if a key exists in both, the nested one takes precedence\n",
    "transfomer_key = 'language_model'\n",
    "if hasattr(args_from_yaml, transfomer_key):\n",
    "    args_dict.update(vars(getattr(args_from_yaml, transfomer_key)))\n",
    "\n",
    "if hasattr(args_from_yaml, 'model_parallel'):\n",
    "    args_dict.update(vars(args_from_yaml.model_parallel))\n",
    "\n",
    "# 3. Create the namespace from the merged dictionary\n",
    "args = SimpleNamespace(**args_dict)\n",
    "\n",
    "initialize_megatron(\n",
    "    extra_args_provider=None,\n",
    "    args_defaults=vars(args),\n",
    "    ignore_unknown_args=True # Useful if your YAML has extra fields\n",
    ")\n",
    "\n",
    "# Provide gpt_builder with that config (config=... skips internal config loading)\n",
    "def model_provider_with_yaml_config(pre_process=True, post_process=True, vp_stage=None):\n",
    "    # You must still have get_args() returning the full args (e.g. from same YAML)\n",
    "    # args = get_args()\n",
    "\n",
    "        # Initalize and get arguments, timers, and Tensorboard writer.\n",
    "    extra_args_provider, args_defaults, get_embedding_ranks, get_position_embedding_ranks = None, None, None, None\n",
    "    store = None\n",
    "    \n",
    "    return gpt_builder(args, pre_process, post_process, vp_stage, config=transformer_config)\n",
    "\n",
    "model = model_provider_with_yaml_config()\n",
    "# Then in pretrain:\n",
    "# pretrain(..., partial(model_provider, model_provider_with_yaml_config), ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e62d00-401a-4649-bfb3-7fe05d2be9ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "989f855f-baa4-44e0-b1bd-d91c43f97699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 140.8388671875 MB\n",
      "Reserved: 158.0 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"Allocated:\", torch.cuda.memory_allocated() / 1024**2, \"MB\")\n",
    "print(\"Reserved:\", torch.cuda.memory_reserved() / 1024**2, \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edbd9d43-85ba-458b-a134-7b5a92762a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbd84560-51cb-4c2c-bc21-915c0ec01c2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (embedding): LanguageModelEmbedding(\n",
       "    (word_embeddings): VocabParallelEmbedding()\n",
       "    (embedding_dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (decoder): TransformerBlock(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x BidirTransformerLayer(\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (self_attention): BidirMLASelfAttention(\n",
       "          (core_attention): FlexDotProductAttention()\n",
       "          (linear_proj): TERowParallelLinear(in_features=1024, out_features=128, bias=False, TP=1)\n",
       "          (rotary_pos_emb): YarnRotaryEmbedding()\n",
       "          (linear_q_down_proj): TELinear()\n",
       "          (linear_q_up_proj): TEColumnParallelLinear(in_features=512, out_features=1536, bias=False, TP=1)\n",
       "          (linear_kv_down_proj): TELinear()\n",
       "          (linear_kv_up_proj): TEColumnParallelLinear(in_features=512, out_features=2048, bias=False, TP=1)\n",
       "          (q_layernorm): IdentityOp()\n",
       "          (kv_layernorm): IdentityOp()\n",
       "          (core_attention_backward): FlexDotProductAttention()\n",
       "          (bidir_attention_forward): FlexDotProductAttention()\n",
       "          (bidir_attention_backward): FlexDotProductAttention()\n",
       "        )\n",
       "        (pre_cross_attn_layernorm): IdentityOp()\n",
       "        (cross_attention): IdentityOp()\n",
       "        (cross_attn_bda): IdentityFuncOp()\n",
       "        (pre_mlp_layernorm): IdentityOp()\n",
       "        (mlp): MLP(\n",
       "          (linear_fc1): TELayerNormColumnParallelLinear(in_features=128, out_features=1024, bias=False, TP=1)\n",
       "          (linear_fc2): TERowParallelLinear(in_features=512, out_features=128, bias=False, TP=1)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layernorm): RMSNorm()\n",
       "  )\n",
       "  (output_layer): ColumnParallelLinear(in_features=128, out_features=100048, bias=False, TP=1)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "200ccb1e-a890-4f2d-bfc1-b803f52b918b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embedding_shape = model.embedding.word_embeddings.weight.shape\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    model.embedding.word_embeddings.weight.copy_(torch.zeros(embedding_shape))\n",
    "    model.embedding.word_embeddings.weight[0].copy_(torch.rand(embedding_shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58c5c6ba-6965-4127-bbda-0b19e8e41e4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.0290, 0.4019, 0.2598,  ..., 0.6084, 0.0928, 0.0719],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        ...,\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding.word_embeddings.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "193219cc-86e6-4747-9d66-1c897549b10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationMonitor:\n",
    "    def __init__(self, model):\n",
    "        self.hooks = []\n",
    "        self.activations = dict()\n",
    "        \n",
    "        for name, module in model.named_modules():\n",
    "            # Filter for layers you care about (e.g., Norm, Linear, Attention)\n",
    "            # if isinstance(module, (torch.nn.Linear, torch.nn.LayerNorm)):\n",
    "            hook = module.register_forward_hook(self.make_forward_hook(name))\n",
    "            self.hooks.append(hook)\n",
    "\n",
    "    def make_forward_hook(self, name):\n",
    "        def hook(module, act_input, act_output):\n",
    "            # grad_output[0] is the gradient of the loss w.r.t. the output of this layer\n",
    "        \n",
    "            self.activations[name] = act_output\n",
    "                    \n",
    "        return hook\n",
    "        \n",
    "    def remove(self):\n",
    "        for h in self.hooks:\n",
    "            h.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e777cbf5-e3f5-417e-8e41-1b2e9d42cc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor = ActivationMonitor(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a385bac-52a8-4586-8d74-95d344f3b3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# monitor.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2062c964-3264-47d7-a172-0b761c86b4e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.0290, 0.4019, 0.2598,  ..., 0.6084, 0.0928, 0.0719],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        ...,\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding.word_embeddings.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88588d9b-90e5-419f-a938-04dd74c56f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86332f6c-43cf-4014-bf6b-2a40b57007ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.ones(2,128, dtype=torch.int32)\n",
    "input_ids = input_ids.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e413772-25ce-4815-a71f-d9eaf6585daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids[0, 20] = 0\n",
    "input_ids[0, 30] = 0\n",
    "input_ids[0, 50] = 0\n",
    "\n",
    "input_ids[1, 15] = 0\n",
    "input_ids[1, 25] = 0\n",
    "input_ids[1, 45] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11744a1c-143b-49a4-a1a1-410eca8d7a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0', dtype=torch.int32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9cbfb19c-3294-4e60-b091-4ddd1ade04e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 2, 128])\n",
      "core_attn_out_backward_to_backward\n",
      "tensor([   0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "           0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "           0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "           0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "           0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "           0.0000,   71.8074,   68.5434,   65.5633,   62.8315,   60.3182,\n",
      "          57.9983,   55.8502,   53.8556,   51.9985,   50.2652,   97.2874,\n",
      "          94.2472,   91.3912,   88.7033,   86.1689,   83.7753,   81.5111,\n",
      "          79.3661,   77.3310, 1050.9423,   73.5588,   71.8074,   70.1375,\n",
      "          68.5434,   67.0202,   65.5633,   64.1683,   62.8315,   61.5492,\n",
      "          60.3182,   88.7033,   86.9974,   85.3560,   83.7753,   82.2521,\n",
      "          80.7833,   79.3661,   77.9977,   76.6757,   75.3978,   74.1618,\n",
      "          72.9656,   71.8074,   70.6854,   69.5979,   68.5434,   67.5204,\n",
      "          66.5275,   65.5633,   64.6267,   63.7164,   62.8315,   61.9708,\n",
      "          61.1333,   60.3182,   59.5246,   58.7515,   57.9983,   57.2641,\n",
      "          56.5483,   55.8502,   55.1691,   54.5044,   53.8555,   53.2220,\n",
      "          52.6031,   51.9985,   51.4076,   50.8300,   50.2652,   49.7128,\n",
      "          49.1725,   48.6437,   48.1262,   47.6196,   47.1236,   46.6378,\n",
      "          46.1619,   45.6956,   45.2387,   44.7908,   44.3516,   43.9210,\n",
      "          43.4987,   43.0844,   42.6780,   42.2791,   41.8876,   41.5034,\n",
      "          41.1261,   40.7556,   40.3917,   40.0342,   39.6830,   39.3380,\n",
      "          38.9988,   38.6655], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "attn_out_forward_to_backward\n",
      "tensor([   0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "           0.0000,    0.0000,    0.0000,    0.0000,   71.8074,   68.5434,\n",
      "          65.5633,   62.8315,   60.3182,   57.9983,   55.8502,   53.8556,\n",
      "          51.9985,   50.2652,  754.4796,   94.2472,   91.3912,   88.7033,\n",
      "          86.1689,   83.7753,   81.5111,   79.3661,   77.3310,   75.3978,\n",
      "         743.7546,   71.8074,   70.1375,   68.5434,   67.0202,   65.5633,\n",
      "          64.1683,   62.8315,   61.5492,   60.3182,   88.7033,   86.9974,\n",
      "          85.3560,   83.7753,   82.2521,   80.7833,   79.3661,   77.9977,\n",
      "          76.6757,   75.3978, 1026.1721,   72.9656,   71.8074,   70.6854,\n",
      "          69.5979,   68.5434,   67.5204,   66.5275,   65.5633,   64.6267,\n",
      "          63.7164,   62.8315,   61.9708,   61.1333,   60.3182,   59.5246,\n",
      "          58.7515,   57.9983,   57.2641,   56.5483,   55.8502,   55.1691,\n",
      "          54.5044,   53.8555,   53.2220,   52.6031,   51.9985,   51.4076,\n",
      "          50.8300,   50.2652,   49.7128,   49.1725,   48.6437,   48.1262,\n",
      "          47.6196,   47.1236,   46.6378,   46.1619,   45.6956,   45.2387,\n",
      "          44.7908,   44.3516,   43.9210,   43.4987,   43.0844,   42.6780,\n",
      "          42.2791,   41.8876,   41.5034,   41.1261,   40.7556,   40.3917,\n",
      "          40.0342,   39.6830,   39.3380,   38.9988,   38.6655,   38.3378,\n",
      "          38.0157,   37.6989,   37.3873,   37.0809,   36.7794,   36.4828,\n",
      "          36.1909,   35.9037,   35.6210,   35.3427,   35.3427,   35.3427,\n",
      "          35.3427,   35.3427,   35.3427,   35.3427,   35.3427,   35.3427,\n",
      "          35.3427,   35.3427], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "attn_out_forward\n",
      "tensor([   0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "           0.0000,    0.0000,    0.0000,    0.0000,   71.8074,   68.5434,\n",
      "          65.5633,   62.8315,   60.3182,   57.9983,   55.8502,   53.8556,\n",
      "          51.9985,   50.2652,  754.4796,   94.2472,   91.3912,   88.7033,\n",
      "          86.1689,   83.7753,   81.5111,   79.3661,   77.3310,   75.3978,\n",
      "         743.7546,   71.8074,   70.1375,   68.5434,   67.0202,   65.5633,\n",
      "          64.1683,   62.8315,   61.5492,   60.3182,   88.7033,   86.9974,\n",
      "          85.3560,   83.7753,   82.2521,   80.7833,   79.3661,   77.9977,\n",
      "          76.6757,   75.3978, 1026.1721,   72.9656,   71.8074,   70.6854,\n",
      "          69.5979,   68.5434,   67.5204,   66.5275,   65.5633,   64.6267,\n",
      "          63.7164,   62.8315,   61.9708,   61.1333,   60.3182,   59.5246,\n",
      "          58.7515,   57.9983,   57.2641,   56.5483,   55.8502,   55.1691,\n",
      "          54.5044,   53.8555,   53.2220,   52.6031,   51.9985,   51.4076,\n",
      "          50.8300,   50.2652,   49.7128,   49.1725,   48.6437,   48.1262,\n",
      "          47.6196,   47.1236,   46.6378,   46.1619,   45.6956,   45.2387,\n",
      "          44.7908,   44.3516,   43.9210,   43.4987,   43.0844,   42.6780,\n",
      "          42.2791,   41.8876,   41.5034,   41.1261,   40.7556,   40.3917,\n",
      "          40.0342,   39.6830,   39.3380,   38.9988,   38.6655,   38.3378,\n",
      "          38.0157,   37.6989,   37.3873,   37.0809,   36.7794,   36.4828,\n",
      "          36.1909,   35.9037,   35.6210,   35.3427,   35.3427,   35.3427,\n",
      "          35.3427,   35.3427,   35.3427,   35.3427,   35.3427,   35.3427,\n",
      "          35.3427,   35.3427], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "torch.Size([128, 2, 128])\n",
      "core_attn_out_backward_to_backward\n",
      "tensor([   0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "           0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "           0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "           0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "           0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "           0.0000,  676.3876,  695.4316,  707.7841,  713.2551,  718.9582,\n",
      "         736.8419,  809.7061,  848.5236,  852.9082,  853.2212, 1382.7882,\n",
      "        1362.5566, 1308.0466, 1318.9192, 1338.3512, 1345.4758, 1340.6910,\n",
      "        1256.9824, 1093.7507, 1514.7849, 1361.0703, 1355.6503, 1438.3730,\n",
      "        1440.5526, 1340.9175, 1292.6974, 1288.0635, 1323.4663, 1317.1753,\n",
      "        1233.0554, 1214.0104, 1213.6980, 1252.3884, 1269.3127, 1270.7969,\n",
      "        1326.3933, 1318.8689, 1271.9456, 1318.2728, 1233.5161, 1304.4458,\n",
      "        1327.7686, 1297.3887, 1285.5310, 1312.0498, 1498.7410, 1519.0315,\n",
      "        1455.5071, 1407.2227, 1349.1765, 1370.3527, 1424.3353, 1440.1580,\n",
      "        1402.3802, 1365.4832, 1470.6188, 1521.0549, 1541.5371, 1531.3997,\n",
      "        1436.1309, 1406.1239, 1376.5172, 1447.4014, 1420.0728, 1332.5056,\n",
      "        1296.7036, 1318.0073, 1452.5350, 1426.4958, 1444.9004, 1455.9836,\n",
      "        1325.0425, 1371.5656, 1323.4536, 1270.5239, 1263.0422, 1264.7333,\n",
      "        1267.3210, 1263.9104, 1262.8508, 1278.1559, 1440.3020, 1570.5797,\n",
      "        1525.2521, 1447.6035, 1402.7012, 1417.5724, 1421.2007, 1410.7117,\n",
      "        1435.3590, 1457.3287, 1472.7610, 1479.0422, 1488.4082, 1499.8138,\n",
      "        1502.5010, 1499.9316], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "attn_out_forward_to_backward\n",
      "tensor([ 146.8827,  269.2543,  372.7806,  461.4993,  538.3746,  605.6274,\n",
      "         664.9583,  717.6884,  764.8559,  807.3060, 1311.2130, 1173.1392,\n",
      "        1257.0984, 1309.9622, 1305.7549, 1308.1919, 1349.2688, 1348.3275,\n",
      "        1293.2969, 1307.0269, 1456.6685, 1322.9932, 1328.2823, 1330.1929,\n",
      "        1334.7861, 1340.8770, 1370.0653, 1386.9097, 1364.8674, 1354.4948,\n",
      "        1479.3716, 1359.6470, 1373.5844, 1415.5061, 1415.0769, 1412.8523,\n",
      "        1387.7135, 1360.2236, 1366.5073, 1366.6912, 1515.7559, 1486.4210,\n",
      "        1423.0734, 1418.1448, 1417.8400, 1414.8950, 1413.2670, 1407.6448,\n",
      "        1369.1272, 1364.6589, 1535.7460, 1355.2162, 1344.9392, 1346.5605,\n",
      "        1353.6642, 1345.3701, 1341.9729, 1333.1416, 1344.9751, 1275.7161,\n",
      "        1174.2318, 1152.9592, 1162.2939, 1169.8535, 1206.0117, 1274.8943,\n",
      "        1247.1344, 1234.8389, 1350.1448, 1385.9814, 1315.4940, 1294.3906,\n",
      "        1302.0814, 1330.8097, 1369.7764, 1442.8856, 1499.8749, 1475.2693,\n",
      "        1439.4929, 1430.9255, 1429.0699, 1426.3029, 1419.8530, 1471.4818,\n",
      "        1430.1235, 1422.2711, 1487.5442, 1528.7324, 1502.7087, 1468.6359,\n",
      "        1438.8412, 1446.1993, 1429.7115, 1468.6624, 1449.7957, 1359.2651,\n",
      "        1320.9707, 1334.0281, 1396.5969, 1459.5886, 1455.4375, 1458.7524,\n",
      "        1337.7511, 1335.2988, 1319.7875, 1299.0029, 1297.1509, 1308.4706,\n",
      "        1323.4285, 1306.8357, 1287.5221, 1285.2362, 1393.2114, 1496.1406,\n",
      "        1490.7461, 1451.0515, 1423.6805, 1445.7659, 1464.6951, 1462.0629,\n",
      "        1470.5763, 1487.6587, 1485.9172, 1508.3523, 1493.8783, 1512.7589,\n",
      "        1517.4919, 1519.6395], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "attn_out_forward\n",
      "tensor([ 146.8827,  269.2543,  372.7806,  461.4993,  538.3746,  605.6274,\n",
      "         664.9583,  717.6884,  764.8559,  807.3060, 1305.2760, 1169.1696,\n",
      "        1251.1669, 1310.5659, 1305.5991, 1312.0232, 1350.3918, 1348.8275,\n",
      "        1290.4568, 1304.9561, 1424.3567, 1318.1162, 1331.8707, 1333.3348,\n",
      "        1327.2588, 1332.8618, 1362.4414, 1381.9733, 1359.9031, 1349.0613,\n",
      "        1450.0461, 1359.6470, 1373.1301, 1415.2668, 1415.0067, 1412.7051,\n",
      "        1387.9081, 1360.2937, 1365.9147, 1366.7427, 1513.0632, 1485.8806,\n",
      "        1423.4742, 1418.7948, 1418.5208, 1415.3621, 1413.5646, 1408.0292,\n",
      "        1369.7083, 1364.8540, 1527.1389, 1354.6945, 1344.9392, 1347.0569,\n",
      "        1353.7388, 1345.7273, 1342.0125, 1332.7563, 1344.9873, 1274.2937,\n",
      "        1172.8788, 1154.2103, 1164.1434, 1173.8005, 1215.8890, 1285.0858,\n",
      "        1251.4224, 1238.4771, 1361.3054, 1396.8009, 1323.9150, 1301.7615,\n",
      "        1308.5950, 1336.0402, 1377.6068, 1450.5593, 1502.8506, 1477.2356,\n",
      "        1442.4626, 1433.0254, 1432.0378, 1429.6521, 1423.1965, 1474.1528,\n",
      "        1433.1409, 1426.3218, 1489.5000, 1529.1335, 1503.5377, 1469.6307,\n",
      "        1440.3010, 1447.8384, 1431.6550, 1470.6715, 1453.0425, 1360.8945,\n",
      "        1322.0951, 1335.5945, 1397.9551, 1459.0137, 1454.9237, 1458.4434,\n",
      "        1339.7329, 1337.4473, 1321.9812, 1299.9421, 1297.9736, 1310.8267,\n",
      "        1326.9814, 1308.9495, 1287.2971, 1284.5583, 1394.2367, 1497.6250,\n",
      "        1491.6665, 1450.9830, 1425.2047, 1447.2891, 1466.8954, 1462.1531,\n",
      "        1469.6526, 1486.6233, 1484.3430, 1507.3551, 1492.4890, 1511.7002,\n",
      "        1516.5059, 1518.9280], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "torch.Size([128, 2, 128])\n",
      "core_attn_out_backward_to_backward\n",
      "tensor([   0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "           0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "           0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "           0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "           0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "           0.0000, 1610.3402, 1603.0225, 1596.6763, 1587.7329, 1580.0830,\n",
      "        1578.7494, 1590.9214, 1589.7981, 1586.6177, 1568.2040, 1468.8662,\n",
      "        1471.3057, 1439.1179, 1442.5791, 1480.9736, 1503.2676, 1454.6853,\n",
      "        1497.0323, 1476.7098, 1499.9832, 1491.0066, 1492.5004, 1490.3882,\n",
      "        1500.8427, 1531.8156, 1495.3267, 1519.7595, 1474.0458, 1480.0944,\n",
      "        1513.6873, 1500.0347, 1517.8986, 1487.5442, 1571.6185, 1566.0583,\n",
      "        1549.6055, 1546.6174, 1580.5636, 1560.9004, 1573.1797, 1587.8374,\n",
      "        1565.8098, 1540.5151, 1542.2554, 1548.2897, 1544.4913, 1522.4824,\n",
      "        1497.0237, 1513.3530, 1540.7649, 1532.6714, 1566.6831, 1502.9987,\n",
      "        1571.7341, 1553.8242, 1542.5120, 1530.8291, 1528.7920, 1548.9509,\n",
      "        1575.3601, 1585.2734, 1583.1768, 1538.4263, 1560.1584, 1536.8934,\n",
      "        1508.4357, 1510.4795, 1509.7656, 1486.8328, 1494.6897, 1502.5938,\n",
      "        1544.3770, 1500.8940, 1503.8260, 1516.8302, 1544.7183, 1550.4429,\n",
      "        1547.1956, 1538.9697, 1528.6604, 1510.4293, 1551.8555, 1542.6453,\n",
      "        1551.7037, 1545.0645, 1559.2502, 1537.5194, 1520.0688, 1492.0854,\n",
      "        1448.9452, 1447.3633, 1458.3728, 1468.9835, 1522.0293, 1509.5436,\n",
      "        1467.6650, 1479.8827], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "attn_out_forward_to_backward\n",
      "tensor([1522.2888, 1535.3973, 1499.3378, 1464.0256, 1455.6487, 1457.2612,\n",
      "        1481.8596, 1480.2563, 1480.7936, 1486.2085, 1459.2161, 1494.7157,\n",
      "        1508.2775, 1415.0024, 1377.3032, 1457.2048, 1436.7241, 1419.8718,\n",
      "        1428.6389, 1459.0735, 1516.5068, 1438.5258, 1499.2749, 1510.0988,\n",
      "        1524.1287, 1494.5016, 1467.2300, 1480.3098, 1493.2563, 1499.1361,\n",
      "        1520.9829, 1469.0107, 1522.3003, 1520.6895, 1516.3333, 1517.2405,\n",
      "        1513.5869, 1514.9539, 1474.3477, 1403.5558, 1479.5630, 1491.1913,\n",
      "        1500.2886, 1499.3043, 1485.9675, 1446.8828, 1443.0022, 1423.5833,\n",
      "        1457.8647, 1434.5103, 1492.0505, 1476.4769, 1485.2307, 1456.8052,\n",
      "        1464.0117, 1493.2720, 1469.0670, 1487.5869, 1447.7662, 1443.0105,\n",
      "        1530.2021, 1503.7627, 1525.1298, 1479.4265, 1531.2285, 1503.7812,\n",
      "        1484.8416, 1493.8396, 1517.6647, 1549.5150, 1547.2980, 1542.4117,\n",
      "        1555.8726, 1524.2651, 1515.3528, 1508.1472, 1519.0472, 1512.0886,\n",
      "        1480.0726, 1462.4197, 1489.8500, 1498.9812, 1557.5686, 1498.0020,\n",
      "        1531.3643, 1542.7228, 1536.4766, 1555.0649, 1532.6543, 1543.2728,\n",
      "        1566.2981, 1545.7112, 1554.2092, 1537.3506, 1541.8890, 1544.9059,\n",
      "        1553.0410, 1559.8740, 1545.1816, 1500.1876, 1462.8790, 1496.7289,\n",
      "        1554.7194, 1556.2443, 1562.1735, 1569.1820, 1576.9877, 1580.9929,\n",
      "        1572.0830, 1558.9213, 1549.8367, 1563.9197, 1578.9041, 1527.8669,\n",
      "        1535.6775, 1526.9067, 1546.7810, 1554.3767, 1549.2292, 1536.8035,\n",
      "        1469.0225, 1482.4532, 1482.1277, 1495.1917, 1540.0354, 1503.7766,\n",
      "        1464.6475, 1443.6011], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "attn_out_forward\n",
      "tensor([1514.3989, 1533.2163, 1497.5836, 1462.0161, 1459.7113, 1484.7474,\n",
      "        1498.1199, 1498.5330, 1497.3262, 1505.9304, 1499.7869, 1478.5587,\n",
      "        1533.8501, 1458.7838, 1474.0115, 1500.5972, 1508.9420, 1478.1573,\n",
      "        1468.0663, 1461.5276, 1475.5835, 1440.9700, 1482.7910, 1527.6892,\n",
      "        1529.4705, 1503.7983, 1481.4119, 1519.1569, 1527.9370, 1496.7620,\n",
      "        1525.6515, 1505.5435, 1517.3789, 1498.6646, 1498.7611, 1504.3484,\n",
      "        1499.1755, 1437.2228, 1444.5726, 1477.2919, 1502.9135, 1490.7661,\n",
      "        1492.7410, 1486.1887, 1478.0122, 1435.4717, 1449.6416, 1446.4492,\n",
      "        1480.7920, 1429.5134, 1501.1288, 1491.0603, 1504.5338, 1462.6462,\n",
      "        1462.2469, 1508.7295, 1460.8748, 1485.4407, 1467.6720, 1467.3872,\n",
      "        1523.1826, 1513.2439, 1516.6598, 1490.4668, 1538.5935, 1509.4822,\n",
      "        1524.9143, 1518.2888, 1520.3853, 1540.2123, 1530.4043, 1549.4540,\n",
      "        1556.8165, 1525.1970, 1516.9226, 1517.3263, 1519.2131, 1514.3577,\n",
      "        1493.0220, 1473.8806, 1486.5312, 1524.3070, 1532.5432, 1503.8127,\n",
      "        1532.6226, 1541.8313, 1540.9636, 1549.5741, 1535.3285, 1545.6176,\n",
      "        1539.8296, 1541.3036, 1533.3474, 1526.4901, 1535.8728, 1547.7939,\n",
      "        1551.5900, 1558.9402, 1539.7471, 1488.7559, 1455.0952, 1485.6963,\n",
      "        1554.6274, 1555.5322, 1562.2371, 1569.1582, 1577.0668, 1581.5330,\n",
      "        1572.0193, 1558.5637, 1551.1597, 1561.6508, 1576.1003, 1544.4023,\n",
      "        1545.1873, 1527.8167, 1548.5847, 1556.0287, 1550.4197, 1531.6687,\n",
      "        1468.3755, 1487.1326, 1483.7283, 1489.0023, 1536.4877, 1505.9952,\n",
      "        1466.6261, 1441.8284], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "torch.Size([128, 2, 128])\n",
      "core_attn_out_backward_to_backward\n",
      "tensor([   0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "           0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "           0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "           0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "           0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "           0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "           0.0000,    0.0000,    0.0000,    0.0000,    0.0000, 1592.3719,\n",
      "        1587.4119, 1596.5706, 1597.0579, 1598.1227, 1601.6921, 1593.9326,\n",
      "        1594.2493, 1617.2769, 1619.4668, 1585.8862, 1585.8254, 1604.5403,\n",
      "        1569.1437, 1597.9230, 1587.5533, 1614.2522, 1616.1589, 1596.4409,\n",
      "        1538.4058, 1535.4280, 1544.4335, 1564.5386, 1534.3889, 1501.9404,\n",
      "        1569.2686, 1604.0269, 1604.2640, 1578.7400, 1605.7314, 1599.0132,\n",
      "        1577.2252, 1575.6125, 1441.8757, 1558.4268, 1547.2080, 1547.3679,\n",
      "        1603.2456, 1595.6746, 1511.4741, 1498.8401, 1461.2446, 1556.6638,\n",
      "        1555.2900, 1559.0000, 1557.5979, 1538.3898, 1539.8665, 1553.2529,\n",
      "        1522.9130, 1550.1331, 1598.6018, 1609.0767, 1583.3572, 1581.1337,\n",
      "        1592.8015, 1583.5062, 1557.4365, 1578.0121, 1558.5482, 1577.0414,\n",
      "        1617.4825, 1625.7932, 1604.3494, 1593.4351, 1620.5881, 1600.3286,\n",
      "        1600.7639, 1565.8813, 1571.4883, 1569.5842, 1585.8201, 1620.7847,\n",
      "        1539.6284, 1572.0312, 1577.9873, 1585.0928, 1583.6660, 1578.1222,\n",
      "        1518.1818, 1547.4623, 1522.0923, 1499.3352, 1528.4792, 1555.4410,\n",
      "        1525.7440, 1523.7893], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "attn_out_forward_to_backward\n",
      "tensor([1571.4270, 1569.9121, 1501.6898, 1548.7979, 1537.0732, 1552.9846,\n",
      "        1562.0686, 1524.0348, 1561.4398, 1565.9858, 1503.2456, 1498.0840,\n",
      "        1548.3831, 1540.2021, 1518.0986, 1527.0317, 1526.3450, 1518.7261,\n",
      "        1518.4518, 1525.7402, 1505.6833, 1511.0204, 1496.2083, 1534.6616,\n",
      "        1556.9539, 1485.9375, 1457.3140, 1565.7053, 1561.1243, 1466.0940,\n",
      "        1411.4060, 1480.1976, 1475.3685, 1480.7585, 1473.3804, 1496.6609,\n",
      "        1502.8267, 1505.8423, 1514.3547, 1505.1488, 1541.8588, 1554.6792,\n",
      "        1551.9447, 1513.0505, 1533.0952, 1527.0469, 1543.7471, 1502.8561,\n",
      "        1453.0378, 1515.4939, 1571.9004, 1552.0032, 1539.1572, 1552.8762,\n",
      "        1564.6288, 1564.4109, 1552.4626, 1550.4982, 1534.6373, 1552.6329,\n",
      "        1580.3557, 1484.5999, 1484.4321, 1534.6426, 1553.7856, 1563.9188,\n",
      "        1589.2878, 1582.2332, 1617.5559, 1586.2832, 1569.2086, 1531.1655,\n",
      "        1527.3469, 1503.2493, 1499.1086, 1480.5103, 1474.4128, 1534.5659,\n",
      "        1570.9015, 1569.5640, 1521.4124, 1444.4401, 1521.2336, 1544.5999,\n",
      "        1553.6858, 1538.0948, 1554.9890, 1572.7417, 1571.4500, 1563.5635,\n",
      "        1541.2024, 1561.0974, 1542.7634, 1526.0254, 1546.4648, 1538.2046,\n",
      "        1543.9510, 1548.7340, 1511.8374, 1531.7534, 1513.7924, 1524.9961,\n",
      "        1570.9758, 1579.3308, 1527.4424, 1522.6265, 1543.1649, 1529.5583,\n",
      "        1538.5859, 1509.4421, 1511.2976, 1535.2410, 1575.4612, 1554.1824,\n",
      "        1479.0203, 1483.7229, 1538.1549, 1527.8065, 1553.9541, 1514.9506,\n",
      "        1573.7267, 1577.9497, 1564.2139, 1548.2280, 1426.6860, 1466.5922,\n",
      "        1537.0969, 1548.4243], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "attn_out_forward\n",
      "tensor([1571.6465, 1569.4342, 1537.0175, 1552.5514, 1564.0295, 1559.7114,\n",
      "        1563.9482, 1565.8451, 1523.9310, 1510.5249, 1490.4341, 1528.2197,\n",
      "        1552.8781, 1551.9960, 1548.4854, 1533.4915, 1542.8641, 1537.9119,\n",
      "        1512.8367, 1514.6161, 1486.8575, 1547.9163, 1496.7750, 1539.2000,\n",
      "        1528.2356, 1505.4524, 1458.9937, 1495.3496, 1526.1283, 1531.1970,\n",
      "        1500.2236, 1495.4717, 1485.1299, 1496.9957, 1506.9897, 1514.6115,\n",
      "        1507.3315, 1494.3542, 1516.3208, 1491.2893, 1503.5659, 1528.9039,\n",
      "        1543.0046, 1532.0514, 1535.1152, 1530.6628, 1546.7289, 1516.6366,\n",
      "        1461.9148, 1512.9159, 1569.3501, 1558.0601, 1556.6003, 1562.2180,\n",
      "        1535.0205, 1566.6935, 1532.1694, 1549.7517, 1529.3174, 1553.4818,\n",
      "        1559.1487, 1496.9224, 1490.4258, 1555.0591, 1554.9683, 1588.0905,\n",
      "        1595.9912, 1565.0688, 1630.6399, 1609.8787, 1578.7771, 1508.4476,\n",
      "        1538.6350, 1519.8242, 1539.1061, 1457.5039, 1467.7585, 1548.8882,\n",
      "        1573.3877, 1556.3037, 1545.8590, 1487.2979, 1522.6744, 1560.8589,\n",
      "        1545.1702, 1524.9205, 1559.1985, 1582.5336, 1581.1002, 1586.6637,\n",
      "        1577.5709, 1556.9705, 1548.2351, 1535.5759, 1553.7742, 1543.8955,\n",
      "        1548.1449, 1553.2349, 1535.3184, 1509.4849, 1511.8054, 1507.5131,\n",
      "        1554.1233, 1576.1193, 1533.0737, 1533.6086, 1546.6223, 1534.7010,\n",
      "        1535.4309, 1507.2310, 1506.4690, 1527.5203, 1572.1702, 1540.4283,\n",
      "        1490.9944, 1486.2500, 1517.4897, 1534.0795, 1543.9260, 1536.4083,\n",
      "        1572.8409, 1571.9572, 1559.0762, 1575.7476, 1438.7438, 1470.5708,\n",
      "        1531.1799, 1552.7329], device='cuda:0', grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "output = model(input_ids, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff55f0e0-6b0f-4fce-ae28-2d9135795f9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['embedding.word_embeddings', 'embedding.embedding_dropout', 'embedding', 'decoder.layers.0.input_layernorm', 'decoder.layers.0.self_attention.linear_q_down_proj', 'decoder.layers.0.self_attention.linear_kv_down_proj', 'decoder.layers.0.self_attention.q_layernorm', 'decoder.layers.0.self_attention.kv_layernorm', 'decoder.layers.0.self_attention.linear_q_up_proj', 'decoder.layers.0.self_attention.linear_kv_up_proj', 'decoder.layers.0.self_attention.core_attention_backward', 'decoder.layers.0.self_attention.bidir_attention_backward', 'decoder.layers.0.self_attention.bidir_attention_forward', 'decoder.layers.0.self_attention.linear_proj', 'decoder.layers.0.self_attention', 'decoder.layers.0.pre_cross_attn_layernorm', 'decoder.layers.0.cross_attention', 'decoder.layers.0.cross_attn_bda', 'decoder.layers.0.pre_mlp_layernorm', 'decoder.layers.0.mlp.linear_fc1', 'decoder.layers.0.mlp.linear_fc2', 'decoder.layers.0.mlp', 'decoder.layers.0', 'decoder.layers.1.input_layernorm', 'decoder.layers.1.self_attention.linear_q_down_proj', 'decoder.layers.1.self_attention.linear_kv_down_proj', 'decoder.layers.1.self_attention.q_layernorm', 'decoder.layers.1.self_attention.kv_layernorm', 'decoder.layers.1.self_attention.linear_q_up_proj', 'decoder.layers.1.self_attention.linear_kv_up_proj', 'decoder.layers.1.self_attention.core_attention_backward', 'decoder.layers.1.self_attention.bidir_attention_backward', 'decoder.layers.1.self_attention.bidir_attention_forward', 'decoder.layers.1.self_attention.linear_proj', 'decoder.layers.1.self_attention', 'decoder.layers.1.pre_cross_attn_layernorm', 'decoder.layers.1.cross_attention', 'decoder.layers.1.cross_attn_bda', 'decoder.layers.1.pre_mlp_layernorm', 'decoder.layers.1.mlp.linear_fc1', 'decoder.layers.1.mlp.linear_fc2', 'decoder.layers.1.mlp', 'decoder.layers.1', 'decoder.layers.2.input_layernorm', 'decoder.layers.2.self_attention.linear_q_down_proj', 'decoder.layers.2.self_attention.linear_kv_down_proj', 'decoder.layers.2.self_attention.q_layernorm', 'decoder.layers.2.self_attention.kv_layernorm', 'decoder.layers.2.self_attention.linear_q_up_proj', 'decoder.layers.2.self_attention.linear_kv_up_proj', 'decoder.layers.2.self_attention.core_attention_backward', 'decoder.layers.2.self_attention.bidir_attention_backward', 'decoder.layers.2.self_attention.bidir_attention_forward', 'decoder.layers.2.self_attention.linear_proj', 'decoder.layers.2.self_attention', 'decoder.layers.2.pre_cross_attn_layernorm', 'decoder.layers.2.cross_attention', 'decoder.layers.2.cross_attn_bda', 'decoder.layers.2.pre_mlp_layernorm', 'decoder.layers.2.mlp.linear_fc1', 'decoder.layers.2.mlp.linear_fc2', 'decoder.layers.2.mlp', 'decoder.layers.2', 'decoder.layers.3.input_layernorm', 'decoder.layers.3.self_attention.linear_q_down_proj', 'decoder.layers.3.self_attention.linear_kv_down_proj', 'decoder.layers.3.self_attention.q_layernorm', 'decoder.layers.3.self_attention.kv_layernorm', 'decoder.layers.3.self_attention.linear_q_up_proj', 'decoder.layers.3.self_attention.linear_kv_up_proj', 'decoder.layers.3.self_attention.core_attention_backward', 'decoder.layers.3.self_attention.bidir_attention_backward', 'decoder.layers.3.self_attention.bidir_attention_forward', 'decoder.layers.3.self_attention.linear_proj', 'decoder.layers.3.self_attention', 'decoder.layers.3.pre_cross_attn_layernorm', 'decoder.layers.3.cross_attention', 'decoder.layers.3.cross_attn_bda', 'decoder.layers.3.pre_mlp_layernorm', 'decoder.layers.3.mlp.linear_fc1', 'decoder.layers.3.mlp.linear_fc2', 'decoder.layers.3.mlp', 'decoder.layers.3', 'decoder.final_layernorm', 'decoder', 'output_layer', ''])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monitor.activations.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2717afd6-8a2e-4363-9cc7-14b575a26d44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1024])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monitor.activations['decoder.layers.0.self_attention.core_attention_backward'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "25662799-750d-4c6e-832f-99e4b5c7f88a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1024])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monitor.activations['decoder.layers.0.self_attention.core_attention_forward_to_backward'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ffb03f68-c6c9-48b6-90b9-a85b2719bc18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1024])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monitor.activations['decoder.layers.0.self_attention.core_attention_forward'][3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4e4254c7-007e-4993-8af2-295ae213d186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(monitor.activations['decoder.layers.0.self_attention.core_attention_forward'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "aa204b65-1ffa-49ce-aa4e-ae53a42894e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "       grad_fn=<_LinearBackward>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monitor.activations['decoder.layers.0.self_attention.linear_q_up_proj'][0]["
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fd0b99e9-7a30-4c71-85f2-d024ef5a2f3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0365, 0.0310, 0.0266, 0.0231, 0.0203, 0.0179, 0.0159, 0.0142, 0.0128,\n",
       "        0.0116, 0.4773, 0.0365, 0.0336, 0.0310, 0.0287, 0.0266, 0.0248, 0.0231,\n",
       "        0.0216, 0.0203, 0.4773, 0.0839, 0.0761, 0.0694, 0.0636, 0.0585, 0.0540,\n",
       "        0.0501, 0.0466, 0.0434, 0.1057, 0.0998, 0.0943, 0.0893, 0.0847, 0.0804,\n",
       "        0.0764, 0.0728, 0.0693, 0.0662, 0.4773, 0.0604, 0.0578, 0.0553, 0.0530,\n",
       "        0.0509, 0.0489, 0.0469, 0.0452, 0.0434, 0.0623, 0.0600, 0.0579, 0.0559,\n",
       "        0.0540, 0.0522, 0.0504, 0.0488, 0.0472, 0.0457, 0.0443, 0.0429, 0.0416,\n",
       "        0.0404, 0.0392, 0.0381, 0.0370, 0.0359, 0.0349, 0.0339, 0.0330, 0.0321,\n",
       "        0.0313, 0.0305, 0.0297, 0.0289, 0.0282, 0.0275, 0.0268, 0.0262, 0.0255,\n",
       "        0.0249, 0.0243, 0.0238, 0.0232, 0.0227, 0.0222, 0.0217, 0.0212, 0.0208,\n",
       "        0.0203, 0.0199, 0.0194, 0.0190, 0.0187, 0.0183, 0.0179, 0.0175, 0.0172,\n",
       "        0.0169, 0.0165, 0.0162, 0.0159, 0.0156, 0.0153, 0.0150, 0.0147, 0.0145,\n",
       "        0.0142, 0.0140, 0.0137, 0.0135, 0.0132, 0.0130, 0.0128, 0.0126, 0.0123,\n",
       "        0.0121, 0.0120, 0.0119, 0.0118, 0.0117, 0.0116, 0.0115, 0.0114, 0.0113,\n",
       "        0.0112, 0.0111], device='cuda:0', grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.abs(monitor.activations['decoder.layers.0.mlp'][0][:, 0, :]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d5564792-071b-4c4b-aba9-c78d4858985e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 2, 512])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monitor.activations['decoder.layers.2.self_attention.kv_layernorm'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b2249e6d-e0f1-4e2b-acbd-7a215e8665fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0756,  0.1465,  0.2092,  0.2620,  0.3079,  0.3436,  0.3740,  0.3990,\n",
       "         0.4162,  0.4311,  1.4825,  1.4497,  1.4253,  1.4049,  1.3814,  1.3611,\n",
       "         1.3433,  1.3297,  1.3216,  1.3129, -1.7523,  2.0350,  1.9856,  1.9344,\n",
       "         1.8830,  1.8321,  1.7864,  1.7571,  1.7371,  1.7264, -1.7528,  1.1020,\n",
       "         1.1481,  1.1880,  1.2228,  1.2453,  1.2774,  1.2982,  1.3299,  1.3458,\n",
       "         1.5958,  2.3061,  2.2492,  2.2012,  2.1502,  2.1090,  2.0653,  2.0375,\n",
       "         2.0174,  2.0075, -1.7450,  2.5770,  2.5405,  2.5012,  2.4693,  2.4347,\n",
       "         2.4002,  2.3598,  2.3247,  2.2960,  2.2709,  2.0933,  2.0839,  2.0754,\n",
       "         2.0612,  2.0494,  2.0353,  2.0126,  1.9946,  1.9783,  1.9650,  2.2816,\n",
       "         2.2746,  2.2670,  2.2602,  2.2480,  2.2308,  2.2065,  2.1903,  2.1726,\n",
       "         2.1616,  2.1571,  2.1436,  2.1345,  2.1295,  2.1181,  2.1087,  2.0955,\n",
       "         2.0807,  2.0716,  2.0572,  2.0454,  2.0361,  2.0337,  2.0222,  2.0202,\n",
       "         2.0097,  2.0089,  2.0041,  1.9938,  1.9855,  1.9787,  1.9655,  1.9602,\n",
       "         1.9551,  1.9510,  1.9515,  1.9422,  1.9427,  1.9298,  1.9262,  1.9201,\n",
       "         1.9093,  1.8990,  1.8930,  1.8898,  1.8870,  1.8871,  1.8856,  1.8791,\n",
       "         1.8774,  1.8690,  1.8665,  1.8610,  1.8584,  1.8534,  1.8524,  1.8489],\n",
       "       device='cuda:0', grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(monitor.activations['decoder.layers.2.self_attention.kv_layernorm'][:, 0, :], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "458b9b27-e4cd-4f82-9fb6-95cbcecad2ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000, 16.1385, 15.3944, 14.7198, 14.1040, 13.5382, 13.0171,\n",
       "        12.5349, 12.0872, 11.6713, 11.2813, 92.0106, 21.3193, 20.6569, 20.0322,\n",
       "        19.4414, 18.8866, 18.3655, 17.8717, 17.4036, 16.9596, 92.0110, 32.7712,\n",
       "        31.6329, 30.5748, 29.5888, 28.6667, 27.8030, 26.9936, 26.2313, 25.5144,\n",
       "        31.7010, 41.8403, 40.8284, 39.8634, 38.9433, 38.0580, 37.2097, 36.3909,\n",
       "        35.6114, 34.8556, 92.0095, 33.4430, 32.7712, 32.1287, 31.5075, 30.9155,\n",
       "        30.3403, 29.7849, 29.2521, 28.7325, 28.2362, 34.5880, 34.0012, 33.4327,\n",
       "        32.8825, 32.3474, 31.8314, 31.3324, 30.8436, 30.3704, 29.9157, 29.4669,\n",
       "        29.0358, 28.6134, 28.2062, 27.8118, 27.4259, 27.0537, 26.6866, 26.3309,\n",
       "        25.9854, 25.6492, 25.3203, 25.0018, 24.6911, 24.3870, 24.0924, 23.7982,\n",
       "        23.5159, 23.2389, 22.9677, 22.7018, 22.4441, 22.1925, 21.9466, 21.7041,\n",
       "        21.4673, 21.2359, 21.0092, 20.7876, 20.5706, 20.3560, 20.1481, 19.9429,\n",
       "        19.7439, 19.5460, 19.3535, 19.1646, 18.9795, 18.8009, 18.6217, 18.4478,\n",
       "        18.2769, 18.1118, 17.9459, 17.7838, 17.6257, 17.4699, 17.3792, 17.2926,\n",
       "        17.2052, 17.1195, 17.0370, 16.9552, 16.8739, 16.7952, 16.7180, 16.6417],\n",
       "       device='cuda:0', grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.abs(monitor.activations['decoder.layers.1.self_attention.kv_layernorm'][:, 0, :]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ecd0b945-7ed7-4c4e-8c10-52d0cdc25bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 2, 1024])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d911205a-ec02-40c0-99c7-82d55cbfc664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 7.6915, 7.0506, 6.5082, 6.0433, 5.6404, 5.2879,\n",
       "        4.9769, 4.7004, 4.4530, 4.2303, 8.0578, 7.6915, 7.3571, 7.0506, 6.7685,\n",
       "        6.5082, 6.2672, 6.0433, 5.8349, 5.7221, 5.4585, 5.2879, 5.1277, 4.9769,\n",
       "        4.8347, 4.7004, 4.5733, 4.4530, 4.3388, 4.2303, 6.1907, 6.0433, 5.9028,\n",
       "        5.7686, 5.6404, 5.5178, 5.4004, 5.2879, 5.1800, 5.0764, 4.9769, 4.8812,\n",
       "        4.7891, 4.7004, 4.6149, 4.5325, 4.4530, 4.3762, 4.3020, 4.2303, 4.1610,\n",
       "        4.0939, 4.0289, 3.9659, 3.9049, 3.8458, 3.7884, 3.7326, 3.6786, 3.6260,\n",
       "        3.5749, 3.5253, 3.4770, 3.4300, 3.3843, 3.3397, 3.2964, 3.2541, 3.2129,\n",
       "        3.1728, 3.1336, 3.0954, 3.0581, 3.0217, 2.9861, 2.9514, 2.9175, 2.8843,\n",
       "        2.8519, 2.8202, 2.7892, 2.7589, 2.7292, 2.7002, 2.6718, 2.6440, 2.6167,\n",
       "        2.5900, 2.5638, 2.5382, 2.5131, 2.4884, 2.4643, 2.4406, 2.4173, 2.3945,\n",
       "        2.3722, 2.3502, 2.3286, 2.3075, 2.2867, 2.2663, 2.2462, 2.2265, 2.2071,\n",
       "        2.1881, 2.1694], device='cuda:0', grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act = torch.concat([torch.unsqueeze(x, dim=0) for x in monitor.activations['decoder.layers.0.self_attention.core_attention_backward']], dim=0)\n",
    "torch.sum(torch.abs(act[:, 0, :]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0e93ab5-b1d7-4ef0-b247-aba7ea72a7be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7.6915, 7.0506, 6.5082, 6.0433, 5.6404, 5.2879, 4.9769, 4.7004, 4.4530,\n",
       "        4.2303, 8.2497, 7.6915, 7.3571, 7.0506, 6.7685, 6.5082, 6.2672, 6.0433,\n",
       "        5.8349, 5.6404, 5.5682, 5.2879, 5.1277, 4.9769, 4.8347, 4.7004, 4.5733,\n",
       "        4.4530, 4.3388, 4.2303, 6.1907, 6.0433, 5.9028, 5.7686, 5.6404, 5.5178,\n",
       "        5.4004, 5.2879, 5.1800, 5.0764, 5.0683, 4.8812, 4.7891, 4.7004, 4.6149,\n",
       "        4.5325, 4.4530, 4.3762, 4.3020, 4.2303, 4.1610, 4.0939, 4.0289, 3.9659,\n",
       "        3.9049, 3.8458, 3.7884, 3.7326, 3.6786, 3.6260, 3.5749, 3.5253, 3.4770,\n",
       "        3.4300, 3.3843, 3.3397, 3.2964, 3.2541, 3.2129, 3.1728, 3.1336, 3.0954,\n",
       "        3.0581, 3.0217, 2.9861, 2.9514, 2.9175, 2.8843, 2.8519, 2.8202, 2.7892,\n",
       "        2.7589, 2.7292, 2.7002, 2.6718, 2.6440, 2.6167, 2.5900, 2.5638, 2.5382,\n",
       "        2.5131, 2.4884, 2.4643, 2.4406, 2.4173, 2.3945, 2.3722, 2.3502, 2.3286,\n",
       "        2.3075, 2.2867, 2.2663, 2.2462, 2.2265, 2.2071, 2.1881, 2.1694, 2.1510,\n",
       "        2.1329, 2.1152, 2.0977, 2.0805, 2.0636, 2.0469, 2.0306, 2.0144, 1.9986,\n",
       "        1.9830, 1.9830, 1.9830, 1.9830, 1.9830, 1.9830, 1.9830, 1.9830, 1.9830,\n",
       "        1.9830, 1.9830], device='cuda:0', grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act = torch.concat([torch.unsqueeze(x, dim=0) for x in monitor.activations['decoder.layers.0.self_attention.bidir_attention_forward']], dim=0)\n",
    "torch.sum(torch.abs(act[:, 0, :]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "256322ad-223a-42ee-89e0-cf2ce66cc229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7.6915, 7.0506, 6.5082, 6.0433, 5.6404, 5.2879, 4.9769, 4.7004, 4.4530,\n",
       "        4.2303, 8.2497, 7.6915, 7.3571, 7.0506, 6.7685, 6.5082, 6.2672, 6.0433,\n",
       "        5.8349, 5.6404, 5.5682, 5.2879, 5.1277, 4.9769, 4.8347, 4.7004, 4.5733,\n",
       "        4.4530, 4.3388, 4.2303, 6.1907, 6.0433, 5.9028, 5.7686, 5.6404, 5.5178,\n",
       "        5.4004, 5.2879, 5.1800, 5.0764, 5.0683, 4.8812, 4.7891, 4.7004, 4.6149,\n",
       "        4.5325, 4.4530, 4.3762, 4.3020, 4.2303, 4.1610, 4.0939, 4.0289, 3.9659,\n",
       "        3.9049, 3.8458, 3.7884, 3.7326, 3.6786, 3.6260, 3.5749, 3.5253, 3.4770,\n",
       "        3.4300, 3.3843, 3.3397, 3.2964, 3.2541, 3.2129, 3.1728, 3.1336, 3.0954,\n",
       "        3.0581, 3.0217, 2.9861, 2.9514, 2.9175, 2.8843, 2.8519, 2.8202, 2.7892,\n",
       "        2.7589, 2.7292, 2.7002, 2.6718, 2.6440, 2.6167, 2.5900, 2.5638, 2.5382,\n",
       "        2.5131, 2.4884, 2.4643, 2.4406, 2.4173, 2.3945, 2.3722, 2.3502, 2.3286,\n",
       "        2.3075, 2.2867, 2.2663, 2.2462, 2.2265, 2.2071, 2.1881, 2.1694, 2.1510,\n",
       "        2.1329, 2.1152, 2.0977, 2.0805, 2.0636, 2.0469, 2.0306, 2.0144, 1.9986,\n",
       "        1.9830, 1.9830, 1.9830, 1.9830, 1.9830, 1.9830, 1.9830, 1.9830, 1.9830,\n",
       "        1.9830, 1.9830], device='cuda:0', grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act = torch.concat([torch.unsqueeze(x, dim=0) for x in monitor.activations['decoder.layers.0.self_attention.bidir_attention_backward']], dim=0)\n",
    "torch.sum(torch.abs(act[:, 0, :]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3997be4-c3b6-4f86-97c1-353e99a83353",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2f6865",
   "metadata": {},
   "outputs": [],
   "source": [
    "from megatron.core import parallel_state\n",
    "\n",
    "parallel_state.model_parallel_is_initialized()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9543a86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from megatron.core.models.gpt import GPTModel\n",
    "from megatron.core.transformer.spec_utils import import_module\n",
    "\n",
    "import megatron.core.models.gpt as gpt\n",
    "import importlib\n",
    "importlib.reload(gpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47fbb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_process = True\n",
    "post_process = True\n",
    "mtp_block_spec = None\n",
    "vp_stage = None\n",
    "\n",
    "model = gpt.GPTModel(\n",
    "    config=config,\n",
    "    transformer_layer_spec=transformer_layer_spec,\n",
    "    vocab_size=args.padded_vocab_size,\n",
    "    max_sequence_length=args.max_position_embeddings,\n",
    "    pre_process=pre_process,\n",
    "    post_process=post_process,\n",
    "    fp16_lm_cross_entropy=args.fp16_lm_cross_entropy,\n",
    "    parallel_output=True,\n",
    "    share_embeddings_and_output_weights=not args.untie_embeddings_and_output_weights,\n",
    "    position_embedding_type=args.position_embedding_type,\n",
    "    rotary_percent=args.rotary_percent,\n",
    "    rotary_base=args.rotary_base,\n",
    "    rope_scaling=args.use_rope_scaling,\n",
    "    mtp_block_spec=mtp_block_spec,\n",
    "    vp_stage=vp_stage,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29dc659",
   "metadata": {},
   "outputs": [],
   "source": [
    "from megatron.training import get_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764b7cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from megatron.training.yaml_arguments import load_yaml, core_transformer_config_from_yaml\n",
    "\n",
    "# Load YAML and build transformer config once\n",
    "args_from_yaml = load_yaml(\"/home/ubuntu/projects/llm-models/llm-models/models/bidirectional/gpt_config.yaml\")\n",
    "transformer_config = core_transformer_config_from_yaml(args_from_yaml, \"language_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2c3e62-422c-48ce-ba90-f4704da8eef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from megatron.training.yaml_arguments import load_yaml, core_transformer_config_from_yaml\n",
    "\n",
    "# Load YAML and build transformer config once\n",
    "args_from_yaml = load_yaml(\"/home/ubuntu/projects/llm-models/llm-models/models/bidirectional/gpt_config.yaml\")\n",
    "transformer_config = core_transformer_config_from_yaml(args_from_yaml, \"language_model\")\n",
    "\n",
    "# Provide gpt_builder with that config (config=... skips internal config loading)\n",
    "def model_provider_with_yaml_config(pre_process=True, post_process=True, vp_stage=None):\n",
    "    # You must still have get_args() returning the full args (e.g. from same YAML)\n",
    "    args = get_args()\n",
    "    return gpt_builder(args, pre_process, post_process, vp_stage, config=transformer_config)\n",
    "\n",
    "# Then in pretrain:\n",
    "pretrain(..., partial(model_provider, model_provider_with_yaml_config), ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a505a7-8599-41bb-b010-3f9a4fdc5c0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
