{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d31fb919-9ef7-4a14-9f20-941e1da6663a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, time, gc\n",
    "\n",
    "# Timing utilities\n",
    "start_time = None\n",
    "\n",
    "def start_timer():\n",
    "    global start_time\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_max_memory_allocated()\n",
    "    torch.cuda.synchronize()\n",
    "    start_time = time.time()\n",
    "\n",
    "def end_timer_and_print(local_msg):\n",
    "    torch.cuda.synchronize()\n",
    "    end_time = time.time()\n",
    "    print(\"\\n\" + local_msg)\n",
    "    print(\"Total execution time = {:.3f} sec\".format(end_time - start_time))\n",
    "    print(\"Max memory used by tensors = {} bytes\".format(torch.cuda.max_memory_allocated()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4790878-778c-4e48-afd6-2f55277e6c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(in_size, out_size, num_layers):\n",
    "    layers = []\n",
    "    for _ in range(num_layers - 1):\n",
    "        layers.append(torch.nn.Linear(in_size, in_size))\n",
    "        layers.append(torch.nn.ReLU())\n",
    "    layers.append(torch.nn.Linear(in_size, out_size))\n",
    "    return torch.nn.Sequential(*tuple(layers)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd2ccc2c-7ccd-409b-a304-740536bae6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512 # Try, for example, 128, 256, 513.\n",
    "in_size = 4096\n",
    "out_size = 4096\n",
    "num_layers = 3\n",
    "num_batches = 50\n",
    "epochs = 3\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261a1116-72b2-4b23-9fa3-76426f55ed4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1faaf5e-914a-4157-bc48-76039d65d46f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/projects/llm-models/llm/lib/python3.12/site-packages/torch/cuda/__init__.py:283: UserWarning: \n",
      "    Found GPU0 NVIDIA T4G which is of cuda capability 7.5.\n",
      "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "    (8.0) - (12.0)\n",
      "    \n",
      "  warnings.warn(\n",
      "/home/ubuntu/projects/llm-models/llm/lib/python3.12/site-packages/torch/cuda/__init__.py:304: UserWarning: \n",
      "    Please install PyTorch with a following CUDA\n",
      "    configurations:  12.6 12.8 12.9 following instructions at\n",
      "    https://pytorch.org/get-started/locally/\n",
      "    \n",
      "  warnings.warn(matched_cuda_warn.format(matched_arches))\n",
      "/home/ubuntu/projects/llm-models/llm/lib/python3.12/site-packages/torch/cuda/__init__.py:326: UserWarning: \n",
      "NVIDIA T4G with CUDA capability sm_75 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_80 sm_90 sm_100 sm_120.\n",
      "If you want to use the NVIDIA T4G GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AcceleratorError",
     "evalue": "CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAcceleratorError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Creates data in default precision.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# The same data is used for both default and mixed precision trials below.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# You don't need to manually change inputs' ``dtype`` when enabling mixed precision.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m data = [\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_size\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches)]\n\u001b[32m      5\u001b[39m targets = [torch.randn(batch_size, out_size) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches)]\n\u001b[32m      7\u001b[39m loss_fn = torch.nn.MSELoss().cuda()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/llm-models/llm/lib/python3.12/site-packages/torch/utils/_device.py:103\u001b[39m, in \u001b[36mDeviceContext.__torch_function__\u001b[39m\u001b[34m(self, func, types, args, kwargs)\u001b[39m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    102\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.device\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mAcceleratorError\u001b[39m: CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# Creates data in default precision.\n",
    "# The same data is used for both default and mixed precision trials below.\n",
    "# You don't need to manually change inputs' ``dtype`` when enabling mixed precision.\n",
    "data = [torch.randn(batch_size, in_size) for _ in range(num_batches)]\n",
    "targets = [torch.randn(batch_size, out_size) for _ in range(num_batches)]\n",
    "\n",
    "loss_fn = torch.nn.MSELoss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50705e3b-bd55-400d-b9ad-7bd11cc5805c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/pytorch/lib/python3.12/site-packages/torch/cuda/memory.py:491: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Default precision:\n",
      "Total execution time = 5.868 sec\n",
      "Max memory used by tensors = 1283817984 bytes\n"
     ]
    }
   ],
   "source": [
    "net = make_model(in_size, out_size, num_layers)\n",
    "opt = torch.optim.SGD(net.parameters(), lr=0.001)\n",
    "\n",
    "start_timer()\n",
    "for epoch in range(epochs):\n",
    "    for input, target in zip(data, targets):\n",
    "        output = net(input)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad() # set_to_none=True here can modestly improve performance\n",
    "end_timer_and_print(\"Default precision:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2daa63f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "280f4fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Default precision:\n",
      "Total execution time = 2.782 sec\n",
      "Max memory used by tensors = 1409672192 bytes\n"
     ]
    }
   ],
   "source": [
    "net = make_model(in_size, out_size, num_layers)\n",
    "opt = torch.optim.SGD(net.parameters(), lr=0.001)\n",
    "\n",
    "scaler = torch.amp.GradScaler(\"cuda\")\n",
    "\n",
    "start_timer()\n",
    "with torch.autocast(device_type=device, dtype=torch.float16):\n",
    "    for epoch in range(epochs):\n",
    "        for input, target in zip(data, targets):\n",
    "            output = net(input)\n",
    "            \n",
    "            loss = loss_fn(output, target)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "            opt.zero_grad() # set_to_none=True here can modestly improve performance\n",
    "end_timer_and_print(\"Default precision:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98c7f128-78ef-4b9b-bad2-98e60374bf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f688b729-376d-42be-bb95-99d1557e480a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sm_80', 'sm_90', 'sm_100', 'sm_120']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.torch.cuda.get_arch_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84bd1bfa-0108-421a-b74c-ad645a7b9cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.9\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "402908eb-e4b9-499d-a808-592d65d0c2cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(tf.sysconfig.get_build_info()[\u001b[33m'\u001b[39m\u001b[33mcuda_version\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.sysconfig.get_build_info()['cuda_version'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1df30807-c83f-4888-8480-3febd4fb53a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.8.0+cu129'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07e02524-8d2b-4238-968e-304240b779a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_CudaDeviceProperties(name='NVIDIA T4G', major=7, minor=5, total_memory=14912MB, multi_processor_count=40, uuid=b9de142b-7912-5e4d-e780-40641135317c, pci_bus_id=0, pci_device_id=31, pci_domain_id=0, L2_cache_size=4MB)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_properties(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26202c87-7247-4257-aac7-d27c73803cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sm_80', 'sm_90', 'sm_100', 'sm_120']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_arch_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cdc2c8-6854-43e4-9e50-f5217fb3483d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
